// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model.h"

// includes from bcl - sorted alphabetically
#include "bcl_version.h"
#include "command/bcl_command_app_default_flags.h"
#include "command/bcl_command_flag_static.h"
#include "command/bcl_command_parameter.h"
#include "command/bcl_command_parameter_check_file_in_search_path.h"
#include "util/bcl_util_runtime_environment_interface.h"

// external includes - sorted alphabetically

//path for models
#if defined (__MINGW32__)
  #define BCL_MODEL_PATH "model/"
#elif defined (__GNUC__)
  #define BCL_MODEL_PATH "/dors/meilerlab/apps/bcl/model/rev_5051/"
#elif defined (_MSC_VER)
  #define BCL_MODEL_PATH "../../../model/"
#endif

namespace bcl
{
  namespace model
  {

    //! flag to change model path
    util::ShPtr< command::FlagInterface> &Model::GetModelPathFlag()
    {
      static util::ShPtr< command::FlagInterface> s_model_path_flag
      (
        new command::FlagStatic
        (
          "model_path",
          "change path for reading and writing models",
          command::Parameter
          (
            "model_path_name",
            "relative or absolute model path",
            command::ParameterCheckFileInSearchPath
            (
              "model",
              GetVersion().IsLicense()
              ? GetVersion().GetInstallPrefix() + "/model/"
              : BCL_MODEL_PATH,
              io::Directory::e_Dir
            ),
            ""
          )
        )
      );

      return s_model_path_flag;
    }

    //! @brief enum, that adds model params flag to default app flags
    static const util::ShPtr< command::FlagInterface> e_ModelPathFlag
    (
      command::GetAppDefaultFlags().AddDefaultFlag( Model::GetModelPathFlag(), command::e_Model)
    );

    //! @brief identifier for the name space
    //! @return the name of the namespace
    const std::string &GetNamespaceIdentifier()
    {
      static const std::string *s_namespace_name( new std::string( util::ExtractNamespaceIdentifier( __PRETTY_FUNCTION__)));
      return *s_namespace_name;
    }

    //! @brief given a FILENAME, the model path is prepended to the filename
    //! @param FILENAME the filename to a model that is used in one of the scores
    //! @return string with modelpath/FILENAME
    std::string Model::AddModelPath( const std::string &FILENAME)
    {
      const std::string resolved_filename
      (
        util::GetRuntimeEnvironment().ResolveFileName
        (
          GetModelPathFlag()->GetFirstParameter()->GetValue() + FILENAME
        )
      );

      BCL_Assert
      (
        !resolved_filename.empty(),
        "unable to resolve filename: " + GetModelPathFlag()->GetFirstParameter()->GetValue() + FILENAME
      );

      return resolved_filename;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_align_cutoff.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> AlignCutoff::s_Instance
    (
      GetObjectInstances().AddInstance( new AlignCutoff())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    AlignCutoff::AlignCutoff() :
      m_InternalModel(),
      m_ExperimentalCutoff(),
      m_AdjustedCutoff(),
      m_SlopeAboveCutoff(),
      m_SlopeBelowCutoff(),
      m_RescaleInput(),
      m_RescaleOutput()
    {
    }

    //! @brief constructor with a model, cutoff as parameters
    AlignCutoff::AlignCutoff
    (
      const util::ShPtr< Interface> &MODEL,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_INPUT,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_OUTPUT,
      const float EXPERIMENTAL_CUTOFF,
      const float ADJUSTED_CUTOFF,
      const float SLOPE_ABOVE_CUTOFF,
      const float SLOPE_BELOW_CUTOFF
    ) :
      m_InternalModel( MODEL),
      m_ExperimentalCutoff( EXPERIMENTAL_CUTOFF),
      m_AdjustedCutoff( ADJUSTED_CUTOFF),
      m_SlopeAboveCutoff( SLOPE_ABOVE_CUTOFF),
      m_SlopeBelowCutoff( SLOPE_BELOW_CUTOFF),
      m_RescaleInput( RESCALE_INPUT),
      m_RescaleOutput( RESCALE_OUTPUT)
    {
    }

    //! copy constructor
    AlignCutoff *AlignCutoff::Clone() const
    {
      return new AlignCutoff( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &AlignCutoff::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &AlignCutoff::GetAlias() const
    {
      static const std::string s_Name( "AlignCutoff");
      return s_Name;
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief get number outputs
    //! @return number of prediction outputs equivalent to the number of result columns
    size_t AlignCutoff::GetNumberOutputs() const
    {
      return m_InternalModel->GetNumberOutputs();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void AlignCutoff::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !m_RescaleInput.IsDefined())
      {
        FEATURE.DeScale();
        return;
      }
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_RescaleInput);
      }
    }

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> AlignCutoff::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURE
    ) const
    {
      FeatureDataSet< float> predictions( m_InternalModel->PredictWithoutRescaling( FEATURE));

      // number of rows in predictions
      const size_t num_rows( predictions.GetNumberFeatures());

      // number cols in predictions
      const size_t num_cols( predictions.GetFeatureSize());

//      BCL_Message
//      (
//        util::Message::e_Verbose,
//        "m_AdjustedCutoff: " + util::Format()( m_AdjustedCutoff)
//        + " --- Slope above: " + util::Format()( m_SlopeBelowCutoff)
//        + " Slope Below: " + util::Format()( m_SlopeAboveCutoff)
//      );

      // get the raw matrix
      linal::Matrix< float> &classifications( predictions.GetRawMatrix());
      // subtract the adjusted cutoff
      classifications -= m_AdjustedCutoff;
      for( size_t row_id( 0); row_id < num_rows; ++row_id)
      {
        // get the row out of the matrix
        float *classification_row( classifications[ row_id]);
        for( size_t col_id( 0); col_id < num_cols; ++col_id)
        {
          // multiply by the appropriate slope
          classification_row[ col_id] *= classification_row[ col_id] > 0.0f ? m_SlopeAboveCutoff : m_SlopeBelowCutoff;
        }
      }
      // add in the experimental cutoff
      classifications += m_ExperimentalCutoff;

      return
        m_RescaleOutput.IsDefined()
        ? FeatureDataSet< float>( classifications, *m_RescaleOutput)
        : FeatureDataSet< float>( classifications);
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> AlignCutoff::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( m_RescaleInput.IsDefined())
      {
        if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
        {
          FeatureDataSet< float> feature( FEATURE);
          feature.Rescale( *m_RescaleInput);
          return PredictWithoutRescaling( feature).DeScale();
        }
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read AlignCutoff from std::istream
    std::istream &AlignCutoff::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_InternalModel, ISTREAM);
      io::Serialize::Read( m_ExperimentalCutoff, ISTREAM);
      io::Serialize::Read( m_AdjustedCutoff,     ISTREAM);
      io::Serialize::Read( m_SlopeAboveCutoff,   ISTREAM);
      io::Serialize::Read( m_SlopeBelowCutoff,   ISTREAM);
      io::Serialize::Read( m_RescaleInput, ISTREAM);
      io::Serialize::Read( m_RescaleOutput, ISTREAM);

      // end
      return ISTREAM;
    }

    //! write AlignCutoff into std::ostream
    std::ostream &AlignCutoff::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_InternalModel, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ExperimentalCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_AdjustedCutoff,     OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SlopeAboveCutoff,   OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SlopeBelowCutoff,   OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleInput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleOutput, OSTREAM, INDENT) << '\n';

      // end
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_align_cutoff.h"

// includes from bcl - sorted by alphabetically
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"
#include "model/bcl_model_align_cutoff.h"
#include "model/bcl_model_objective_function_constant.h"

// external includes - sorted by alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorAlignCutoff::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorAlignCutoff())
    );

    //! @brief get the numerical tolerance used in this class
    //! @return the numerical tolerance used in this class
    const float &ApproximatorAlignCutoff::GetNumericalTolerance()
    {
      static const float s_tolerance( 1.0e-5);
      return s_tolerance;
    }

    //! @brief default constructor
    ApproximatorAlignCutoff::ApproximatorAlignCutoff() :
      m_InternalIterate(),
      m_DescaledExperimentalCutoff( 0.5),
      m_ExperimentalCutoff( 0.5),
      m_ModelCutoff( 0.5),
      m_ExperimentalAverageAboveCutoff( 1.0),
      m_ExperimentalAverageBelowCutoff( 0.0),
      m_ModelAverageAboveCutoff( 1.0),
      m_ModelAverageBelowCutoff( 0.0),
      m_PositivesAboveThreshold( false),
      m_NumberActives( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new ApproximatorAlignCutoff copied from this instance
    ApproximatorAlignCutoff *ApproximatorAlignCutoff::Clone() const
    {
      return new ApproximatorAlignCutoff( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorAlignCutoff::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorAlignCutoff::GetAlias() const
    {
      static const std::string s_Name( "AlignCutoff");
      return s_Name;
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorAlignCutoff::GetCurrentModel() const
    {
      // make a new model out of the current data members
      return
        util::ShPtr< Interface>
        (
          new AlignCutoff
          (
            m_LastInternallyGeneratedModel.IsDefined() ? m_LastInternallyGeneratedModel : m_InternalIterate->GetCurrentModel(),
            GetRescaleFeatureDataSet(),
            GetRescaleResultDataSet(),
            m_ExperimentalCutoff,
            m_ModelCutoff,
            ( m_ExperimentalAverageAboveCutoff - m_ExperimentalCutoff) / ( m_ModelAverageAboveCutoff - m_ModelCutoff),
            ( m_ExperimentalAverageBelowCutoff - m_ExperimentalCutoff) / ( m_ModelAverageBelowCutoff - m_ModelCutoff)
          )
        );
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorAlignCutoff::GetCurrentApproximation() const
    {
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>
          (
            m_LastModel,
            m_LastObjectiveFunctionValue
          )
        );
    }

    //! @brief set objective function to evaluate a monitoring dataset
    //! @param OBJ objective function of interest
    void ApproximatorAlignCutoff::SetObjectiveFunction( const util::ShPtr< ObjectiveFunctionWrapper> &OBJ)
    {
      ApproximatorBase::SetObjectiveFunction( OBJ);

      // reset the last internally generated model to force re-evaluation of the objective function if it is
      // called again
      m_LastInternallyGeneratedModel = util::ShPtr< Interface>();
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorAlignCutoff::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_TrainingData = DATA;
      m_InternalIterate->SetTrainingData( DATA);
      const FeatureDataSetInterface< float> &results( *m_InternalIterate->GetTrainingData()->GetResultsPtr());

      m_ExperimentalCutoff = m_DescaledExperimentalCutoff;
      if( m_InternalIterate->GetRescaleResultDataSet().IsDefined())
      {
        // need to adjust the experimental threshold
        m_ExperimentalCutoff = m_InternalIterate->GetRescaleResultDataSet()->RescaleValue( 0, m_DescaledExperimentalCutoff);
      }

      math::RunningAverage< float> average_above_cutoff, average_below_cutoff;
      for( size_t row_id( 0), num_rows( results.GetNumberFeatures()); row_id < num_rows; ++row_id)
      {
        const float &result_value( results[ row_id][ 0]);

        // average values on each side of the cutoff cutoff
        if( result_value > m_ExperimentalCutoff)
        {
          average_above_cutoff += result_value;
        }
        else
        {
          average_below_cutoff += result_value;
        }
      }
      m_ExperimentalAverageAboveCutoff = average_above_cutoff.GetWeight() ? average_above_cutoff : m_ExperimentalCutoff;
      m_ExperimentalAverageBelowCutoff = average_below_cutoff.GetWeight() ? average_below_cutoff : m_ExperimentalCutoff;
      if( math::EqualWithinTolerance( m_ExperimentalAverageAboveCutoff, m_ExperimentalCutoff, GetNumericalTolerance()))
      {
        // essentially everything above the cutoff shares the same value
        // To allow ranking, and to avoid divisions by zero, make the average above cutoff slightly larger
        m_ExperimentalAverageAboveCutoff =
            m_ExperimentalCutoff > GetNumericalTolerance()
            ? m_ExperimentalCutoff * ( 1.0 + GetNumericalTolerance())
            : m_ExperimentalCutoff < -GetNumericalTolerance()
              ? m_ExperimentalCutoff / ( 1.0 + GetNumericalTolerance())
              : m_ExperimentalCutoff + GetNumericalTolerance();
      }

      if( math::EqualWithinTolerance( m_ExperimentalAverageBelowCutoff, m_ExperimentalCutoff, GetNumericalTolerance()))
      {
        // essentially everything below the cutoff shares the same value
        // To allow ranking, and to avoid divisions by zero, make the average above cutoff slightly larger
        m_ExperimentalAverageBelowCutoff =
            m_ExperimentalCutoff > GetNumericalTolerance()
            ? m_ExperimentalCutoff / ( 1.0 + GetNumericalTolerance())
            : m_ExperimentalCutoff < -GetNumericalTolerance()
              ? m_ExperimentalCutoff * ( 1.0 + GetNumericalTolerance())
              : m_ExperimentalCutoff - GetNumericalTolerance();
      }

      m_ModelAverageAboveCutoff = m_ExperimentalAverageAboveCutoff;
      m_ModelAverageBelowCutoff = m_ExperimentalAverageBelowCutoff;
      m_NumberActives = m_PositivesAboveThreshold ? average_above_cutoff.GetWeight() : average_below_cutoff.GetWeight();

      BCL_MessageVrb( "Initialize num actives: " + util::Format()( m_NumberActives));
      BCL_MessageVrb
      (
        "Initialize average above / below: " + util::Format()( m_ExperimentalAverageAboveCutoff)
        + " / " + util::Format()( m_ExperimentalAverageBelowCutoff)
      );

    } // SetTrainingData

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorAlignCutoff::Next()
    {
      m_InternalIterate->Next();
      util::ShPtr< Interface> internal_model( m_InternalIterate->GetCurrentModel());

      // check whether the model was actually changed
      if( internal_model != m_LastInternallyGeneratedModel)
      {
        // model changed, recompute the cutoffs and create the new model
        m_LastInternallyGeneratedModel = internal_model;

        // determine the new cutoffs and averages
        DetermineAdjustedCutoff();

        // create a new classifier
        m_LastModel = GetCurrentModel();

        // compute the objective function
        m_LastObjectiveFunctionValue = ( *m_ObjectiveFunction)( m_LastModel);
      }

      // combine it with the objective function evaluation, best precision seen with associated cutoff
      this->GetTracker().Track( GetCurrentApproximation());
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorAlignCutoff::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Adjusts the values such that the model's output can be used with the input cutoff for rank-classification purposes "
        "Results of different machine learning methods or cross validation runs should not normally be averaged, for the "
        "purposes of making a jury prediction or presenting the results of a ranked-classification model, without using "
        "this wrapper.   This iterate applies a linear function to map the values given by the internal iterate.  The "
        "function is created so as to maximize precision of predictions, while maintaining a similar hit rate to the training data"
      );

      parameters.AddInitializer
      (
        "iterate",
        "iterate string that defines the machine learning training algorithm",
        io::Serialization::GetAgent( &m_InternalIterate)
      );

      parameters.AddInitializer
      (
        "cutoff",
        "potency cutoff",
        io::Serialization::GetAgent( &m_DescaledExperimentalCutoff)
      );

      parameters.AddInitializer
      (
        "parity",
        "specifies whether to prefer models the predict values above the cutoff (1) or below the cutoff (0)",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );

      return parameters;
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERROR_STREAM the stream to write errors to
    bool ApproximatorAlignCutoff::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      // acquire the objective function given to the internal iterate
      SetObjectiveFunction( m_InternalIterate->GetObjectiveFunction());

      // give the internal iterate an undefined objective function since its output will not be used
      m_InternalIterate->SetObjectiveFunction
      (
        util::ShPtr< ObjectiveFunctionWrapper>
        (
          new ObjectiveFunctionWrapper
          (
            ObjectiveFunctionConstant
            (
              0.0,
              m_ObjectiveFunction->GetImprovementType(),
              m_ObjectiveFunction->GetGoalType(),
              m_ObjectiveFunction->GetThreshold()
            )
          )
        )
      );

      // set the model cutoff == the experimental cutoff initially
      m_ExperimentalCutoff = m_DescaledExperimentalCutoff;
      m_ModelCutoff = m_ExperimentalCutoff;

      // set the experimental average below cutoff slightly below the threshold, to prevent divisions by zero
      m_ExperimentalAverageBelowCutoff =
        m_ExperimentalCutoff > 1.0
        ? m_ExperimentalCutoff / 2.0
        : m_ExperimentalCutoff < -1.0
          ? m_ExperimentalCutoff * 2.0
          : m_ExperimentalCutoff - 1.0;

      // set the experimental average above cutoff above the threshold, to prevent divisions by zero
      m_ExperimentalAverageAboveCutoff =
        m_ExperimentalCutoff > 1.0
        ? m_ExperimentalCutoff * 2.0
        : m_ExperimentalCutoff < -1.0
          ? m_ExperimentalCutoff / 2.0
          : m_ExperimentalCutoff + 1.0;

      m_ModelAverageAboveCutoff = m_ExperimentalAverageAboveCutoff;
      m_ModelAverageBelowCutoff = m_ExperimentalAverageBelowCutoff;

      return true;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorAlignCutoff::Read( std::istream &ISTREAM)
    {
      descriptor::Dataset::s_Instance.IsDefined();

      // read members
      io::Serialize::Read( m_InternalIterate, ISTREAM);
      io::Serialize::Read( m_DescaledExperimentalCutoff, ISTREAM);
      io::Serialize::Read( m_ExperimentalCutoff, ISTREAM);
      io::Serialize::Read( m_ModelCutoff, ISTREAM);
      io::Serialize::Read( m_ExperimentalAverageAboveCutoff, ISTREAM);
      io::Serialize::Read( m_ExperimentalAverageBelowCutoff, ISTREAM);
      io::Serialize::Read( m_ModelAverageAboveCutoff, ISTREAM);
      io::Serialize::Read( m_ModelAverageBelowCutoff, ISTREAM);
      io::Serialize::Read( m_PositivesAboveThreshold, ISTREAM);
      io::Serialize::Read( m_NumberActives, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorAlignCutoff::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_InternalIterate, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_DescaledExperimentalCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ExperimentalCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ModelCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ExperimentalAverageAboveCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ExperimentalAverageBelowCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ModelAverageAboveCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ModelAverageBelowCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_PositivesAboveThreshold, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NumberActives, OSTREAM, INDENT) << '\n';

      // return the stream
      return OSTREAM;
    }

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorAlignCutoff::CanContinue() const
    {
      return m_InternalIterate->CanContinue();
    }

    //! @brief determine the adjusted cutoff that maximizes the precision on the classification model
    void ApproximatorAlignCutoff::DetermineAdjustedCutoff()
    {
      const FeatureDataSet< float> predicted
      (
        m_LastInternallyGeneratedModel->PredictWithoutRescaling( *( m_InternalIterate->GetTrainingData()->GetFeaturesPtr()))
      );
      const FeatureDataSetInterface< float> &experimental( *( m_InternalIterate->GetTrainingData()->GetResultsPtr()));

      linal::MatrixConstReference< float> predicted_matrix( predicted.GetMatrix());

      BCL_Assert
      (
        experimental.GetNumberFeatures() == predicted.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );

      BCL_Assert
      (
        experimental.GetFeatureSize() == predicted.GetFeatureSize(),
        "Experimental and predicted values do not have the same number of elements!"
      );

      // number of experimental values
      const size_t data_set_size( experimental.GetNumberFeatures());

      // number of predicted result columns
      const size_t result_size( predicted.GetFeatureSize());

      if( data_set_size == 0 || result_size == 0)
      {
        // no data available, no result column given
        m_ModelAverageBelowCutoff =  m_ExperimentalAverageBelowCutoff - m_ExperimentalCutoff + m_ModelCutoff;
        m_ModelAverageAboveCutoff = m_ExperimentalAverageAboveCutoff - m_ExperimentalCutoff + m_ModelCutoff;
        return;
      }

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        float best_precision( 0);

        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>
            (
              predicted( counter)( result_number),
              experimental( counter)( result_number)
            )
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_ExperimentalCutoff, m_PositivesAboveThreshold);

        const storage::Vector< math::ROCCurve::Point> &sorted_counts( roc_curve.GetSortedCounts());
        size_t best_cutoff_count( 0);

        for
        (
          storage::Vector< math::ROCCurve::Point>::const_iterator
            itr( sorted_counts.Begin()), itr_end( sorted_counts.End());
          itr != itr_end;
          ++itr
        )
        {
          // # predicted (true & false) positives
          const size_t predicted( itr->GetNumberPredictedPositives());

          // require at least the actives are predicted before computing information gain ratio to avoid artifacts
          if( predicted < m_NumberActives)
          {
            continue;
          }

          math::ContingencyMatrix matrix( itr->GetContingencyMatrix( sorted_counts.LastElement()));

          // determine best seen precision value so far
          const float precision( matrix.GetInformationGainRatio());
          if( precision > best_precision)
          {
            best_precision = precision;
            best_cutoff_count = predicted;
            // set best cutoff seen so far as model cutoff
            m_ModelCutoff = itr->GetCutoff();
          }
        }

        // compute the average values above and below the cutoff for the model's values
        math::RunningAverage< float> average_above_cutoff, average_below_cutoff;
        for( size_t result_row_number( 0); result_row_number < data_set_size; ++result_row_number)
        {
          const float value( predicted_matrix( result_row_number, result_number));
          if( value > m_ModelCutoff)
          {
            average_above_cutoff += value;
          }
          else
          {
            average_below_cutoff += value;
          }
        }

        m_ModelAverageAboveCutoff = average_above_cutoff;
        m_ModelAverageBelowCutoff = average_below_cutoff;

        //
        if( average_above_cutoff.GetWeight() < 0.5)
        {
          // nothing above the cutoff was found
          m_ModelAverageAboveCutoff = m_ExperimentalAverageAboveCutoff - m_ExperimentalCutoff + m_ModelCutoff;
        }
        else if( average_below_cutoff.GetWeight() < 0.5)
        {
          m_ModelAverageBelowCutoff = m_ExperimentalAverageBelowCutoff - m_ExperimentalCutoff + m_ModelCutoff;
        }

        BCL_MessageStd
        (
          "AdjustedCutoff: " + util::Format()( m_ModelCutoff)
          + " best precision:" + util::Format()( best_precision)
          + " at fraction: " + util::Format()( float( best_cutoff_count) / float( data_set_size))
          + " av below: " + util::Format()( m_ModelAverageBelowCutoff)
          + " av above: " + util::Format()( m_ModelAverageAboveCutoff)
          + " # above: " + util::Format()( average_above_cutoff.GetWeight())
          + " # below: " + util::Format()( average_below_cutoff.GetWeight())
        );
      }

      // if the model does not predict any actives, then set the model slope above ave equal to the experimental slope
      if( math::EqualWithinTolerance( m_ModelAverageAboveCutoff, m_ModelCutoff, GetNumericalTolerance()))
      {
        m_ModelAverageAboveCutoff = m_ModelCutoff + m_ExperimentalAverageAboveCutoff - m_ExperimentalCutoff;
      }

      if( math::EqualWithinTolerance( m_ModelAverageBelowCutoff, m_ModelCutoff, GetNumericalTolerance()))
      {
        m_ModelAverageBelowCutoff = m_ModelCutoff + m_ExperimentalAverageBelowCutoff - m_ExperimentalCutoff;
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_applicability_domain_kohonen.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_file.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_piecewise_function.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_min_max.h"
#include "model/bcl_model_kohonen_network_applicability_domain.h"
#include "model/bcl_model_retrieve_data_set_base.h"
#include "model/bcl_model_retrieve_interface.h"
#include "storage/bcl_storage_pair.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorApplicabilityDomainKohonen::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorApplicabilityDomainKohonen( true))
    );
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorApplicabilityDomainKohonen::s_ContingencyInstance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorApplicabilityDomainKohonen( false))
    );

    //! @brief default constructor
    ApproximatorApplicabilityDomainKohonen::ApproximatorApplicabilityDomainKohonen( const bool &USE_DISTANCE_METRIC) :
      ApproximatorKohonenNetwork(),
      m_ShareDistanceMetric( USE_DISTANCE_METRIC),
      m_UseDistanceMetric( USE_DISTANCE_METRIC),
      m_Monotonize( true)
    {
      ApproximatorKohonenNetwork::m_NoTrack = true;
    }

    //! @brief constructor from all necessary parameters
    //! @param MAP_DIMENSIONS dimensions of the map
    //! @param INITIAL_LENGTH how many iterations to train for (if applicable).
    //! @param INITAL_RADIUS the initial neighborhood radius
    //! @param OBJECTIVE_FUNCTION the objective function from the approximator framework
    //! @param UPDATE_EVERY_NTH_FEATURE update the nodes after seeing this many features
    //! @param NEIGHBOR_KERNEL the neighbor kernel type
    ApproximatorApplicabilityDomainKohonen::ApproximatorApplicabilityDomainKohonen
    (
      const linal::Vector< double> &MAP_DIMENSIONS,
      const size_t &INITIAL_LENGTH,
      const float &INITIAL_RADIUS,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const size_t &UPDATE_EVERY_NTH_FEATURE,
      const NeighborKernel &NEIGHBOR_KERNEL,
      const util::Implementation< RetrieveInterface> &RETREIVER
    ) :
      ApproximatorKohonenNetwork
      (
        MAP_DIMENSIONS,
        INITIAL_LENGTH,
        INITIAL_RADIUS,
        OBJECTIVE_FUNCTION,
        UPDATE_EVERY_NTH_FEATURE,
        NEIGHBOR_KERNEL
      ),
      m_Retriever( RETREIVER),
      m_ShareDistanceMetric( true),
      m_UseDistanceMetric( true),
      m_Monotonize( true)
    {
      ApproximatorKohonenNetwork::m_NoTrack = true;
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorApplicabilityDomainKohonen::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorApplicabilityDomainKohonen::GetAlias() const
    {
      static const std::string s_alias( "ApplicabilityDomainKohonen"), s_con_alias( "ApplicabilityMeasureKohonen");
      return m_UseDistanceMetric ? s_alias : s_con_alias;
    }

    //! @brief set training data set for a specific iterate in approximator framework
    //! @param DATA training data set
    void ApproximatorApplicabilityDomainKohonen::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_FinalModel.Reset();
      this->GetTracker() = opti::Tracker< util::ShPtr< Interface>, float>( opti::e_SmallerEqualIsBetter);
      if( m_Retriever.IsDefined())
      {
        BCL_Assert( !m_Retriever->GetAllKeys().IsEmpty(), "No models in: " + m_Retriever->GetString());
        BCL_Assert
        (
          DATA->GetFeatures().GetFeatureLabelSet().IsDefined(),
          "Features are required for model applicability, otherwise predictions could not be made"
        );
        BCL_Assert
        (
          m_Retriever->RetrieveUniqueDescriptor().GetArguments()
          == DATA->GetFeatures().GetFeatureLabelSet()->GetMemberLabels().InternalData(),
          "The applicability domain kohonen map is presently only trainable if it has the  be trained on the same set "
          "of features as the underlying model"
        );
        m_ResultDataset = m_Retriever->ReadMergedIndependentPredictions();
        BCL_Assert
        (
          m_ResultDataset->GetFeatureSize() == DATA->GetResultSize(),
          "Different # of results for model than in given dataset!"
        );
        BCL_Assert
        (
          math::EqualWithinTolerance
          (
            DATA->GetResults().GetMatrix(),
            m_ResultDataset->GetResults().GetMatrix()
          ),
          "Independent set results did not match dataset results!"
        );
      }
      ApproximatorKohonenNetwork::SetTrainingData( DATA);

      // compute average distance between gridpoints to give user idea of how stable the map is
      math::RunningAverage< double> ave_distance_between_gridpoints;
      for
      (
        size_t i( 0), n_gridpoints( ApproximatorKohonenNetwork::m_Network.GetCodeBook().GetSize());
        i < n_gridpoints;
        ++i
      )
      {
        const linal::VectorConstInterface< float> &position_i
        (
          ApproximatorKohonenNetwork::m_Network.GetCodeBook()( i).GetPosition()
        );
        for( size_t j( 0); j < i; ++j)
        {
          ave_distance_between_gridpoints +=
            linal::Distance( position_i, ApproximatorKohonenNetwork::m_Network.GetCodeBook()( j).GetPosition());
        }
        // on-diagonal
        ave_distance_between_gridpoints.AddWeightedObservation( 0.0, 0.5);
      }
      m_AverageDistanceBetweenGridpoints = ave_distance_between_gridpoints.GetAverage();

      if( !this->ShouldContinue())
      {
        MakeFinalModel();
        this->GetTracker().Track( GetCurrentApproximation(), opti::e_Improved);
      }
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorApplicabilityDomainKohonen::GetCurrentModel() const
    {
      return GetCurrentApproximation()->First();
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > ApproximatorApplicabilityDomainKohonen::GetCurrentApproximation() const
    {
      if( m_FinalModel.IsDefined())
      {
        return
          util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
          (
            new storage::Pair< util::ShPtr< Interface>, float>
            (
              m_FinalModel,
              m_AverageDisplacement / m_AverageDistanceBetweenGridpoints
            )
          );
      }
      util::ShPtr< KohonenNetworkApplicabilityDomain> app_domain
      (
        new KohonenNetworkApplicabilityDomain( m_Network, m_ShareDistanceMetric, m_Retriever)
      );
      util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > approximation
      (
        new storage::Pair< util::ShPtr< Interface>, float>
        (
          app_domain,
          m_AverageDisplacement / m_AverageDistanceBetweenGridpoints
        )
      );

      // more iterations ahead; just setup the splines without regards to any metric
      app_domain->SetupSplines
      (
        *m_TrainingData->GetFeaturesPtr(),
        ApproximatorKohonenNetwork::m_LastClosestNodes,
        m_ShareDistanceMetric
      );

      return approximation;
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorApplicabilityDomainKohonen::Next()
    {
      m_AverageDisplacement = 0;
      storage::Vector< size_t> last_round_assigned( ApproximatorKohonenNetwork::m_LastClosestNodes);
      ApproximatorKohonenNetwork::Next();
      math::RunningAverage< double> ave_distance_moved, ave_dist_between_points;
      for( size_t i( 0), n_data( last_round_assigned.GetSize()); i < n_data; ++i)
      {
        const size_t prev_node( last_round_assigned( i));
        const size_t curr_node( ApproximatorKohonenNetwork::m_LastClosestNodes( i));

        if( prev_node != curr_node)
        {
          ave_distance_moved +=
            linal::Distance
            (
              m_Network.GetCodeBook()( prev_node).GetPosition(),
              m_Network.GetCodeBook()( curr_node).GetPosition()
            );
        }
        else
        {
          ave_distance_moved += 0.0;
        }
      }
      m_AverageDisplacement = ave_distance_moved.GetAverage();
      BCL_MessageStd
      (
        util::Format()( m_AverageDisplacement) + " average distance each feature moved on map, "
        + util::Format().FFP( 2)
          (
            100.0 - std::min( 100.0, m_AverageDisplacement / m_AverageDistanceBetweenGridpoints * 100.0)
          )
        + " % solidified"
      );
      this->GetTracker().Track( GetCurrentApproximation());
      if( !this->ShouldContinue())
      {
        MakeFinalModel();
        this->GetTracker().Track( GetCurrentApproximation(), opti::e_Improved);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorApplicabilityDomainKohonen::GetSerializer() const
    {
      io::Serializer parameters( ApproximatorKohonenNetwork::GetSerializer());
      parameters.SetClassDescription
      (
        "A kohonen-map based implementation to detect whether a point is within the applicability domain of a model. "
        "Inspired by Assessment of applicability domain for multivariate counter-propagation artificial neural network "
        "predictive models by minimum Euclidean distance space analysis: A case study"
      );

      // fix the objective function to Constant
      parameters.SetDefault( "objective function", "Constant(direction=SmallerIsBetter)");

      if( m_UseDistanceMetric)
      {
        parameters.AddInitializer
        (
          "share distance metric",
          "If true, all nodes will use the same spline for computing applicability. This implies an assumption that the "
          "model has the most difficulty predicting things far from any node center, regardless of which node center it "
          "is. If False, all nodes will have their own distance metric, which is valid if the model is capable of "
          "distinguishing between classes of features (e.g. a kohonen map itself).",
          io::Serialization::GetAgent( &m_ShareDistanceMetric),
          "True"
        );
      }
      else
      {
        parameters.AddInitializer
        (
          "model",
          "The model to use to make predictions.",
          io::Serialization::GetAgent( &m_Retriever)
        );
        parameters.AddInitializer
        (
          "monotonic",
          "True if monotonicity should be enforced",
          io::Serialization::GetAgent( &m_Monotonize),
          "True"
        );
        parameters.AddInitializer
        (
          "measure",
          "Contingency matrix measure to compute spline over.",
          io::Serialization::GetAgent( &m_Measure)
        );
      }
      return parameters;
    }

    //! @brief produce the final model
    //! Updates m_FinalModel
    void ApproximatorApplicabilityDomainKohonen::MakeFinalModel()
    {
      m_FinalModel =
        util::ShPtr< KohonenNetworkApplicabilityDomain>
        (
          new KohonenNetworkApplicabilityDomain( m_Network, m_ShareDistanceMetric, m_Retriever)
        );

      if( m_UseDistanceMetric)
      {
        // fix the current approximation
        storage::Vector< sched::Mutex> empty_mutex_vector;
        ApproximatorKohonenNetwork::m_Network.GetWinningNodeIndices
        (
          *ApproximatorKohonenNetwork::m_TrainingData->GetFeaturesPtr(),
          math::Range< size_t>( 0, ApproximatorKohonenNetwork::m_Schedule.GetOrder().GetSize()),
          storage::Vector< size_t>(),
          ApproximatorKohonenNetwork::m_LastClosestNodes,
          empty_mutex_vector
        );
        m_FinalModel->SetupSplines
        (
          *m_TrainingData->GetFeaturesPtr(),
          ApproximatorKohonenNetwork::m_LastClosestNodes,
          m_ShareDistanceMetric
        );
        return;
      }

      BCL_Assert( m_Retriever.IsDefined(), "Must have a defined set of models to compute the applicability metrics for!");

      linal::MatrixConstReference< float> independent_results( m_ResultDataset->GetFeaturesReference());
      linal::MatrixConstReference< float> true_results( m_ResultDataset->GetResultsReference());
      const size_t n_results( independent_results.GetNumberCols());

      // winning nodes selection
      storage::Vector< size_t> last_closest_nodes;
      if( !m_ShareDistanceMetric)
      {
        storage::Vector< sched::Mutex> empty_mutex_vector;
        ApproximatorKohonenNetwork::m_Network.GetWinningNodeIndices
        (
          *m_TrainingData->GetFeaturesPtr(),
          math::Range< size_t>( 0, m_TrainingData->GetSize()),
          storage::Vector< size_t>(),
          last_closest_nodes,
          empty_mutex_vector
        );
      }

      const math::ContingencyMatrixMeasures measure
      (
        math::ContingencyMatrixMeasures::MeasureEnum( m_Measure->GetAlias())
      );

      const size_t n_nodes( this->m_Network.GetCodeBook().GetSize());
      // create ROC curves for each result, optionally per-node
      storage::Vector< storage::Vector< math::CubicSplineDamped> > splines
      (
        m_ShareDistanceMetric ? size_t( 1) : n_nodes,
        storage::Vector< math::CubicSplineDamped>( n_results)
      );
      m_FinalModel->SetupModels( m_Retriever);
      util::Implementation< ObjectiveFunctionInterface> objective
      (
        m_Retriever->RetrieveCommonCVInfo().GetObjective()
      );
      for( size_t result_n( 0); result_n < n_results; ++result_n)
      {
        storage::List< storage::Pair< double, double> > test_results;
        for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
        {
          test_results.PushBack
          (
            storage::Pair< double, double>( independent_results( i, result_n), true_results( i, result_n))
          );
        }
        storage::Map< double, math::ContingencyMatrix> roc_map
        (
          math::ROCCurve
          (
            test_results,
            objective->GetThreshold(),
            objective->GetRankingParity()
          ).ToMap()
        );

        const bool computing_local_ppv
        (
          m_Measure->GetAlias()
          == math::ContingencyMatrixMeasures::GetMeasureName( math::ContingencyMatrixMeasures::e_LocalPPV)
        );

        math::PiecewiseFunction local_ppv;
        if( computing_local_ppv)
        {
          local_ppv = math::ROCCurve
                      (
                        test_results,
                        objective->GetThreshold(),
                        objective->GetRankingParity()
                      ).GetLocalPPVCurve();
        }

        if( m_ShareDistanceMetric)
        {
          storage::Vector< storage::Pair< double, double> > dataset_roc_points;
          dataset_roc_points.AllocateMemory( m_TrainingData->GetSize());
          for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
          {
            dataset_roc_points.PushBack
            (
              storage::Pair< double, double>
              (
                independent_results( i, result_n),
                computing_local_ppv
                ? local_ppv( independent_results( i, result_n))
                : measure( roc_map[ independent_results( i, result_n)])
              )
            );
          }
          // sort ROC curve points
          dataset_roc_points.Sort( std::less< storage::Pair< double, double> >());

          math::RunningMinMax< double> min_max_y;
          for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
          {
            min_max_y += dataset_roc_points( i).Second();
          }
          const double gradation( min_max_y.GetRange() / std::min( size_t( 100), m_TrainingData->GetSize()));
          storage::Vector< double> selected_x, selected_y;
          // only select ROC points that are significantly different in PPV or whatever measure
          if( !m_Monotonize)
          {
            double last_y( -1000.0);
            for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
            {
              if( math::Absolute( dataset_roc_points( i).Second() - last_y) > gradation)
              {
                selected_x.PushBack( dataset_roc_points( i).First());
                selected_y.PushBack( dataset_roc_points( i).Second());
                last_y = dataset_roc_points( i).Second();
              }
            }
          }
          else if( objective->GetRankingParity() == measure.GetOptimizationParity())
          {
            double last_y( -1000.0);
            for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
            {
              if( ( dataset_roc_points( i).Second() - last_y) > gradation)
              {
                selected_x.PushBack( dataset_roc_points( i).First());
                selected_y.PushBack( dataset_roc_points( i).Second());
                last_y = dataset_roc_points( i).Second();
              }
            }
          }
          else
          {
            double last_y( 1000.0);
            for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
            {
              if( ( last_y - dataset_roc_points( i).Second()) > gradation)
              {
                selected_x.PushBack( dataset_roc_points( i).First());
                selected_y.PushBack( dataset_roc_points( i).Second());
                last_y = dataset_roc_points( i).Second();
              }
            }
          }
          splines( 0)( result_n).Train
          (
            linal::Vector< double>( selected_x.Begin(), selected_x.End()),
            linal::Vector< double>( selected_y.Begin(), selected_y.End())
          );
        }
        else
        {
          storage::Vector< storage::Vector< storage::Pair< double, double> > > dataset_roc_points
          (
            n_nodes,
            storage::Vector< storage::Pair< double, double> >
            (
              size_t( 1),
              storage::Pair< double, double>
              (
                roc_map.Begin()->first,
                computing_local_ppv
                ? local_ppv( roc_map.Begin()->first)
                : ( *m_Measure)( roc_map[ roc_map.Begin()->first])
              )
            )
          );
          storage::Vector< storage::Vector< storage::Pair< double, double> > > dataset_points
          (
            n_nodes
          );

          for( size_t i( 0), dataset_n( m_TrainingData->GetSize()); i < dataset_n; ++i)
          {
            dataset_points( last_closest_nodes( i)).PushBack
            (
              storage::Pair< double, double>
              (
                independent_results( i, result_n),
                true_results( i, result_n)
              )
            );
          }
          for( size_t node_n( 0); node_n < n_nodes; ++node_n)
          {
            // sort ROC curve points
            storage::Vector< storage::Pair< double, double> > &node_points( dataset_points( node_n));

            storage::List< storage::Pair< double, double> > node_points_list( node_points.Begin(), node_points.End());
            math::ROCCurve local_roc( node_points_list, objective->GetThreshold(), objective->GetRankingParity());
            math::PiecewiseFunction node_local_ppv( local_roc.GetLocalPPVCurve());
            storage::Map< double, math::ContingencyMatrix> local_roc_map( local_roc.ToMap());

            for( size_t i( 0), n_points( node_points.GetSize()); i < n_points; ++i)
            {
              dataset_roc_points( node_n).PushBack
              (
                storage::Pair< double, double>
                (
                  node_points( i).First(),
                  computing_local_ppv
                  ? node_local_ppv( node_points( i).First())
                  : ( *m_Measure)( local_roc_map[ node_points( i).First()])
                )
              );
            }
          }
          for( size_t node_n( 0); node_n < n_nodes; ++node_n)
          {
            // sort ROC curve points
            storage::Vector< storage::Pair< double, double> > &node_roc_points( dataset_roc_points( node_n));

            node_roc_points.Sort( std::less< storage::Pair< double, double> >());

            // only select ROC points that are significantly different in PPV or whatever measure
            storage::Vector< double> selected_x, selected_y;
            math::RunningMinMax< double> min_max_y;
            for( size_t i( 0), dataset_n( node_roc_points.GetSize()); i < dataset_n; ++i)
            {
              min_max_y += node_roc_points( i).Second();
            }
            const double gradation( ( min_max_y.GetRange() + 1.0e-5) / std::min( size_t( 1000), node_roc_points.GetSize()));
            // only select ROC points that are significantly different in PPV or whatever measure
            if( !m_Monotonize)
            {
              double last_y( -1000.0);
              for( size_t i( 0), dataset_n( node_roc_points.GetSize()); i < dataset_n; ++i)
              {
                if( math::Absolute( node_roc_points( i).Second() - last_y) > gradation)
                {
                  selected_x.PushBack( node_roc_points( i).First());
                  selected_y.PushBack( node_roc_points( i).Second());
                  last_y = node_roc_points( i).Second();
                }
              }
            }
            else if( objective->GetRankingParity() == measure.GetOptimizationParity())
            {
              double last_y( -1000.0);
              for( size_t i( 0), dataset_n( node_roc_points.GetSize()); i < dataset_n; ++i)
              {
                if( ( node_roc_points( i).Second() - last_y) > gradation)
                {
                  selected_x.PushBack( node_roc_points( i).First());
                  selected_y.PushBack( node_roc_points( i).Second());
                  last_y = node_roc_points( i).Second();
                }
              }
            }
            else
            {
              double last_y( 1000.0);
              for( size_t i( 0), dataset_n( node_roc_points.GetSize()); i < dataset_n; ++i)
              {
                if( ( last_y - node_roc_points( i).Second()) > gradation)
                {
                  selected_x.PushBack( node_roc_points( i).First());
                  selected_y.PushBack( node_roc_points( i).Second());
                  last_y = node_roc_points( i).Second();
                }
              }
            }
            splines( node_n)( result_n).Train
            (
              linal::Vector< double>( selected_x.Begin(), selected_x.End()),
              linal::Vector< double>( selected_y.Begin(), selected_y.End())
            );
          }
        }
      }
      m_FinalModel->SetupSplines( splines);
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_base.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    //! @detail sets the improvement type of the tracker to SmallerIsBetter
    ApproximatorBase::ApproximatorBase() :
      opti::ApproximatorModularBase< util::ShPtr< Interface>, float>( opti::s_NumberImprovementTypes),
      m_TrainingContinued( false),
      m_ObjectiveFunction( new ObjectiveFunctionWrapper)
    {
    }

    //! @brief construct from objective function
    //! @detail sets the improvement type of the tracker to SmallerIsBetter
    //! @param OBJECTIVE_FUNCTION objective function used to evaluate a monitoring data set
    ApproximatorBase::ApproximatorBase( const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION) :
      opti::ApproximatorModularBase< util::ShPtr< Interface>, float>
      (
        OBJECTIVE_FUNCTION.IsDefined()
        ? OBJECTIVE_FUNCTION->GetImprovementType()
        : opti::s_NumberImprovementTypes
      ),
      m_TrainingContinued( false),
      m_ObjectiveFunction
      (
        OBJECTIVE_FUNCTION.IsDefined()
        ? OBJECTIVE_FUNCTION
        : util::ShPtr< ObjectiveFunctionWrapper>( new ObjectiveFunctionWrapper)
      )
    {
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns a shared pointer to the training data set
    //! @return shared pointer to the data set interface with training data
    const util::ShPtr< descriptor::Dataset> &ApproximatorBase::GetTrainingData() const
    {
      return m_TrainingData;
    }

    //! @brief get objective function to evaluate a monitoring data set
    //! @return data set interface with training data
    const util::ShPtr< ObjectiveFunctionWrapper> &ApproximatorBase::GetObjectiveFunction() const
    {
      return m_ObjectiveFunction;
    }

    //! @brief sets the objective function used to evaluate a monitoring data set
    //! @param OBJECTIVE_FUNCTION objective function used to evaluate a monitoring data set
    void ApproximatorBase::SetObjectiveFunction( const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION)
    {
      m_ObjectiveFunction = OBJECTIVE_FUNCTION;
      this->GetTracker() = opti::Tracker< util::ShPtr< Interface>, float>( m_ObjectiveFunction->GetImprovementType());
    }

    //! @brief returns the flag for continuation of training
    //! @return flag for continuation of training
    bool ApproximatorBase::IsTrainingContinued() const
    {
      return m_TrainingContinued;
    }

    //! @brief sets the flag for continuation of training
    //! @param flag for continuation of training
    void ApproximatorBase::SetTrainingContinued( const bool &CONTINUED_TRAINING)
    {
      m_TrainingContinued = CONTINUED_TRAINING;
    }

    //! @brief returns a shared pointer to the rescale function to transfer data sets into a scaled feature space
    //! @return shared pointer to rescale function to rescale into normalized feature space
    util::ShPtr< RescaleFeatureDataSet> ApproximatorBase::GetRescaleFeatureDataSet() const
    {
      return
        m_TrainingData.IsDefined()
        ? m_TrainingData->GetFeaturesPtr()->GetScaling()
        : util::ShPtr< RescaleFeatureDataSet>();
    }

    //! @brief returns a shared pointer to the rescale function to transfer data sets into a scaled result space
    //! @return shared pointer to the rescale function to rescale into normalized result space
    util::ShPtr< RescaleFeatureDataSet> ApproximatorBase::GetRescaleResultDataSet() const
    {
      return
        m_TrainingData.IsDefined()
        ? m_TrainingData->GetResultsPtr()->GetScaling()
        : util::ShPtr< RescaleFeatureDataSet>();
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ApproximatorBase::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERR_STREAM)
    {
      if( m_ObjectiveFunction.IsDefined())
      {
        this->GetTracker() = opti::Tracker< util::ShPtr< Interface>, float>( m_ObjectiveFunction->GetImprovementType());
      }
      return true;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorBase::Read( std::istream &ISTREAM)
    {
      // call Read of base class
      opti::ApproximatorModularBase< util::ShPtr< Interface>, float>::Read( ISTREAM);

      // All members of ApproximatorBase are intended for being reset whenever the object is read back in for
      // continuation, since the purpose of reading it back in may be to change the training data or objective function
      m_TrainingContinued = true;

      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorBase::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // call Write of base class
      opti::ApproximatorModularBase< util::ShPtr< Interface>, float>::Write( OSTREAM, INDENT) << '\n';

      // All members of ApproximatorBase are intended for being reset whenever the object is read back in for
      // continuation, since the purpose of reading it back in may be to change the training data or objective function

      // end
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_decision_tree.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorDecisionTree::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorDecisionTree())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ApproximatorDecisionTree::ApproximatorDecisionTree() :
      m_DataPartitioner(),
      m_ActivityCutoff(),
      m_UnexpandedSubTrees(),
      m_DecisionTree( new DecisionTree()),
      m_MinIncorrect( 0)
    {
    }

    //! @brief constructor from all necessary parameters
    //! @param DATA_PARTITIONER ShPtr to DtreeDataPartitionFunctionInterface
    //! @param OBJECTIVE_FUNCTION ShPtr to objective function
    //! @param ACTIVITY_CUTOFF the cutoff for binary classification
    //! @param TRAINING_DATA the dataset used to build the tree
    ApproximatorDecisionTree::ApproximatorDecisionTree
    (
      const util::Implementation< DtreeDataPartitionFunctionInterface> &DATA_PARTITIONER,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const float ACTIVITY_CUTOFF,
      const util::ShPtr< descriptor::Dataset> &TRAINING_DATA
    ) :
      ApproximatorBase( OBJECTIVE_FUNCTION),
      m_DataPartitioner( DATA_PARTITIONER),
      m_ActivityCutoff( ACTIVITY_CUTOFF),
      m_PartitionScoreType( DtreeBinaryPartition::e_SplitRating),
      m_UnexpandedSubTrees(),
      m_DecisionTree( new DecisionTree()),
      m_MinIncorrect( 0)
    {
      if( TRAINING_DATA.IsDefined())
      {
        util::ShPtr< descriptor::Dataset> training_data( TRAINING_DATA);
        SetTrainingData( training_data);
      }
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorDecisionTree::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorDecisionTree::GetAlias() const
    {
      static const std::string s_Name( "DecisionTree");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorDecisionTree::SetTrainingData( util::ShPtr< descriptor::Dataset> &DATA)
    {
      m_TrainingData = DATA;

      //! set up processed data set for future use, contains bools for activities
      util::ShPtr< storage::Vector< FeatureResultAndState> > processed_dataset
      (
        CreateDataSetReferences
        (
          m_TrainingData->GetFeaturesPtr()->GetMatrix(),
          m_TrainingData->GetResultsPtr()->GetMatrix()
        )
      );

      m_UnexpandedSubTrees.Reset();

      // determine the root node's activity and add it as the next node to expand.
      AddExpandableNode( m_DecisionTree, processed_dataset);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorDecisionTree::GetCurrentModel() const
    {
      if( m_UnexpandedSubTrees.IsEmpty())
      {
        // no more iteration to do, so just return the final decision tree
        return m_DecisionTree;
      }

      // more iteration to do, need to deeply clone the tree
      util::ShPtr< DecisionTree> deep_cloned_tree( m_DecisionTree->DeepClone());
      Prune( *deep_cloned_tree);
      return deep_cloned_tree;
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorDecisionTree::GetCurrentApproximation() const
    {
      util::ShPtr< Interface> model( GetCurrentModel());

      // return the model & objective function value
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( model, m_ObjectiveFunction->operator()( model))
        );
    }

    //! @brief Prune a decision tree model based on size & cutoff (if applicable)
    //! @param MODEL the decision tree to prune
    void ApproximatorDecisionTree::Prune( DecisionTree &MODEL) const
    {
      // prune the tree if the objective function is a strict cutoff-based objective function
      if( m_ObjectiveFunction->GetGoalType() == ObjectiveFunctionInterface::e_Classification)
      {
        MODEL.Prune( m_ActivityCutoff, m_MinIncorrect);
      }
      else
      {
        // otherwise, just prune by size
        MODEL.Prune( util::GetUndefined< double>(), m_MinIncorrect);
      }
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorDecisionTree::Next()
    {
      // if there are still unexpanded nodes left
      if( !m_UnexpandedSubTrees.IsEmpty())
      {
        // expand the node with the best partition rating
        storage::Triplet
        <
          util::ShPtr< DecisionTree>,                             // the tree to be expanded
          util::ShPtr< storage::Vector< FeatureResultAndState> >, // references of feature results to expand
          DtreeBinaryPartition                                    // best binary partition for this dataset
        > best_partition_info( m_UnexpandedSubTrees.LastElement());

        // remove that element from the unexpanded subtrees list
        m_UnexpandedSubTrees.PopBack();

        // expand the given nodee_Debug
        Expand( best_partition_info.First(), best_partition_info.Second(), best_partition_info.Third());

        if( m_UnexpandedSubTrees.IsEmpty())
        {
          // last node was just expanded, prune the tree, if necessary
          Prune( *m_DecisionTree);
        }
      }

      this->GetTracker().Track( GetCurrentApproximation());
    } // Next

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorDecisionTree::Read( std::istream &ISTREAM)
    {
      // read members
      ApproximatorBase::Read( ISTREAM);
      io::Serialize::Read( m_DataPartitioner, ISTREAM);
      io::Serialize::Read( m_ActivityCutoff, ISTREAM);
      io::Serialize::Read( m_PartitionScoreType, ISTREAM);
      io::Serialize::Read( m_UnexpandedSubTrees, ISTREAM);
      io::Serialize::Read( m_DecisionTree, ISTREAM);
      io::Serialize::Read( m_TrainingData, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorDecisionTree::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      ApproximatorBase::Write( OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_DataPartitioner, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ActivityCutoff, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_PartitionScoreType, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_UnexpandedSubTrees, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_DecisionTree, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_TrainingData, OSTREAM, INDENT);
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorDecisionTree::CanContinue() const
    {
      return !m_UnexpandedSubTrees.IsEmpty();
    }

    //! @brief constructs children nodes, adds them to current node, and queues children for visiting
    //! @param CURRENT_NODE a ShPtr to the node to expand
    //! @param DATA data from the parent node
    //! @param BINARY_PARTITION partition to use
    void ApproximatorDecisionTree::Expand
    (
      util::ShPtr< DecisionTree> &CURRENT_NODE,
      const util::ShPtr< storage::Vector< FeatureResultAndState> > &DATA,
      const DtreeBinaryPartition &BINARY_PARTITION
    )
    {
      //! makes the new node for the returned decision function/dataset pair and sets the activity for the newly
      //! created node based on the accompanying dataset
      util::ShPtr< DecisionTree> left_tree_sp( new DecisionTree()), right_tree_sp( new DecisionTree());

      // partition the dataset into the data that would be attached to the left node and data that would be attached
      // to the right node
      util::ShPtr< storage::Vector< FeatureResultAndState> > left_data_set( new storage::Vector< FeatureResultAndState>);
      util::ShPtr< storage::Vector< FeatureResultAndState> > right_data_set( new storage::Vector< FeatureResultAndState>);

      const size_t split_index( BINARY_PARTITION.GetFeatureIndex());
      const float split_value( BINARY_PARTITION.GetSplitValue());

      // get a reference to the data
      const storage::Vector< FeatureResultAndState> &data( *DATA);

      //! calculates the data set size once
      const size_t total_data_set_elements( data.GetSize());

      BCL_MessageVrb
      (
        "Splitting index: " + util::Format()( split_index)
        + " total elements: " + util::Format()( total_data_set_elements)
        + " split ranking: " + util::Format()( BINARY_PARTITION.GetSplitRating())
      );

      //! Goes through the dataset and partitions it into the right and left datasets as appropriate
      for( size_t dataset_counter( 0); dataset_counter < total_data_set_elements; ++dataset_counter)
      {
        if( data( dataset_counter).GetFeature()( split_index) <= split_value)
        {
          left_data_set->PushBack( data( dataset_counter));
        }
        else
        {
          right_data_set->PushBack( data( dataset_counter));
        }
      }

      // if either partitions are empty, then there is no point in expanding the tree, so return
      if( left_data_set->IsEmpty() || right_data_set->IsEmpty())
      {
        return;
      }

      CURRENT_NODE->ConfigureBranch( split_index, split_value, left_tree_sp, right_tree_sp);

      if( BINARY_PARTITION.GetFinalNumIncorrect() >= m_MinIncorrect)
      {
        AddExpandableNode( left_tree_sp, left_data_set);
        AddExpandableNode( right_tree_sp, right_data_set);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorDecisionTree::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Trains a decision tree using an arbitrary method of choosing which feature index to partition next"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &GetObjectiveFunction()->GetImplementation()),
        "Accuracy"
      );

      parameters.AddInitializer
      (
        "partitioner",
        "method of deciding which component of the feature vector to use to add a new node to the decision tree",
        io::Serialization::GetAgent( &m_DataPartitioner),
        "InformationGain"
      );

      parameters.AddInitializer
      (
        "activity cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoff),
        "0.5"
      );
      parameters.AddInitializer
      (
        "node score",
        "choice of methods by which to pick the next node to expand",
        io::Serialization::GetAgent( &m_PartitionScoreType),
        "SplitRating"
      );
      parameters.AddInitializer
      (
        "min split",
        "Minimum number of incorrect classifications in a node that will be split",
        io::Serialization::GetAgent( &m_MinIncorrect),
        "0"
      );

      return parameters;
    }

    //! @brief converts an original training data set into an already classified version based on a given cutoff
    //! @param FEATURES original features of interest
    //! @param RESULTS corresponding results
    //! @return pre-processed data set
    util::ShPtr< storage::Vector< FeatureResultAndState> > ApproximatorDecisionTree::CreateDataSetReferences
    (
      const linal::MatrixConstInterface< float> &FEATURES,
      const linal::MatrixConstInterface< float> &RESULTS
    )
    {
      const size_t results_size( RESULTS.GetNumberCols());
      const size_t features_size( FEATURES.GetNumberCols());
      const size_t dataset_size( RESULTS.GetNumberRows());

      // setup the result state matrix
      m_ResultStates = linal::Matrix< size_t>( dataset_size, results_size);

      // create a preprocessed dataset from original dataset format
      util::ShPtr< storage::Vector< FeatureResultAndState> > sp_converted_dataset
      (
        new storage::Vector< FeatureResultAndState>( dataset_size)
      );
      storage::Vector< FeatureResultAndState> &converted_dataset( *sp_converted_dataset);
      storage::Vector< math::RunningAverage< float> > averages_above( results_size);
      storage::Vector< math::RunningAverage< float> > averages_below( results_size);

      // calculates the data set size once
      //!set up preprocessed data set for future use, contains bools for activities
      for( size_t dataset_counter( 0); dataset_counter < dataset_size; ++dataset_counter)
      {
        const float *results_row( RESULTS[ dataset_counter]);
        size_t *result_states_row( m_ResultStates[ dataset_counter]);
        // setup the row of m_ResultStates first
        for( size_t result_counter( 0); result_counter < results_size; ++result_counter)
        {
          result_states_row[ result_counter] = results_row[ result_counter] <= m_ActivityCutoff;
          if( result_states_row[ result_counter])
          {
            averages_below( result_counter) += results_row[ result_counter];
          }
          else
          {
            averages_above( result_counter) += results_row[ result_counter];
          }
        }

        converted_dataset( dataset_counter) =
          FeatureResultAndState
          (
            FeatureReference< float>(  features_size, FEATURES[ dataset_counter]),
            FeatureReference< float>(  results_size,  results_row),
            FeatureReference< size_t>( results_size,  m_ResultStates[ dataset_counter])
          );
      }

      m_AverageAboveCutoffSlopes = linal::Vector< float>( results_size, 0.0);
      m_AverageBelowCutoffSlopes = linal::Vector< float>( results_size, 0.0);
      for( size_t result_counter( 0); result_counter < results_size; ++result_counter)
      {
        m_AverageAboveCutoffSlopes( result_counter) = averages_above( result_counter).GetAverage() - m_ActivityCutoff;
        m_AverageBelowCutoffSlopes( result_counter) = m_ActivityCutoff - averages_below( result_counter).GetAverage();
      }

      // return preprocessed dataset
      return sp_converted_dataset;
    }

    //! @brief add a node, determines average activity, etc., sorts it into the list of nodes to expand
    //! @param NODE the node to add
    //! @param DATA the data that maps to that node
    void ApproximatorDecisionTree::AddExpandableNode
    (
      util::ShPtr< DecisionTree> NODE,
      const util::ShPtr< storage::Vector< FeatureResultAndState> > &DATA
    )
    {
      // for empty dataset, just reset the activity level of the tree
      if( DATA->IsEmpty())
      {
        NODE->SetActivity( storage::Vector< math::RunningAverage< float> >());
        return;
      }

      // determine # of results/outputs
      const size_t results_size( DATA->FirstElement().GetResult().GetSize());

      if
      (
        m_ObjectiveFunction->GetGoalType() != ObjectiveFunctionInterface::e_Classification
        && m_ObjectiveFunction->GetGoalType() != ObjectiveFunctionInterface::e_RankClassification
      )
      {
        // a regression (or other) objective function type.  Set the activity up to be the actual average
        // set up the activity to the raw average; this is the optimal value for regression-style objective functions

        storage::Vector< math::RunningAverage< float> > average_values( results_size);

        for
        (
          storage::Vector< FeatureResultAndState>::const_iterator itr( DATA->Begin()), itr_end( DATA->End());
          itr != itr_end;
          ++itr
        )
        {
          // get a reference on the result and its state
          const FeatureReference< float> &result_ref( itr->GetResult());
          for( size_t result_number( 0); result_number < results_size; ++result_number)
          {
            average_values( result_number) += result_ref( result_number);
          }
        }

        NODE->SetActivity( average_values);
      }
      else
      {
        // sum up the total in each state
        linal::Vector< size_t> state_counts( results_size, size_t( 0));
        const size_t number_results( DATA->GetSize());
        for
        (
          storage::Vector< FeatureResultAndState>::const_iterator itr( DATA->Begin()), itr_end( DATA->End());
          itr != itr_end;
          ++itr
        )
        {
          const FeatureReference< size_t> &state_ref( itr->GetResultState());
          for( size_t result_number( 0); result_number < results_size; ++result_number)
          {
            state_counts( result_number) += state_ref( result_number);
          }
        }

        // determine the results average value for categorical values such that the value reflects the purity of the node
        // That is, the there are exactly as many counts in state x as not in state x, use the cutoff
        // otherwise use the slopes and purity to give a value between cutoff and the average result value of all states
        // that far away from the cutoff according to the following function:
        // ave_state( x) = state_counts( x) / size
        // f(x) = cutoff + ( 1 - 2 * ave_state( x)) * m_AverageBelowCutoffSlopes( x) if ave_state( x) >= 0.5
        // else, f( x) = cutoff + ( 1 - 2 * ave_state( x)) * m_AverageAbvoveCutoffSlopes( x) if ave_state( x) < 0.5

        storage::Vector< math::RunningAverage< float> > activities( results_size);

        for( size_t result_number( 0); result_number < results_size; ++result_number)
        {
          const float ave_state( float( state_counts( result_number)) / float( number_results));
          float value( m_ActivityCutoff);
          if( ave_state >= 0.5)
          {
            value += ( 1.0 - 2.0 * ave_state) * m_AverageBelowCutoffSlopes( result_number);
          }
          else
          {
            value += ( 1.0 - 2.0 * ave_state) * m_AverageAboveCutoffSlopes( result_number);
          }
          activities( result_number).AddWeightedObservation( value, double( number_results));
        }

        // set the average activity for the node
        NODE->SetActivity( activities);
      }

      // determine the best partition of the data
      DtreeBinaryPartition node_partition( m_DataPartitioner->operator()( *DATA));

      // only add the partition if it was fully defined
      if
      (
        util::IsDefined( node_partition.GetFeatureIndex())
        && util::IsDefined( node_partition.GetSplitValue())
        && util::IsDefined( node_partition.GetSplitRating())
        && node_partition.GetInitialNumIncorrect() >= m_MinIncorrect
      )
      {
        // store the partition's rating for easy access
        const float partiton_rating( node_partition.GetData( m_PartitionScoreType));

        storage::Triplet
        <
          util::ShPtr< DecisionTree>,                             // the tree to be expanded
          util::ShPtr< storage::Vector< FeatureResultAndState> >, // references of feature results to expand
          DtreeBinaryPartition                                    // best binary partition for this dataset
        > new_node_to_expand( NODE, DATA, node_partition);

        // determine where to insert this node in the nodes to be partitioned by sorting it into the list based
        // on the split rating.  Always put the node with the highest rating last, so we can just always pop the last
        // member of the list off in operator()
        storage::List
        <
          storage::Triplet
          <
            util::ShPtr< DecisionTree>,                             // the tree to be expanded
            util::ShPtr< storage::Vector< FeatureResultAndState> >, // references of feature results to expand
            DtreeBinaryPartition                                    // best binary partition for this dataset
          >
        >::iterator itr_list( m_UnexpandedSubTrees.Begin()), itr_list_end( m_UnexpandedSubTrees.End());

        // determine a secondary partition rating as a tie-breaker
        const DtreeBinaryPartition::Data tie_breaker_score_type
        (
          m_PartitionScoreType == DtreeBinaryPartition::e_InitialNumIncorrect
          || m_PartitionScoreType == DtreeBinaryPartition::e_InitialIncorrectPlusFinalCorrect
          ? DtreeBinaryPartition::e_SplitRating
          : DtreeBinaryPartition::e_InitialNumIncorrect
        );

        const double tie_breaker_score( node_partition.GetData( tie_breaker_score_type));

        while
        (
          itr_list != itr_list_end
          && partiton_rating >= itr_list->Third().GetData( m_PartitionScoreType)
        )
        {
          if( partiton_rating == itr_list->Third().GetData( m_PartitionScoreType))
          {
            if( tie_breaker_score <= itr_list->Third().GetData( tie_breaker_score_type))
            {
              break;
            }
          }
          ++itr_list;
        }

        // insert the new node to expand
        m_UnexpandedSubTrees.InsertElement( itr_list, new_node_to_expand);
      }

    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_kappa_nearest_neighbor.h"

// includes from bcl - sorted alphabetically
#include "model/bcl_model_kappa_nearest_neighbor.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorKappaNearestNeighbor::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorKappaNearestNeighbor())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ApproximatorKappaNearestNeighbor::ApproximatorKappaNearestNeighbor() :
      ApproximatorBase(),
      m_MinKappa( 1),
      m_Kappa( 1),
      m_MaxKappa( 10),
      m_LastObjectiveFunctionResult( util::GetUndefined< float>())
    {
    }

    //! @brief clone function
    //! @return pointer to new ApproximatorKappaNearestNeighbor
    ApproximatorKappaNearestNeighbor *ApproximatorKappaNearestNeighbor::Clone() const
    {
      return new ApproximatorKappaNearestNeighbor( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief sets the training data
    //! @param DATA training data set to be set
    void ApproximatorKappaNearestNeighbor::SetTrainingData( util::ShPtr< descriptor::Dataset> &DATA)
    {
      ApproximatorBase::m_TrainingData = DATA;
      BCL_Assert( !DATA->IsEmpty(), "no training data loaded");

      // rescale the input features
      ApproximatorBase::m_TrainingData->GetFeatures().Rescale( KappaNearestNeighbor::s_DefaultInputRange);

      BCL_Message
      (
        util::Message::e_Standard,
        "Setting up training data with " + util::Format()( ApproximatorBase::m_TrainingData->GetSize()) + " points"
      );
    }

    //! @brief returns the current model
    //! @return current model
    util::ShPtr< Interface> ApproximatorKappaNearestNeighbor::GetCurrentModel() const
    {
      return util::ShPtr< Interface>( new KappaNearestNeighbor( this->m_TrainingData, m_Kappa));
    }

    //! @brief returns the current approximation
    //! @return ShPtr to the current argument result pair
    const util::ShPtr
    <
      storage::Pair< util::ShPtr< Interface>, float>
    > ApproximatorKappaNearestNeighbor::GetCurrentApproximation() const
    {
      // create ShPtr to the model
      util::ShPtr< Interface> sp_model( new KappaNearestNeighbor( this->m_TrainingData, m_Kappa));

      // return ShPtr to model result pair
      return util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      (
        new storage::Pair< util::ShPtr< Interface>, float>
        (
          sp_model,
          m_ObjectiveFunction->operator()( sp_model)
        )
      );
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorKappaNearestNeighbor::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorKappaNearestNeighbor::GetAlias() const
    {
      static const std::string s_alias( "KappaNearestNeighbor");
      return s_alias;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorKappaNearestNeighbor::CanContinue() const
    {
      return m_Kappa < m_MaxKappa || !util::IsDefined( m_LastObjectiveFunctionResult);
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorKappaNearestNeighbor::Next()
    {
      // assert when there is no interval
      BCL_Assert( this->m_TrainingData.IsDefined() && !this->m_TrainingData->IsEmpty(), "no training data given!");

      // ShPtr to the model
      util::ShPtr< Interface> model;

      // check for first round
      if( !util::IsDefined( m_LastObjectiveFunctionResult))
      {
        m_Kappa = m_MinKappa;
        model = GetCurrentApproximation()->First();
        m_LastObjectiveFunctionResult = m_ObjectiveFunction->operator()( model);

        BCL_Message
        (
          util::Message::e_Standard,
          "Kappa: " + util::Format()( m_Kappa) + " ObjFunction: "
          + util::Format()( m_LastObjectiveFunctionResult)
        );
      }
      else if( m_Kappa < m_MaxKappa)
      {
        ++m_Kappa;
        model = GetCurrentApproximation()->First();
        m_LastObjectiveFunctionResult = m_ObjectiveFunction->operator()( model);
        BCL_Message
        (
          util::Message::e_Standard,
          "Kappa: " + util::Format()( m_Kappa) + " ObjFunction: "
          + util::Format()( m_LastObjectiveFunctionResult)
        );
      }
      else
      {
        model = GetCurrentApproximation()->First();
      }

      // track the current model
      this->Track
      (
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>
          (
            model,
            m_LastObjectiveFunctionResult
          )
        )
      );
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorKappaNearestNeighbor::Read( std::istream &ISTREAM)
    {
      // call Read of base class
      ApproximatorBase::Read( ISTREAM);

      // read members
      io::Serialize::Read( m_MinKappa, ISTREAM);
      io::Serialize::Read( m_Kappa, ISTREAM);
      io::Serialize::Read( m_MaxKappa, ISTREAM);
      io::Serialize::Read( m_LastObjectiveFunctionResult, ISTREAM);

      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorKappaNearestNeighbor::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // call Write of base class
      ApproximatorBase::Write( OSTREAM, INDENT) << '\n';

      // write members
      io::Serialize::Write( m_MinKappa, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Kappa, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_MaxKappa, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_LastObjectiveFunctionResult, OSTREAM, INDENT) << '\n';

      // end
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorKappaNearestNeighbor::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "A k-nearest-neighbor predictor; iteration optimizes k. see http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &ApproximatorBase::m_ObjectiveFunction->GetImplementation()),
        "RMSD"
      );
      parameters.AddInitializer
      (
        "min kappa",
        "minimum # of nearest neighbors for kNN selection",
        io::Serialization::GetAgentWithMin( &m_MinKappa, size_t( 1)),
        "1"
      );
      parameters.AddInitializer
      (
        "max kappa",
        "maximum # of nearest neighbors for kNN selection",
        io::Serialization::GetAgentWithMin( &m_MaxKappa, size_t( 1)),
        "10"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_kohonen_network.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_data_set_reduced_to_k_means.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_tertiary_function_job_with_data.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorKohonenNetwork::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorKohonenNetwork())
    );

    //! @brief Kernel as string
    //! @param KERNEL the kernel
    //! @return the string for the kernel
    const std::string &ApproximatorKohonenNetwork::GetKernelName( const ApproximatorKohonenNetwork::NeighborKernel &KERNEL)
    {
      static const std::string s_Names[] =
      {
        "Gaussian",
        "Bubble",
        GetStaticClassName< ApproximatorKohonenNetwork::NeighborKernel>()
      };
      return s_Names[ size_t( KERNEL)];
    }

    //! @brief Initializer as string
    //! @param INITIALIZER the initialization
    //! @return the string for the initializer
    const std::string &ApproximatorKohonenNetwork::GetInitializationName( const Initialization &INITIALIZER)
    {
      static const std::string s_Names[] =
      {
        "FirstVectors",           // Initial vectors taken from the start of the training data
        "RandomlyChosenVectors",  // Initial vectors randomly chosen from the training data
        "RandomlyChosenElements", // Initial vectors composed of randomly chosen values for each element of the vector
        "KMeans",
        GetStaticClassName< ApproximatorKohonenNetwork::Initialization>()
      };
      return s_Names[ size_t( INITIALIZER)];
    }

    //! @brief default constructor
    ApproximatorKohonenNetwork::ApproximatorKohonenNetwork() :
      m_Network(),
      m_Length( 1),
      m_Radius( 0.0),
      m_CurrentRadius( 0.0),
      m_UpdateEveryNthFeature( 0),
      m_NeighborKernel( e_Gaussian),
      m_Initializer( e_FirstVectors),
      m_NoTrack( false),
      m_RescaleType( RescaleFeatureDataSet::e_MinMax),
      m_Cutoff( 0.5)
    {
    }

    //! @brief constructor from all necessary parameters
    //! @param MAP_DIMENSIONS dimensions of the map
    //! @param INITIAL_LENGTH how many iterations to train for (if applicable).
    //! @param INITAL_RADIUS the initial neighborhood radius
    //! @param OBJECTIVE_FUNCTION the objective function from the approximator framework
    //! @param UPDATE_EVERY_NTH_FEATURE update the nodes after seeing this many features
    //! @param NEIGHBOR_KERNEL the neighbor kernel type
    ApproximatorKohonenNetwork::ApproximatorKohonenNetwork
    (
      const linal::Vector< double> &MAP_DIMENSIONS,
      const size_t &INITIAL_LENGTH,
      const float &INITAL_RADIUS,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const size_t &UPDATE_EVERY_NTH_FEATURE,
      const NeighborKernel &NEIGHBOR_KERNEL
    ) :
      ApproximatorBase( OBJECTIVE_FUNCTION),
      m_Network( MAP_DIMENSIONS),
      m_Length( INITIAL_LENGTH),
      m_Radius( INITAL_RADIUS),
      m_CurrentRadius( INITAL_RADIUS),
      m_UpdateEveryNthFeature( UPDATE_EVERY_NTH_FEATURE),
      m_NeighborKernel( NEIGHBOR_KERNEL),
      m_Initializer( e_FirstVectors),
      m_NoTrack( false),
      m_RescaleType( RescaleFeatureDataSet::e_MinMax),
      m_Cutoff( 0.5)
    {
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorKohonenNetwork::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorKohonenNetwork::GetAlias() const
    {
      static const std::string s_alias( "Kohonen");
      return s_alias;
    }

    //! @brief set training data set for a specific iterate in approximator framework
    //! @param DATA training data set
    void ApproximatorKohonenNetwork::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_CurrentRadius = m_Radius;
      m_CurrentIteration = 0;
      BCL_Assert( !DATA->IsEmpty(), "no training data loaded");

      const size_t dataset_size( DATA->GetResultsPtr()->GetNumberFeatures());
      m_TrainingData = DATA;
      m_TrainingData->GetFeatures().Rescale( math::Range< float>( 0.0, 1.0), m_RescaleType);

      BCL_MessageStd( "Setting up training data with " + util::Format()( DATA->GetSize()) + " points");

      // Setup the initial network's positions
      m_Network = KohonenNetworkAverage( m_Network.GetMapDimensions(), GetRescaleFeatureDataSet());

      m_Schedule.Setup( DATA->GetResults(), m_Cutoff);

      // choose the initial feature vectors
      m_Network.InitializeNodes( *m_TrainingData);
      if( m_Initializer == e_FirstVectors)
      {
        // map is already correctly initialized
      }
      else if( m_Initializer == e_RandomlyChosenElements || m_Initializer == e_RandomlyChosenVectors)
      {
        // create a new feature dataset with randomly selected vectors or elements
        // determine the # of nodes in the code book
        const size_t map_size( m_Network.GetCodeBook().GetSize());

        // Because the results are not used for node assignment, their initial values do not matter, so long
        // as they are the right length, thus, the initial results can always be taken from the first N vectors, where
        // N is the map size
        linal::Matrix< float> first_results( m_TrainingData->GetResultsPtr()->GetMatrix( 0, map_size));

        // determine the size of each feature
        const size_t feature_size( m_TrainingData->GetFeatureSize());

        // create a matrix to store the selected features
        linal::Matrix< float> selected_features( map_size, feature_size);

        if( m_Initializer == e_RandomlyChosenVectors)
        {
          // copy random vectors from the training data into each row of the selected features
          for( size_t node_id( 0); node_id < map_size; ++node_id)
          {
            // choose the training data point index to use
            const size_t chosen_feature_index( random::GetGlobalRandom().Random( dataset_size - 1));

            // get a feature reference to that feature
            const FeatureReference< float> chosen_feature
            (
              m_TrainingData->GetFeaturesPtr()->operator()( chosen_feature_index)
            );

            // copy that feature into the selected features matrix
            std::copy( chosen_feature.Begin(), chosen_feature.End(), selected_features[ node_id]);
          }
        }
        else // if( m_Initializer == e_RandomlyChosenElements)
        {
          // copy random elements from the training data into each row of the selected features
          for( size_t node_id( 0); node_id < map_size; ++node_id)
          {
            for( size_t element_id( 0); element_id < feature_size; ++element_id)
            {
              // choose the training data point index to use
              const size_t chosen_feature_index( random::GetGlobalRandom().Random( dataset_size - 1));

              // get a feature reference to that feature
              const FeatureReference< float> chosen_feature
              (
                m_TrainingData->GetFeaturesPtr()->operator()( chosen_feature_index)
              );

              // copy that feature into the element of the selected feature matrix
              selected_features( node_id, element_id) = chosen_feature( element_id);
            }
          }
        }

        // create a new feature data set with those matrices
        const descriptor::Dataset seed_positions( selected_features, first_results);

        // use that data set to set up the initial features
        m_Network.InitializeNodes( seed_positions);
      }
      else if( m_Initializer == e_KMeans)
      {
        // create a new feature dataset with randomly selected vectors or elements
        // determine the # of nodes in the code book
        const size_t map_size( m_Network.GetCodeBook().GetSize());
        DataSetReducedToKMeans kmeans( map_size, 4 * map_size, false);
        m_Network.InitializeNodes( *kmeans( DATA, m_Schedule));
      }

      size_t features_per_update( m_UpdateEveryNthFeature);
      // if the m_UpdateEveryNthFeature was 0, then just use the data set size
      if( m_UpdateEveryNthFeature == size_t( 0))
      {
        features_per_update = m_Schedule.GetSize();
      }

      // set up the training ranges for each thread
      m_DataSetRanges.Reset();

      // calculate the # of weigh updates per run through the data set
      const size_t number_epochs( m_Schedule.GetSize() / features_per_update);

      m_DataSetRanges.AllocateMemory( number_epochs);

      // if m_TrainingData->GetSize() is not a multiple of features_per_update, distribute the extra features
      // over the initial number_epochs_with_extra_feature epochs
      const size_t number_epochs_with_extra_feature( m_Schedule.GetSize() % features_per_update);

      for( size_t epoch_number( 0), end_index( 0); epoch_number < number_epochs; ++epoch_number)
      {
        const size_t start_index( end_index);

        // add the # of features that will be examined in this epoch
        end_index += features_per_update + size_t( epoch_number < number_epochs_with_extra_feature);

        m_DataSetRanges.PushBack
        (
          math::Range< size_t>
          (
            math::RangeBorders::e_LeftClosed,
            start_index,
            end_index,
            math::RangeBorders::e_RightOpen
          )
        );
      }

      SetupThreadRanges();

      m_LastClosestNodes.Resize( dataset_size);
      m_LastClosestNodes.SetAllElements( 0);
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorKohonenNetwork::GetCurrentModel() const
    {
      return util::ShPtr< Interface>( m_Network.Clone());
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > ApproximatorKohonenNetwork::GetCurrentApproximation() const
    {
      util::ShPtr< Interface> new_network( m_Network.Clone());
      util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > return_value
      (
        new storage::Pair< util::ShPtr< Interface>, float>( new_network, m_ObjectiveFunction->operator ()( new_network))
      );
      return return_value;
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorKohonenNetwork::Next()
    {
      m_Schedule.Next();
      // assert when there is no interval
      BCL_Assert( m_TrainingData.IsDefined() && !m_TrainingData->IsEmpty(), "no training data given!");

      // stay at the last point if length is reached
      if( m_CurrentRadius < 0.4)
      {
        m_CurrentIteration = m_Length - 1;
      }
      else
      {
        m_CurrentRadius = std::max( m_CurrentRadius * ( 1.0 - m_CurrentIteration / ( 4.0 * m_Length)), 0.4);
      }

      util::ShPtrVector< sched::JobInterface> all_jobs( m_NumberThreadsNodes);

      const size_t group_id( 0);

      // store the id of each thread for use by the jobs
      storage::Vector< size_t> thread_ids( m_NumberThreadsNodes);
      for( size_t thread_number( 0); thread_number < m_NumberThreadsNodes; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
      }

      // create a const reference to the features of the dataset
      const FeatureDataSetInterface< float> &features( *m_TrainingData->GetFeaturesPtr());

      // if the training schedule includes balancing, it is necessary to have a mutex to protect from
      // concurrent access to m_PreviousWinners
      if( m_Schedule.IsBalanced())
      {
        m_Muteces.Resize( features.GetNumberFeatures());
      }

      // run through all the data once, updating nodes after every m_UpdateEveryNthFeature features
      for
      (
        size_t epoch_number( 0), number_epochs( m_DataSetRanges.GetSize());
        epoch_number < number_epochs;
        ++epoch_number
      )
      {
        // get the winning indices for this segment of the data set (threaded)
        m_Network.GetWinningNodeIndices
        (
          features,
          m_DataSetRanges( epoch_number),
          m_Schedule.GetOrder(),
          m_LastClosestNodes,
          m_Muteces
        );

        // run through the data set once, updating nodes after every epoch (m_UpdateEveryNthFeature features, data set size by default)
        for( size_t thread_number( 0); thread_number < m_NumberThreadsNodes; ++thread_number)
        {
          all_jobs( thread_number)
            = util::ShPtr< sched::JobInterface>
              (
                new sched::TertiaryFunctionJobWithData
                <
                  const size_t,
                  const size_t,
                  const size_t,
                  void,
                  ApproximatorKohonenNetwork
                >
                (
                  group_id,
                  *this,
                  &ApproximatorKohonenNetwork::TrainThread,
                  thread_ids( thread_number),
                  m_DataSetRanges( epoch_number).GetMin(),
                  m_DataSetRanges( epoch_number).GetMax(),
                  sched::JobInterface::e_READY,
                  NULL
                )
              );
          sched::GetScheduler().RunJob( all_jobs( thread_number));
        }

        // join the first job
        sched::GetScheduler().Join( all_jobs( 0));

        // join remaining jobs
        for( size_t job_id( 1); job_id < m_NumberThreadsNodes; ++job_id)
        {
          sched::GetScheduler().Join( all_jobs( job_id));

          // add their results to the first new network
          m_NewNetworks( 0) += m_NewNetworks( job_id);
        }

        m_NewNetworks.FirstElement().FixEmptyNodes();
        m_Network = m_NewNetworks( 0);
        m_Network.FixEmptyNodes();
      }

      m_CurrentIteration++;

      if( !m_NoTrack)
      {
        this->GetTracker().Track( GetCurrentApproximation());
      }
    }

    //! @brief thread helper function for iterate, works on a subset of the training data, computes partial
    //!        adjustments to the network, then adds them to the final result.
    //! @param RANGE_ID the partition of the training data to work on
    //! @param WINNING_INDICES indices of the winning nodes for the data being operated on
    //! @param START_DATA_NUMBER the index in the training data for the current epoch
    void ApproximatorKohonenNetwork::TrainThread
    (
      const size_t &RANGE_ID,
      const size_t &START_DATA_NUMBER,
      const size_t &END_DATA_NUMBER
    )
    {
      // get the network where results will be placed
      KohonenNetworkAverage &network( m_NewNetworks( RANGE_ID));

      // reset the weights in that network
      network.Reset();

      // get the range of nodes that will be iterated over
      const math::Range< size_t> &range( m_NodeRanges( RANGE_ID));

      // node count will be a histogram of nodes inside the range
      storage::Vector< size_t> node_counts( range.GetWidth(), size_t( 0));

      size_t number_winners( 0), last_winner( 0);

      // walk over all the winning indices
      for
      (
        storage::Vector< size_t>::const_iterator
          itr_winners( m_Schedule.GetOrder().Begin() + START_DATA_NUMBER),
          itr_winners_end( m_Schedule.GetOrder().Begin() + END_DATA_NUMBER);
        itr_winners != itr_winners_end;
        ++itr_winners
      )
      {
        const size_t winning_node( m_LastClosestNodes( *itr_winners));
        // check if this thread is responsible for this range
        if( range.IsWithin( winning_node))
        {
          // get a reference on the appropriate member of node count
          size_t &node_count( node_counts( winning_node - range.GetMin()));

          // if this was not previously a winning node, mark it as such
          if( node_count == 0)
          {
            node_count = 1;
            ++number_winners;
            last_winner = winning_node;
          }

          // map the data, no need for learning rate with batch iterate
          network.GetCodeBook()( winning_node).MapData
          (
            m_TrainingData->GetFeaturesPtr()->operator()( *itr_winners),
            m_TrainingData->GetResultsPtr()->operator()( *itr_winners)
          );
        }
      }

      if( number_winners == 0)
      {
        // no winners in this range, nothing to do
      }
      // optimization for when there is only one node that was chosen as a winner
      if( number_winners == 1)
      {
        // overlap is impossible, so adapt network directly

        // use a partial vector adapt function and the neighbor adapt function on the network
        AdaptNeighbors( network, network.GetCodeBook()( last_winner), m_CurrentRadius);
      }
      else
      {
        // need a swap network to avoid changing the results of this network
        KohonenNetworkAverage &network_swap( m_NewNetworksSwap( RANGE_ID));
        network_swap.Reset();

        for( size_t counter( 0), size( node_counts.GetSize()); counter < size; ++counter)
        {
          if( node_counts( counter) > 0)
          {
            // get the index of the node to update
            const size_t node_index( counter + range.GetMin());
            // use a partial vector adapt function and the neighbor adapt function on the network
            AdaptNeighbors( network_swap, network.GetCodeBook()( node_index), m_CurrentRadius);
          }
        }

        // swap the networks' codebooks
        network.GetCodeBook().InternalData().swap( network_swap.GetCodeBook().InternalData());
      }
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorKohonenNetwork::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_Network, ISTREAM);
      io::Serialize::Read( m_Length, ISTREAM);
      io::Serialize::Read( m_CurrentIteration, ISTREAM);
      io::Serialize::Read( m_Radius, ISTREAM);
      io::Serialize::Read( m_TrainingData, ISTREAM);
      io::Serialize::Read( m_NeighborKernel, ISTREAM);
      io::Serialize::Read( m_Initializer, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorKohonenNetwork::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_Network, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Length, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_CurrentIteration, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Radius, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_TrainingData, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NeighborKernel, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Initializer, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief setup the data ranges and node ranges for training
    //! This should only be called while setting the training data or after reading
    void ApproximatorKohonenNetwork::SetupThreadRanges()
    {
      // set up the training ranges for each thread
      m_NodeRanges.Reset();

      const size_t number_nodes( m_Network.GetCodeBook().GetSize());

      // determine the # of threads for splitting up nodes
      m_NumberThreadsNodes = std::min( number_nodes, sched::GetNumberCPUs());

      BCL_MessageStd( "Set up node ranges with # threads: " + util::Format()( m_NumberThreadsNodes));

      const size_t nodes_per_thread( number_nodes / m_NumberThreadsNodes);
      const size_t number_threads_with_additional_node( number_nodes % m_NumberThreadsNodes);
      m_NodeRanges.AllocateMemory( m_NumberThreadsNodes);

      for( size_t thread_number( 0), end_node( 0); thread_number < m_NumberThreadsNodes; ++thread_number)
      {
        // save the old end node #
        const size_t old_end_node( end_node);

        // determine the # of nodes that will be handled by this thread
        const size_t nodes_this_thread( nodes_per_thread + size_t( thread_number < number_threads_with_additional_node));

        end_node += nodes_this_thread;

        m_NodeRanges.PushBack
        (
          math::Range< size_t>
          (
            math::RangeBorders::e_LeftClosed, old_end_node, end_node, math::RangeBorders::e_RightOpen
          )
        );
      }

      m_NewNetworks = m_NewNetworksSwap = storage::Vector< KohonenNetworkAverage>( m_NumberThreadsNodes, m_Network);
    }

    //! @brief spreads the adaptation from a particular node to other nodes within RADIUS
    //! @param NETWORK the network to adapt, usually a copy to avoid conflicting changes
    //! @param NODE_TO_SPREAD is the node whose feature and result vectors should be propagated
    //! @param RADIUS the current neighborhood radius
    void ApproximatorKohonenNetwork::AdaptNeighbors
    (
      KohonenNetworkAverage &NETWORK,
      const KohonenNode &NODE_TO_SPREAD,
      const float &RADIUS
    ) const
    {
      const linal::Vector< float> &position( NODE_TO_SPREAD.GetPosition());

      const double node_weight( NODE_TO_SPREAD.GetWeight());

      const double max_radius( m_NeighborKernel == e_Gaussian ? 2.0 * RADIUS : RADIUS);
      for
      (
        storage::Vector< KohonenNode>::iterator
          itr( NETWORK.GetCodeBook().Begin()), itr_end( NETWORK.GetCodeBook().End());
        itr != itr_end;
        ++itr
      )
      {
        // the distance between two map nodes
        const float distance( linal::Distance( position, itr->GetPosition()));

        // only if learning will occur do we adapt the neighbor
        if( distance <= max_radius)
        {
          // the learning rate to be applied
          float learning_rate( node_weight);
          if( m_NeighborKernel == e_Gaussian)
          {
            learning_rate *= std::exp( float( -0.5 * math::Sqr( distance / RADIUS)));
          }
          // if m_function was bubble, then learning rate should == node_weight, which it already is

          if( position == itr->GetPosition())
          {
            itr->MapData( NODE_TO_SPREAD.GetFeatureVector(), NODE_TO_SPREAD.GetResultVector(), 0.8 * learning_rate);
          }
          else
          {
            itr->MapData( NODE_TO_SPREAD.GetFeatureVector(), NODE_TO_SPREAD.GetResultVector(), 0.2 * learning_rate);
          }
        }
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorKohonenNetwork::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "A kohonen-network based predictor. See http://en.wikipedia.org/wiki/Self-organizing_map"
      );
      parameters.Merge( m_Schedule.GetSerializer());
      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        "RMSD"
      );
      parameters.AddInitializer
      (
        "map dimensions",
        "size of each dimension, grid spacing for each node will always be 1.0",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_Network.GetMapDimensions(),
          io::Serialization::GetAgentWithRange( double( 0.0), double( 10000.0))
        )
      );
      parameters.AddInitializer
      (
        "steps per update",
        "# of features seen between each update of nodes (set to 0 to use the size of the training data set)",
        io::Serialization::GetAgent( &m_UpdateEveryNthFeature),
        "0"
      );
      parameters.AddInitializer
      (
        "length",
        "# of iterations it takes for radius to decrease to 0",
        io::Serialization::GetAgent( &m_Length),
        "10"
      );
      parameters.AddInitializer
      (
        "radius",
        "initial radius; larger radii are often better, at the expense of training time, up to about half the distance from one end of the map to the other",
        io::Serialization::GetAgent( &m_Radius),
        "10"
      );
      parameters.AddInitializer
      (
        "neighbor kernel",
        "Determines neighborhood type.  With guassian the influence decreases with distance (within the radii), with bubble it remains constant",
        io::Serialization::GetAgent( &m_NeighborKernel),
        "Bubble"
      );
      parameters.AddInitializer
      (
        "initializer",
        "Determines how the map is initialized.  FirstVectors chooses the first N vectors to populate the map, other methods allow selection of random training vectors or random training vector elements",
        io::Serialization::GetAgent( &m_Initializer),
        "FirstVectors"
      );
      parameters.AddInitializer
      (
        "scaling",
        "Type of input scaling. Normally AveStd works best, but MinMax and None may also be used in some circumstances",
        io::Serialization::GetAgent( &m_RescaleType),
        "MinMax"
      );
      parameters.AddInitializer
      (
        "cutoff",
        "Cutoff between actives and inactives. Needed if balancing",
        io::Serialization::GetAgent( &m_Cutoff),
        "0.5"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_leverage_matrix.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_matrix_inversion_cholesky.h"
#include "linal/bcl_linal_matrix_inversion_moore_penrose.h"
#include "model/bcl_model_leverage_matrix.h"
#include "util/bcl_util_stopwatch.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorLeverageMatrix::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorLeverageMatrix())
    );

    //! @brief default constructor
    ApproximatorLeverageMatrix::ApproximatorLeverageMatrix() :
      m_LastObjectiveFunctionResult( util::GetUndefined< float>())
    {
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorLeverageMatrix::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorLeverageMatrix::GetAlias() const
    {
      static const std::string s_alias( "Leverage");
      return s_alias;
    }

    //! @brief set training data set for a specific iterate in approximator framework
    //! @param DATA training data set
    void ApproximatorLeverageMatrix::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      BCL_Assert( !DATA->IsEmpty(), "no training data loaded");

      // rescale function for in an output and denormalization
      m_TrainingData = DATA;
      m_TrainingData->GetFeatures().Rescale( math::Range< float>( 0.0, 1.0), RescaleFeatureDataSet::e_AveStd);

      linal::Matrix< float> coefficients( m_TrainingData->GetFeatureSize(), m_TrainingData->GetFeatureSize());
      double av_hat( double( m_TrainingData->GetFeatureSize() + 1) / double( m_TrainingData->GetSize()));
      coefficients /= float( av_hat);
      m_LeverageMatrix = util::ShPtr< Interface>( new LeverageMatrix( coefficients, GetRescaleFeatureDataSet()));
      BCL_MessageStd( "Setting up training data with " + util::Format()( DATA->GetSize()) + " points");
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorLeverageMatrix::GetCurrentModel() const
    {
      return m_LeverageMatrix;
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorLeverageMatrix::GetCurrentApproximation() const
    {
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_LeverageMatrix, m_LastObjectiveFunctionResult)
        );
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorLeverageMatrix::Next()
    {
      // check whether the model has already been trained
      if( !util::IsNaN( m_LastObjectiveFunctionResult))
      {
        return;
      }

      // assert when there is no interval
      BCL_Assert( m_TrainingData.IsDefined() && !m_TrainingData->IsEmpty(), "no training data given!");

      const size_t number_features( m_TrainingData->GetSize());
      const size_t feature_size( m_TrainingData->GetFeatureSize());
      BCL_Assert( number_features >= feature_size, "insufficient training data to perform regression!");

      const linal::MatrixConstReference< float> features( m_TrainingData->GetFeatures().GetMatrix());
      const linal::MatrixConstReference< float> results( m_TrainingData->GetResults().GetMatrix());

      // compute the product matrix
      linal::Matrix< float> transpose_matrix_times_matrix( linal::MatrixTransposeTimesMatrix( features));

      BCL_MessageStd( "Solving matrix of size " + util::Format()( transpose_matrix_times_matrix.GetNumberRows()));
      linal::MatrixInversionMoorePenrose< float> mp_solver;
      // compute the inverse of the features matrix
      mp_solver.SetMatrix( transpose_matrix_times_matrix);

      const float av_hat( double( m_TrainingData->GetFeatureSize() + 1) / double( m_TrainingData->GetSize()));
      linal::Matrix< float> mp_inverse( mp_solver.ComputeInverse());
      mp_inverse /= av_hat;
      // create the model
      m_LeverageMatrix = util::ShPtr< Interface>( new LeverageMatrix( mp_inverse, GetRescaleFeatureDataSet()));

      // store the objective function result for later iterations
      m_LastObjectiveFunctionResult = 0.0;

      this->GetTracker().Track( GetCurrentApproximation());
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorLeverageMatrix::Read( std::istream &ISTREAM)
    {
      ApproximatorBase::Read( ISTREAM);
      io::Serialize::Read( m_LeverageMatrix, ISTREAM);
      io::Serialize::Read( m_LastObjectiveFunctionResult, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorLeverageMatrix::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      ApproximatorBase::Write( OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_LeverageMatrix, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_LastObjectiveFunctionResult, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorLeverageMatrix::CanContinue() const
    {
      // this approximation cannot be improved as it is analytic
      return !this->GetTracker().GetIteration();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorLeverageMatrix::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Computes the leverage matrix (aka projection or hat matrix), which allows identification of significant outliers "
        "that would likely substantially influence any simple linear model of the system. A returned value > 2 represents "
        "probable outliers, while greater than 3 represent definitive outliers. The average value is 1 for all values in "
        "the training set"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each iteration",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        "RMSD"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_linear_regression.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_matrix_inversion_cholesky.h"
#include "linal/bcl_linal_matrix_inversion_moore_penrose.h"
#include "util/bcl_util_stopwatch.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ApproximatorLinearRegression::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorLinearRegression())
    );

    //! @brief default constructor
    ApproximatorLinearRegression::ApproximatorLinearRegression() :
      m_MatrixSolver( linal::MatrixInversionCholesky< float>()),
      m_LastObjectiveFunctionResult( util::GetUndefined< float>())
    {
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorLinearRegression::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorLinearRegression::GetAlias() const
    {
      static const std::string s_alias( "LinearRegression");
      return s_alias;
    }

    //! @brief set training data set for a specific iterate in approximator framework
    //! @param DATA training data set
    void ApproximatorLinearRegression::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      BCL_Assert( !DATA->IsEmpty(), "no training data loaded");

      // rescale function for in an output and denormalization
      m_TrainingData = DATA;
      linal::Matrix< float> coefficients( m_TrainingData->GetFeatureSize(), m_TrainingData->GetResultSize());
      m_Weights = util::ShPtr< Interface>( new MultipleLinearRegression( coefficients));
      BCL_MessageStd( "Setting up training data with " + util::Format()( DATA->GetSize()) + " points");
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorLinearRegression::GetCurrentModel() const
    {
      return m_Weights;
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorLinearRegression::GetCurrentApproximation() const
    {
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_Weights, m_LastObjectiveFunctionResult)
        );
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorLinearRegression::Next()
    {
      // check whether the model has already been trained
      if( !util::IsNaN( m_LastObjectiveFunctionResult))
      {
        return;
      }

      // assert when there is no interval
      BCL_Assert( m_TrainingData.IsDefined() && !m_TrainingData->IsEmpty(), "no training data given!");

      const size_t number_features( m_TrainingData->GetSize());
      const size_t feature_size( m_TrainingData->GetFeatureSize());
      const size_t result_size( m_TrainingData->GetResultSize());
      BCL_Assert( number_features >= feature_size, "insufficient training data to perform regression!");

      util::Implementation< linal::MatrixInversionInterface< float> > solver( m_MatrixSolver);

      const linal::MatrixConstReference< float> features( m_TrainingData->GetFeatures().GetMatrix());
      const linal::MatrixConstReference< float> results( m_TrainingData->GetResults().GetMatrix());

      // compute the product matrix
      linal::Matrix< float> transpose_matrix_times_matrix;
      linal::Matrix< float> transpose_matrix_times_results;
      linal::MatrixConstReference< float> matrix_to_solve( features);
      linal::MatrixConstReference< float> b_vectors( results);
      bool is_moore_penrose( m_MatrixSolver->GetAlias() == linal::MatrixInversionMoorePenrose< float>().GetAlias());
      // Moore penrose computes the matrix inverse directly, but for all other methods,
      // it is necessary to compute X^T * X (lhs) and X^T * y (rhs)
      if( number_features > feature_size && !is_moore_penrose)
      {
        BCL_MessageStd
        (
          "Multiplying matrices of size " + util::Format()( number_features)
          + " X " + util::Format()( feature_size)
        );
        util::Stopwatch mult_timer
        (
          "feature Transpose * features & feature Transpose * results",
          util::Time(),
          util::Message::e_Standard
        );
        transpose_matrix_times_matrix = linal::MatrixTransposeTimesMatrix( features);
        matrix_to_solve.Reference( transpose_matrix_times_matrix);
        transpose_matrix_times_results = linal::MatrixTransposeTimesMatrix( features, results);
        b_vectors.Reference( transpose_matrix_times_results); // feature_size X result_size
      }

      BCL_MessageStd( "Solving matrix of size " + util::Format()( matrix_to_solve.GetNumberRows()));

      util::Stopwatch set_matrix_timer( "Decomposition/Inversion", util::Time(), util::Message::e_Standard, false);
      // compute the inverse of the features matrix
      if( !solver->SetMatrix( matrix_to_solve))
      {
        set_matrix_timer.Stop();
        // let the user know that their chosen method of matrix solution failed, use moore-penrose as backup
        BCL_MessageStd
        (
          "Matrix solver " + solver->GetAlias() + " could not handle the matrix; defaulting to moore-penrose algorithm"
        );
        solver = linal::MatrixInversionMoorePenrose< float>();
        // reset the matrix times matrix transpose to conserve memory
        transpose_matrix_times_matrix = linal::Matrix< float>();
        solver->SetMatrix( features);
        is_moore_penrose = true;
      }
      set_matrix_timer.Stop();
      set_matrix_timer.WriteMessage();

      util::Stopwatch solve_matrix_timer( "Solve Matrix", util::Time(), util::Message::e_Standard);
      linal::Matrix< float> coefficients;
      if( is_moore_penrose)
      {
        // compute the coefficients
        coefficients = solver->ComputeInverse() * results;
      }
      else
      {
        // solve for the coefficients
        coefficients = b_vectors;
        for( size_t result_num( 0); result_num < result_size; ++result_num)
        {
          coefficients.ReplaceCol( result_num, solver->Solve( coefficients.GetCol( result_num)));
        }
      }
      solve_matrix_timer.Stop();

      BCL_MessageStd( "Calculated coefficients");

      // create the model
      m_Weights = util::ShPtr< Interface>( new MultipleLinearRegression( coefficients));

      // store the objective function result for later iterations
      m_LastObjectiveFunctionResult = m_ObjectiveFunction->operator ()( m_Weights);
      BCL_MessageStd
      (
        " ObjFunction: " + util::Format()( m_LastObjectiveFunctionResult)
      );

      this->GetTracker().Track( GetCurrentApproximation());
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorLinearRegression::Read( std::istream &ISTREAM)
    {
      ApproximatorBase::Read( ISTREAM);
      io::Serialize::Read( m_Weights, ISTREAM);
      io::Serialize::Read( m_LastObjectiveFunctionResult, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorLinearRegression::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      ApproximatorBase::Write( OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Weights, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_LastObjectiveFunctionResult, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorLinearRegression::CanContinue() const
    {
      // this approximation cannot be improved as it is analytic
      return !this->GetTracker().GetIteration();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorLinearRegression::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Performs multiple linear regression see http://en.wikipedia.org/wiki/Linear_regression"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each iteration",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        "RMSD"
      );
      parameters.AddInitializer
      (
        "solver",
        "Solution method for the matrix",
        io::Serialization::GetAgent( &m_MatrixSolver),
        "Cholesky"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_neural_network.h"

// includes from bcl - sorted alphabetically
#include "iterate/bcl_iterate_reflecting.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_statistics.h"
#include "model/bcl_model_neural_network_selective_backpropagation_default.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorNeuralNetwork::s_IterateInstance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorNeuralNetwork( false))
    );
    const util::SiPtr< const util::ObjectInterface> ApproximatorNeuralNetwork::s_PretrainInstance
    (
      util::Enumerated< PretrainNeuralNetworkInterface>::AddInstance( new ApproximatorNeuralNetwork( true))
    );

    //! @brief InputLayerDropoutType as string
    //! @param TYPE the type
    //! @return the string for the type
    const std::string &ApproximatorNeuralNetwork::GetInputLayerDropoutTypeName( const InputLayerDropoutType &TYPE)
    {
      static const std::string s_names[] =
      {
        "Zero",
        "Noise",
        "CopySingleRandomFeature",
        "CopyEachRandomFeature",
        "CopySingleRandomPeerFeature",
        "CopyEachRandomPeerFeature",
        "CopySingleBalancedFeature",
        "CopyEachBalancedFeature",
        GetStaticClassName< ApproximatorNeuralNetwork::InputLayerDropoutType>()
      };
      return s_names[ size_t( TYPE)];
    }

    //! @brief default constructor
    ApproximatorNeuralNetwork::ApproximatorNeuralNetwork( const bool &PRETRAIN) :
      m_UpdateEveryNthFeature( 0),
      m_IterationsPerRMSDMessage( 1),
      m_ConnectionDensity( 1.0),
      m_AlignCutoff( false),
      m_RescaleOutputDynamicRange( true),
      m_DataSetRangePosition( 0),
      m_TransferFunction(),
      m_DataSelector( NeuralNetworkSelectiveBackpropagationDefault()),
      m_IsPretrainer( PRETRAIN),
      m_NumIterations( 0),
      m_NoiseZScore( 0.0),
      m_InputDropoutType( e_Zero),
      m_RescaleType( RescaleFeatureDataSet::e_AveStd)
    {
    }

    //! @brief constructor from training data, transfer, rescale, and objective functions
    //! @param TRAINING_DATA data to train the NeuralNetwork on
    //! @param UPDATE_EVERY_NTH_FEATURE how often the weights get updated
    //! @param ARCHITECTURE the # of neurons in each hidden layer of the network
    //! @param TRANSFER_FUNCTION ShPtr to the transfer function between input and output of each neuron
    //! @param OBJECTIVE_FUNCTION ShPtr to objective function
    //! @param WEIGHT_UPDATE_FUNCTION method by which to update the weights
    //! @param ITERATIONS_PER_RMSD_REPORT # iterations per report of the rmsd
    ApproximatorNeuralNetwork::ApproximatorNeuralNetwork
    (
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t UPDATE_EVERY_NTH_FEATURE,
      const storage::Vector< size_t> &ARCHITECTURE,
      const util::Implementation< TransferFunctionInterface> &TRANSFER_FUNCTION,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const util::Implementation< NeuralNetworkUpdateWeightsInterface> &WEIGHT_UPDATE_FUNCTION,
      const size_t &ITERATIONS_PER_RMSD_REPORT,
      const RescaleFeatureDataSet::TypeEnum RESCALE_TYPE
    ) :
      ApproximatorBase( OBJECTIVE_FUNCTION),
      m_UpdateEveryNthFeature( UPDATE_EVERY_NTH_FEATURE),
      m_IterationsPerRMSDMessage( ITERATIONS_PER_RMSD_REPORT),
      m_ConnectionDensity( 1.0),
      m_AlignCutoff( false),
      m_RescaleOutputDynamicRange( true),
      m_DataSetRangePosition( 0),
      m_HiddenArchitecture( ARCHITECTURE),
      m_TransferFunction( TRANSFER_FUNCTION),
      m_WeightUpdateType( WEIGHT_UPDATE_FUNCTION),
      m_DataSelector( NeuralNetworkSelectiveBackpropagationDefault()),
      m_IsPretrainer( false),
      m_NumIterations( 0),
      m_NoiseZScore( 0.0),
      m_InputDropoutType( e_Zero),
      m_RescaleType( RESCALE_TYPE)
    {
      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new ApproximatorNeuralNetwork copied from this instance
    ApproximatorNeuralNetwork *ApproximatorNeuralNetwork::Clone() const
    {
      return new ApproximatorNeuralNetwork( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorNeuralNetwork::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorNeuralNetwork::GetAlias() const
    {
      static const std::string s_Name( "NeuralNetwork");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorNeuralNetwork::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_TrainingData = DATA;
      if( !DATA->GetFeaturesPtr()->IsRescaled())
      {
        DATA->GetFeatures().Rescale( NeuralNetwork::s_DefaultInputRange, m_RescaleType);
      }
      if( !DATA->GetResultsPtr()->IsRescaled())
      {
        DATA->GetResults().Rescale
        (
          m_RescaleOutputDynamicRange
          ? m_TransferFunction->GetDynamicOutputRange()
          : m_TransferFunction->GetOutputRange()
        );
      }
      m_RescaleOutputLastRound = DATA->GetResultsPtr()->GetScaling();

      m_TrainingSchedule.Setup( DATA->GetResults(), m_ObjectiveFunction->GetThreshold());
      if( int( m_InputDropoutType) >= int( s_InputLayerDropoutFirstClassBasedMethod))
      {
        if( m_Dropout.IsEmpty() || !m_Dropout( 0))
        {
          BCL_MessageCrt
          (
            m_InputDropoutType.GetString() + " has no effect because no neurons are to be dropped in the input layer"
          );
        }
        if( !util::IsDefined( m_ObjectiveFunction->GetThreshold()))
        {
          const std::string &old_input_dropout_type( m_InputDropoutType.GetString());
          m_InputDropoutType
            = (
                m_InputDropoutType == e_CopyEachRandomPeerFeature || m_InputDropoutType == e_CopyEachBalancedFeature
                ? e_CopyEachRandomFeature
                : e_CopySingleRandomFeature
               );
          BCL_MessageCrt
          (
            old_input_dropout_type
            + " has no effect because the objective function does not have a cutoff, switching to "
            + m_InputDropoutType.GetString()
          );
        }
      }

      BCL_MessageStd
      (
        "Setting up training data with " + util::Format()( m_TrainingData->GetSize()) + " points"
      );

      // initialize the dropout, if necessary
      InitializeDropout();

      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Iterate properly
      if( !IsTrainingContinued())
      {
        if( m_Pretrainer.IsDefined())
        {
          m_Pretrainer->SetIds( ApproximatorBase::GetIdCode());
          m_Pretrainer->SetFeatures( ApproximatorBase::GetFeatureCode());
          m_Pretrainer->SetResults( ApproximatorBase::GetResultCode());

          BCL_MessageStd( "Pretraining with method: " + m_Pretrainer->GetString());
          util::ShPtr< NeuralNetwork> initial_network( m_Pretrainer->PretrainNetwork( DATA, m_ObjectiveFunction));
          // ensure that a model was pretrained
          if( !initial_network.IsDefined())
          {
            BCL_Exit( "Pretraining failed!", -1);
          }

          // copy the bias and weight from the model
          m_Bias = initial_network->GetBias();
          m_Weight = initial_network->GetWeight();

          // copy the hidden architecture from the model
          linal::Vector< size_t> old_hidden_architecture( m_HiddenArchitecture);
          m_HiddenArchitecture = initial_network->GetArchitecture();
          // remove the first and last elements, which are the input and output layer sizes, respectively
          m_HiddenArchitecture.RemoveElements( 0, 1);
          m_HiddenArchitecture.RemoveElements( m_HiddenArchitecture.GetSize() - 1, 1);

          // need to adjust weights if performing dropout in this layer
          if( m_NumberToDrop.Sum())
          {
            // must multiply m_Weight( i) by (1-m_Dropout( i))
            for( size_t layer( 0), n_layers( m_Weight.GetSize()); layer < n_layers; ++layer)
            {
              if( !m_NumberToDrop( layer))
              {
                continue;
              }
              const size_t neurons_this_layer( m_Weight( layer).GetNumberCols());
              const float kept_fraction( float( neurons_this_layer) / float( neurons_this_layer - m_NumberToDrop( layer)));
              m_Weight( layer) *= kept_fraction;
            }
          }

          // make sure that either no hidden architecture was specified or that it agreed with the architecture implicit
          // in the model
          if
          (
            !old_hidden_architecture.IsEmpty()
            && linal::Vector< size_t>( m_HiddenArchitecture) != old_hidden_architecture
          )
          {
            BCL_MessageCrt
            (
              "Ignoring hidden architecture given in iterate; using implicit architecture from initial network file"
            );
          }
        }
      }

      // set the data ranges up for threading (if applicable)
      SetupDataSetRanges();

      m_DataSelector->Initialize( *m_TrainingData, *m_ObjectiveFunction->GetImplementation(), m_NumberThreads);

      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Iterate properly
      if( !IsTrainingContinued())
      {
        // if the initial weights and bias were not read in from a file, they need to be initialized
        if( !m_Pretrainer.IsDefined())
        {
          // set the architecture (m_Bias, m_Weights, etc) up using m_HiddenArchitecture
          SetupArchitecture();
        }
        else
        {
          // destroy the pretrainer to free up memory
          m_Pretrainer = m_Pretrainer.GetLabel();
        }
        // weight updaters need to be initialized regardless
        InitializeWeightUpdaters();
      }

      // update neurons for dropout
      UpdateDroppedNeurons();

    } // SetTrainingData

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorNeuralNetwork::GetCurrentModel() const
    {
      if( m_NumberToDrop.Sum())
      {
        // must multiply m_Weight( i) by (1-m_Dropout( i)).  To avoid unnecessary creation/destruction of matrices,
        // use m_SlopesWeight
        for( size_t layer( 0), n_layers( m_Weight.GetSize()); layer < n_layers; ++layer)
        {
          m_SlopesWeight( 0)( layer) = m_Weight( layer);
          if( m_NumberToDrop( layer))
          {
            const float n_dropped( m_ChosenDropped( 0)( layer).GetSize());
            if( layer || m_InputDropoutType == e_Noise || m_InputDropoutType == e_Zero || m_InputDropoutType == e_CopyEachRandomFeature)
            {
              const float kept_fraction( 1.0 - n_dropped / float( m_Weight( layer).GetNumberCols()));
              m_SlopesWeight( 0)( layer) *= kept_fraction;
            }
//            else if( m_InputDropoutType == e_CopyEachBalancedFeature || m_InputDropoutType == e_CopySingleBalancedFeature)
//            {
//              const float n_classes( m_PeerFeatures.GetSize());
//              const float kept_fraction( 1.0 - n_dropped / float( m_Weight( layer).GetNumberCols() * n_classes));
//              m_SlopesWeight( 0)( layer) *= kept_fraction;
//            }
          }
        }
        return util::ShPtr< Interface>
               (
                 new NeuralNetwork
                 (
                   GetRescaleFeatureDataSet(),
                   m_RescaleOutputLastRound,
                   m_Bias,
                   m_SlopesWeight( 0),
                   m_TransferFunction
                 )
               );
      }
      // make a new model out of the current data members
      return util::ShPtr< Interface>
      (
        new NeuralNetwork
        (
          GetRescaleFeatureDataSet(),
          m_RescaleOutputLastRound,
          m_Bias,
          m_Weight,
          m_TransferFunction
        )
      );
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorNeuralNetwork::GetCurrentApproximation() const
    {
      util::ShPtr< Interface> model( GetCurrentModel());
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>
          (
            model,
            m_ObjectiveFunction->operator ()( model)
          )
        );
    }

    //! @brief the main operation, pretrains a neural network
    //! @param DATA the data for use in pretraining
    //! @param OBJECTIVE ShPtr to the objective function for the network
    util::ShPtr< NeuralNetwork> ApproximatorNeuralNetwork::PretrainNetwork
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE
    )
    {
      this->SetObjectiveFunction( OBJECTIVE);
      this->SetTrainingData( DATA);
      while( this->GetTracker().GetIteration() < m_NumIterations)
      {
        this->Next();
      }
      return GetCurrentModel();
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorNeuralNetwork::Next()
    {
      // handle shuffling
      m_TrainingSchedule.Next();

      util::ShPtrVector< sched::JobInterface> all_jobs( m_NumberThreads);

      const size_t group_id( 0);

      // store the id of each thread for use by the jobs and reset all results tallying objects.
      storage::Vector< size_t> thread_ids( m_NumberThreads);
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
        m_RMSDError( thread_number) = float( 0.0);
      }

      // run through the data set once, updating weights after every epoch (m_UpdateEveryNthFeature features, data set size by default)
      // launch jobs to train the NN with the features for this epoch
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        all_jobs( thread_number)
          = util::ShPtr< sched::JobInterface>
            (
              new sched::UnaryFunctionJobWithData
              <
                const size_t,
                void,
                ApproximatorNeuralNetwork
              >
              (
                group_id,
                *this,
                &ApproximatorNeuralNetwork::TrainThread,
                thread_ids( thread_number),
                sched::JobInterface::e_READY,
                NULL
              )
            );
      }

      m_DataSetRangePosition = 0;
      size_t n_updated = 0;
      while( m_DataSetRangePosition < m_DataSetRanges.GetSize())
      {
        if( m_NumberThreads > size_t( 1))
        {
          for( size_t thread_number( 0); thread_number < m_NumberThreads; thread_number++)
          {
            sched::GetScheduler().RunJob( all_jobs( thread_number));
          }

          sched::GetScheduler().Join( all_jobs( 0));
        }
        else
        {
          // just run the job directly to avoid the scheduler's overhead
          all_jobs( 0)->Run();
        }

        size_t n_updated_this_time( 0);
        n_updated_this_time += m_NumberFeaturesUpdated( 0);
        m_NumberFeaturesUpdated( 0) = 0;
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          sched::GetScheduler().Join( all_jobs( thread_number));
          n_updated_this_time += m_NumberFeaturesUpdated( thread_number);

          if( m_NumberFeaturesUpdated( thread_number))
          {
            // accumulate the slopes from the newly joined jobs with the slopes from the earlier jobs
            for
            (
              size_t hidden_layer_number( 0), number_hidden_layers( m_HiddenArchitecture.GetSize());
              hidden_layer_number < number_hidden_layers;
              ++hidden_layer_number
            )
            {
              m_SlopesBias( 0)( hidden_layer_number) += m_SlopesBias( thread_number)( hidden_layer_number);
              m_SlopesWeight( 0)( hidden_layer_number) += m_SlopesWeight( thread_number)( hidden_layer_number);
            }
            m_NumberFeaturesUpdated( thread_number) = 0;
          }
        }

        if( n_updated_this_time)
        {
          UpdateWeights();
          n_updated += n_updated_this_time;
        }

        // move on the the next epoch
        m_DataSetRangePosition += m_NumberThreads;
      }

      if( ( this->GetTracker().GetIteration() % m_IterationsPerRMSDMessage) == size_t( 0))
      {
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          m_RMSDError( 0) += m_RMSDError( thread_number);
        }

        m_RMSDError( 0) /= m_TrainingData->GetResultSize();
        m_RMSDError( 0) /= m_TrainingData->GetSize();
        m_RMSDError( 0) = std::max( m_RMSDError( 0), float( 0.0));
        BCL_MessageStd
        (
          "Training relative RMSD for iteration " + util::Format()( this->GetTracker().GetIteration())
          + ( n_updated != m_TrainingSchedule.GetSize() ? ", " + util::Format()( n_updated) + " features trained" : std::string())
          + ": " + util::Format()( m_RMSDError( 0))
        );
      }

      m_NumberFeaturesUpdated = 0;

      m_DataSelector->FinalizeRound();

      m_DataSetRangePosition = 0;

      // get current model
      util::ShPtr< NeuralNetwork> current_model( GetCurrentModel());
      current_model->SetRescaleOutput( GetRescaleResultDataSet());

      // determine inaccuracy counts
      if( m_IterationWeightUpdateType.IsDefined())
      {
        linal::Matrix< float> old_weight( m_Weight( 0));
        ( *m_IterationWeightUpdaters( 0))( m_Weight( 0));
        float total_change( 0), total_weight( 0);
        for
        (
          const float *itr_old( old_weight.Begin()), *itr_old_end( old_weight.End()), *itr_new( m_Weight( 0).Begin());
          itr_old != itr_old_end;
          ++itr_old, ++itr_new
        )
        {
          total_change += math::Absolute( *itr_old - *itr_new);
          total_weight += math::Absolute( *itr_new);
        }
        BCL_MessageStd
        (
          "Average weight change: " + util::Format()( total_change / float( m_Weight( 0).GetNumberOfElements()))
          + " Average weight now : " + util::Format()( total_weight / float( m_Weight( 0).GetNumberOfElements()))
        );
      }

      if( m_IsPretrainer)
      {
        this->GetTracker().Track
        (
          util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
          (
            new storage::Pair< util::ShPtr< Interface>, float>
            (
              util::ShPtr< Interface>(),
              float( 0.0)
            )
          )
        );
        // act as a pretrainer for the specified number of steps, no need to create a model
        return;
      }

      // compute results on monitoring dataset
      FeatureDataSet< float> predicted_results( m_ObjectiveFunction->Predict( util::ShPtr< Interface>( current_model)));
      const float objective_result( m_ObjectiveFunction->Evaluate( predicted_results));
      if( m_AlignCutoff)
      {
        // optimize the rescaling function
        m_RescaleOutputLastRound = m_ObjectiveFunction->OptimizeRescalingFunction( GetRescaleResultDataSet(), predicted_results);
        current_model->SetRescaleOutput( m_RescaleOutputLastRound);
      }

      // combine it with the objective function evaluation
      this->GetTracker().Track
      (
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( current_model, objective_result)
        )
      );
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorNeuralNetwork::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "trains a neural network (see http://en.wikipedia.org/wiki/Artificial_neural_network)"
      );

      parameters.Merge( m_TrainingSchedule.GetSerializer());
      parameters.AddInitializer
      (
        "transfer function",
        "function that translates input from neurons in the prior layer into the output of each hidden layer",
        io::Serialization::GetAgent( &m_TransferFunction),
        "Sigmoid"
      );
      parameters.AddInitializer
      (
        "weight update",
        "algorithm used to update the weights",
        io::Serialization::GetAgent( &m_WeightUpdateType),
        "Resilient"
      );
      parameters.AddOptionalInitializer
      (
        "bias update",
        "algorithm used to update the biases; if omitted, the weight update type is used",
        io::Serialization::GetAgent( &m_BiasUpdateType)
      );
      parameters.AddOptionalInitializer
      (
        "iteration weight update",
        "algorithm used to update the weights after every run through the data",
        io::Serialization::GetAgent( &m_IterationWeightUpdateType)
      );
      parameters.AddInitializer
      (
        "steps per update",
        "# of features seen between each update of weights (set to 0 to use the size of the training data set)",
        io::Serialization::GetAgent( &m_UpdateEveryNthFeature),
        "0"
      );
      parameters.AddOptionalInitializer
      (
        "hidden architecture",
        "# of neurons in each hidden layer, e.g. hidden architecture(100) declares a single hidden layer w/ 100 neurons. "
        "This variable can be omitted to train a single layer perception (logistic regression)",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_HiddenArchitecture,
          io::Serialization::GetAgentWithMin( size_t( 1)),
          0
        )
      );
      parameters.AddInitializer
      (
        "rmsd report frequency",
        "# of iterations between reports of rmsd on the last training batch",
        io::Serialization::GetAgentWithMin( &m_IterationsPerRMSDMessage, size_t( 1)),
        "1"
      );
      parameters.AddOptionalInitializer
      (
        "initial network",
        "method by which to create the initial network, if not randomly",
        io::Serialization::GetAgent( &m_Pretrainer)
      );

      parameters.AddInitializer
      (
        "data selector",
        "method to use to select which data to backpropagate",
        io::Serialization::GetAgent( &m_DataSelector),
        "All"
      );
      parameters.AddInitializer
      (
        "connection density",
        "Fraction of connections made between input and hidden layers",
        io::Serialization::GetAgentWithRange( &m_ConnectionDensity, 0.0, 1.0),
        "1"
      );
      parameters.AddOptionalInitializer
      (
        "dropout",
        "fraction of neurons to have \"dropout\" (set to 0) for an entire weight-update session, per layer",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_Dropout,
          io::Serialization::GetAgentWithRange( float( 0.0), float( 1.0)),
          0
        )
      );
      parameters.AddOptionalInitializer
      (
        "dropout partitions",
        "# of partitions between neurons in each layer.  The number of neurons dropped within each partition will always"
        "be the same, so this allows difference regions of hidden neurons to learn different functionalities, which is"
        "particularly useful for multi-output ANNs",
        io::Serialization::GetAgentWithSizeLimits( &m_DropoutPartitions, size_t( 0))
      );
      parameters.AddInitializer
      (
        "scaling",
        "Type of input scaling. Normally AveStd works best, but MinMax and None may also be used in some circumstances",
        io::Serialization::GetAgent( &m_RescaleType),
        "AveStd"
      );
      parameters.AddInitializer
      (
        "input noise",
        "Amount of noise to apply to input features, in units of z-score",
        io::Serialization::GetAgentWithRange( &m_NoiseZScore, 0.0, 3.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "rescale output dynamic range",
        "if true, output will be rescaled to the dynamic range of the transfer function. Useful for "
        "regression, where the output of the function being predicted could feasibly extend beyond the range seen in the "
        "the training set by a marginal amount. It may also result in smaller weights. "
        "Classification problems often benefit from using the False setting",
        io::Serialization::GetAgent( &m_RescaleOutputDynamicRange),
        "True"
      );
      parameters.AddInitializer
      (
        "input dropout type",
        "Method of applying dropout to the input layer. Descriptions for each method:\n"
        "Zero                         <-- Set the dropped neurons to zero; fast, but biased\n"
        "Noise,                       <-- Set the neurons to a random gaussian value sampled according to the input mean/std\n"
        "CopySingleRandomFeature      <-- Copy dropped values from a single randomly-selected feature\n"
        "CopyEachRandomFeature        <-- Copy each dropped value from a randomly-selected feature (training example)\n"
        "CopySingleRandomPeerFeature  <-- Copy values from a single, randomly-selected peer feature\n"
        "CopyEachRandomPeerFeature    <-- Copy each value from a randomly-selected peer feature\n"
        "A peer is a training example that has the same class for every result column.  Use of input dropout types that "
        "have Peer in the name requires that a classification-based objective function is used (typically any objective "
        "function with a cutoff). The output class is a binary value of whether the column is above or below the cutoff"
        ". The balanced variants (CopySingleBalancedFeature, CopyEachBalancedFeature) select a random feature of a "
        " random class.",
        io::Serialization::GetAgent( &m_InputDropoutType),
        "Zero"
      );

      if( !m_IsPretrainer)
      {
        parameters.AddInitializer
        (
          "align cutoff",
          "Adjust output scaling automatically according to the objective functions.  This will only have an effect for "
          "objective functions such as enrichment and fpp vs ppv that have an internal FPR or similar cutoff.",
          io::Serialization::GetAgent( &m_AlignCutoff),
          "False"
        );
        parameters.AddInitializer
        (
          "objective function",
          "function that evaluates the model after each batch step",
          io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
          "RMSD"
        );
      }
      else
      {
        // pretraining-specific parameters
        parameters.AddInitializer
        (
          "iterations",
          "Number of complete iterations through the data to pretrain the network",
          io::Serialization::GetAgent( &m_NumIterations)
        );
      }

      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorNeuralNetwork::Read( std::istream &ISTREAM)
    {
      descriptor::Dataset::s_Instance.IsDefined();

      // read members
      io::Serialize::Read( m_UpdateEveryNthFeature, ISTREAM);
      io::Serialize::Read( m_HiddenArchitecture, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Weight, ISTREAM);
      io::Serialize::Read( m_TransferFunction, ISTREAM);
      io::Serialize::Read( m_WeightUpdateType, ISTREAM);
      io::Serialize::Read( m_IterationWeightUpdateType, ISTREAM);
      io::Serialize::Read( m_BiasUpdaters, ISTREAM);
      io::Serialize::Read( m_WeightUpdaters, ISTREAM);
      io::Serialize::Read( m_IterationWeightUpdaters, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorNeuralNetwork::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_UpdateEveryNthFeature, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_HiddenArchitecture, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Weight, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_TransferFunction, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightUpdateType, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_IterationWeightUpdateType, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_IterationWeightUpdaters, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

    //! run forward through ANN and compute hidden terms m_Hidden (test)
    void ApproximatorNeuralNetwork::CalcHiddenTerms
    (
      const FeatureReference< float> &FEATURE,
      const size_t &THREAD_ID,
      const size_t &FEATURE_ID
    )
    {
      // get the thread-local errors, hidden, and hidden input arrays
      storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      storage::Vector< linal::Vector< float> > &hidden_input( m_HiddenInput( THREAD_ID));
      const storage::Vector< storage::Vector< size_t> > &thread_drop_ids( m_ChosenDropped( THREAD_ID));

      // input layer
      // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
      hidden_input( 0) = m_Bias( 0);
      if( m_NumberToDrop( 0) || m_NoiseZScore)
      {
        linal::Vector< float> &feature_with_dropout( m_FeatureWithDropout( THREAD_ID));
        std::copy( FEATURE.Begin(), FEATURE.End(), feature_with_dropout.Begin());
        linal::MatrixConstReference< float> allfeatures( m_TrainingData->GetFeaturesReference());
        if( m_NoiseZScore)
        {
          random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
          for( float *itr( feature_with_dropout.Begin()), *itr_end( feature_with_dropout.End()); itr != itr_end; ++itr)
          {
            *itr += rng.RandomGaussian( 0.0, m_NoiseZScore);
          }
        }
        if( m_NumberToDrop( 0))
        {
          const storage::Vector< size_t> &drop_ids( thread_drop_ids( 0));
          if( m_InputDropoutType == e_Noise)
          {
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = rng.RandomGaussian( 0.0, 0.5);
            }
          }
          else if( m_InputDropoutType == e_Zero)
          {
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = 0.0;
            }
          }
          else if( m_InputDropoutType == e_CopySingleRandomFeature || m_InputDropoutType == e_CopyEachRandomFeature)
          {
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            const size_t max_feature_to_pick( m_TrainingData->GetSize() - 1);
            if( m_InputDropoutType == e_CopySingleRandomFeature)
            {
              const size_t random_feature( rng.Random( max_feature_to_pick));
              linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature));
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = selected_feature( *itr_drop);
              }
            }
            else
            {
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = allfeatures( rng.Random( max_feature_to_pick), *itr_drop);
              }
            }
          }
          else if
          (
            m_InputDropoutType == e_CopySingleRandomPeerFeature
            || m_InputDropoutType == e_CopyEachRandomPeerFeature
          )
          {
            // peer-based dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            const storage::Vector< size_t> &peers
            (
              m_TrainingSchedule.GetClassMembers()( m_TrainingSchedule.GetClasses()( FEATURE_ID))
            );
            const size_t max_feature_to_pick( peers.GetSize() - 1);
            if( m_InputDropoutType == e_CopySingleRandomPeerFeature)
            {
              const size_t random_feature( peers( rng.Random( max_feature_to_pick)));
              linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature));
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) += selected_feature( *itr_drop);
              }
            }
            else // if( m_InputDropoutType == e_CopyEachRandomPeerFeature)
            {
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = allfeatures( peers( rng.Random( max_feature_to_pick)), *itr_drop);
              }
            }
          }
          else if( m_InputDropoutType == e_CopySingleBalancedFeature)
          {
            // single feature, selected randomly from randomly selected class dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            // choose which class to select the feature from
            const storage::Vector< size_t> &chosen_class
            (
              m_TrainingSchedule.GetClassMembers()( rng.Random( m_TrainingSchedule.GetNumberOfClasses() - 1))
            );
            const size_t random_feature_id( chosen_class( rng.Random( chosen_class.GetSize() - 1)));
            linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature_id));
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = selected_feature( *itr_drop);
            }
          }
          else if( m_InputDropoutType == e_CopyEachBalancedFeature)
          {
            // single feature, selected randomly from randomly selected class dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            // choose which class to select the feature from
            const size_t max_class( m_TrainingSchedule.GetNumberOfClasses() - 1);
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              const storage::Vector< size_t> &chosen_class( m_TrainingSchedule.GetClassMembers()( rng.Random( max_class)));
              const size_t random_feature_id( chosen_class( rng.Random( chosen_class.GetSize() - 1)));
              feature_with_dropout( *itr_drop) = allfeatures( random_feature_id, *itr_drop);
            }
          }
        }
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), feature_with_dropout);
      }
      else
      {
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), FEATURE);
      }

      // run hidden_input through the transfer functions to get the 1st hidden layer output
      m_TransferFunction->F( hidden( 0), hidden_input( 0));

      // remaining layers
      for( size_t k( 1), number_hidden_layers( hidden.GetSize()); k < number_hidden_layers; ++k)
      {
        // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
        hidden_input( k) = m_Bias( k);
        if( m_NumberToDrop( k))
        {
          linal::Vector< float> &hidden_dropped( hidden( k - 1));
          const storage::Vector< size_t> &drop_ids( thread_drop_ids( k));
          for
          (
            storage::Vector< size_t>::const_iterator itr_dropped( drop_ids.Begin()), itr_dropped_end( drop_ids.End());
            itr_dropped != itr_dropped_end;
            ++itr_dropped
          )
          {
            hidden_dropped( *itr_dropped) = 0.0;
          }
        }
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( k), m_Weight( k), hidden( k - 1));

        // run hidden_input through the transfer functions to get the k+1th hidden layer output
        m_TransferFunction->F( hidden( k), hidden_input( k));
      }
    } // CalcHiddenTerms

    //! run backward through ANN and compute err terms m_Errors (train)
    //! @return true if the error should be backpropagated
    bool ApproximatorNeuralNetwork::CalcErrorTerms
    (
      const FeatureReference< float> &RESULT,
      const size_t &THREAD_ID,
      const size_t &FEATURE_ID
    )
    {
      // get the thread-local errors, hidden, and hidden input arrays
      storage::Vector< linal::Vector< float> > &errors( m_Errors( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden_input( m_HiddenInput( THREAD_ID));

      // compute difference
      const linal::Vector< float> &predicted( hidden.LastElement());
      linal::Vector< float> &storage( errors.LastElement());
      storage = RESULT;
      storage -= predicted;
      for( size_t i( 0), n_results( RESULT.GetSize()); i < n_results; ++i)
      {
        // replace undefined values w/ zero
        if( !util::IsDefined( storage( i)))
        {
          storage( i) = 0.0;
        }
      }
      const float error( storage.Norm());
      m_RMSDError( THREAD_ID) += error;

      // decide whether or not to backprop this feature
      // may also decide not to backprop some of the oclumns
      if( !m_DataSelector->ShouldBackpropagate( predicted, storage, FEATURE_ID, THREAD_ID))
      {
        return false;
      }

      // output layer

      // the following code performs this operation
      // errors.LastElement() = ( RESULT - hidden.LastElement())
      //                          * m_TransferFunction->dF( hidden_input.LastElement(), hidden.LastElement())
      // without creating any (expensive) temporary vectors
      m_TransferFunction->MultiplyBydF
      (
        errors.LastElement(),
        hidden_input.LastElement(),
        hidden.LastElement()
      );

      // all other layers
      for( size_t i( hidden.GetSize() - 1); i > 0; --i)
      {
        // errors( i - 1) = errors( i) * m_Weight( i)
        linal::GetDefaultOperations< float>().VectorEqualsVectorTimesMatrix( errors( i - 1), errors( i), m_Weight( i));
        m_TransferFunction->MultiplyBydF
        (
          errors( i - 1),
          hidden_input( i - 1),
          hidden( i - 1)
        );
      }
      return true;
    } // CalcErrorTerms

    //! compute changes m_SlopeBias/Weight to be applied on ANN (train)
    void ApproximatorNeuralNetwork::CalcChangeTerms( const FeatureReference< float> &FEATURE, const size_t &THREAD_ID)
    {
      // get the thread-local arrays
      const storage::Vector< linal::Vector< float> > &errors( m_Errors( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      storage::Vector< linal::Vector< float> > &slopes_bias( m_SlopesBias( THREAD_ID));
      storage::Vector< linal::Matrix< float> > &slopes_weight( m_SlopesWeight( THREAD_ID));

      // first layer
      // m_SlopesWeight( THREAD_ID)( 0) += math::OuterProduct( errors( 0), FEATURE)
      linal::AddOuterProductToMatrix( slopes_weight( 0), errors( 0), FEATURE);

      // all other layers
      for( size_t i( 1), number_hidden_layers( hidden.GetSize()); i < number_hidden_layers; ++i)
      {
        // m_SlopesWeight( THREAD_ID)( i) += math::OuterProduct( errors( i), FEATURE)
        linal::AddOuterProductToMatrix( slopes_weight( i), errors( i), hidden( i - 1));
      }

      // add the errors to the slopes for the biases directly
      for( size_t i( 0), number_hidden_layers( hidden.GetSize()); i < number_hidden_layers; ++i)
      {
        slopes_bias( i) += errors( i);
      }
    } // CalcChangeTerms

    //! train ANN with a feature
    void ApproximatorNeuralNetwork::TrainThread( const size_t &THREAD_ID)
    {
      // zero out the slopes vectors / matrices
      for
      (
        size_t layer_number( 0), number_hidden_layers( m_SlopesBias( THREAD_ID).GetSize());
        layer_number < number_hidden_layers;
        ++layer_number
      )
      {
        m_SlopesBias( THREAD_ID)( layer_number) = float( 0);
        m_SlopesWeight( THREAD_ID)( layer_number) = float( 0);
      }

      // only run the data for this range if it actually existed
      // it may not if m_DataSetRanges.GetSize() is not a multiple of the number of threads
      if( THREAD_ID + m_DataSetRangePosition < m_DataSetRanges.GetSize())
      {
        // the data position is influenced by the data set range position and the thread id
        const math::Range< size_t> &data( m_DataSetRanges( THREAD_ID + m_DataSetRangePosition));
        for
        (
          size_t feature_id( data.GetMin()), feature_end( data.GetMax());
          feature_id < feature_end;
          ++feature_id
        )
        {
          // training step
          const size_t eff_id( m_TrainingSchedule( feature_id));
          const FeatureReference< float> &feature( m_TrainingData->GetFeaturesPtr()->operator()( eff_id));
          UpdateDroppedNeurons( THREAD_ID);
          CalcHiddenTerms( feature, THREAD_ID, eff_id);
          if( CalcErrorTerms( m_TrainingData->GetResultsPtr()->operator()( eff_id), THREAD_ID, eff_id))
          {
            CalcChangeTerms( feature, THREAD_ID);
            ++m_NumberFeaturesUpdated( THREAD_ID);
          }
        }
      }
    } // TrainThread

    void ApproximatorNeuralNetwork::UpdateWeights()
    {
      if( m_UpdateEveryNthFeature == size_t( 1))
      {
        // handle dropout for all hidden neurons (except the output layer)
        for
        (
          size_t layer_number( 1), output_layer_number( m_NumberToDrop.GetSize());
          layer_number < output_layer_number;
          ++layer_number
        )
        {
          if( !m_NumberToDrop( layer_number))
          {
            // nothing to drop, continue
            continue;
          }
          // shuffle the first n-indices in the m_NeuronIndices array
          storage::Vector< size_t>::const_iterator
            itr_chosen( m_ChosenDropped( 0)( layer_number).Begin()),
            itr_chosen_end( m_ChosenDropped( 0)( layer_number).End());
          linal::Matrix< float> &weight_matrix( m_SlopesWeight( 0)( layer_number - 1));
          linal::Vector< float> &bias_vector( m_SlopesBias( 0)( layer_number - 1));
          for( ; itr_chosen != itr_chosen_end; ++itr_chosen)
          {
            weight_matrix.GetRow( *itr_chosen) = util::GetUndefined< float>();
            bias_vector( *itr_chosen) = util::GetUndefined< float>();
          }
        }
        for
        (
          size_t layer_number( 0), output_layer_number( m_NumberToDrop.GetSize());
          layer_number < output_layer_number;
          ++layer_number
        )
        {
          if( !m_NumberToDrop( layer_number))
          {
            // nothing to drop, continue
            continue;
          }
          // shuffle the first n-indices in the m_NeuronIndices array
          storage::Vector< size_t>::const_iterator
            itr_chosen( m_ChosenDropped( 0)( layer_number).Begin()),
            itr_chosen_end( m_ChosenDropped( 0)( layer_number).End());
          linal::Matrix< float> &weight_matrix( m_SlopesWeight( 0)( layer_number));
          for( ; itr_chosen != itr_chosen_end; ++itr_chosen)
          {
            weight_matrix.SetCol( *itr_chosen, util::GetUndefined< float>());
          }
        }
      }

      for
      (
        size_t layer_number( 0), output_layer_number( m_Weight.GetSize());
        layer_number < output_layer_number;
        layer_number++
      )
      {
        // update the bias using the desired propagation method
        ( *m_WeightUpdaters( layer_number))
        (
          m_Weight( layer_number).Begin(),
          m_SlopesWeight( 0)( layer_number).Begin()
        );

        // update the bias using the desired propagation method
        ( *m_BiasUpdaters( layer_number))
        (
          m_Bias( layer_number).Begin(),
          m_SlopesBias( 0)( layer_number).Begin()
        );
      }
    } // UpdateWeights

    //! @brief update dropped neurons
    //! @param THREAD_ID id of the thread for which to update dropped neurons. If set to undefined, all threads will gbe
    //!        updated
    void ApproximatorNeuralNetwork::UpdateDroppedNeurons( const size_t &THREAD_ID)
    {
      if( !util::IsDefined( THREAD_ID))
      {
        for( size_t thread( 0); thread < m_NumberThreads; ++thread)
        {
          UpdateDroppedNeurons( thread);
        }
        return;
      }
      for
      (
        size_t layer_number( 0), output_layer_number( m_Weight.GetSize());
        layer_number < output_layer_number;
        layer_number++
      )
      {
        if( !m_NumberToDrop( layer_number))
        {
          // nothing to drop, continue
          continue;
        }
        storage::Vector< size_t> &chosen_array( m_ChosenDropped( THREAD_ID)( layer_number));
        const size_t nr_blocks( layer_number ? m_DropoutPartitions( layer_number - 1) + 1 : 1);
        const size_t n_to_drop_per_block( m_NumberToDrop( layer_number));
        for( size_t block_number( 0), neuron( 0); block_number < nr_blocks; ++block_number)
        {
          // shuffle the first n-indices in the m_NeuronIndices array
          storage::Vector< size_t> &indices_array( m_NeuronIndices( THREAD_ID)( layer_number)( block_number));
          const size_t layer_size( indices_array.GetSize());
          for( size_t block_neuron( 0); block_neuron < n_to_drop_per_block; ++block_neuron, ++neuron)
          {
            std::swap( indices_array( block_neuron), indices_array( random::GetGlobalRandom().Random( block_neuron, layer_size - 1)));
            chosen_array( neuron) = indices_array( block_neuron);
          }
        }
        if( m_UpdateEveryNthFeature == 0 || m_UpdateEveryNthFeature > chosen_array.GetSize())
        {
          chosen_array.Sort( std::less< size_t>());
        }
      }
    } // UpdateWeights

    //! @brief sets up the neural network with a particular architecture, after training data was set
    void ApproximatorNeuralNetwork::SetupArchitecture()
    {
      BCL_Assert
      (
        m_TrainingData.IsDefined(),
        "SetupArchitecture requires valid training data!"
      );

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      // add the output neurons
      architecture.PushBack( m_TrainingData->GetResultSize());

      m_Bias.Reset();
      m_Weight.Reset();
      m_Bias.AllocateMemory( architecture.GetSize() - 1);
      m_Weight.AllocateMemory( architecture.GetSize() - 1);

      if( m_TrainingData->GetSize() == 0)
      {
        // no training data, can't set the architecture up yet
        return;
      }

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      // initialize data for the given architecture
      for( size_t i( 1); i < architecture.GetSize(); ++i)
      {
        m_Bias.PushBack( linal::Vector< float>( architecture( i), 0.0));
        m_Weight.PushBack( linal::Matrix< float>( architecture( i), architecture( i - 1), 0.0));
      }

      // randomize the initial values of bias and weight
      storage::Vector< size_t>::const_iterator itr_arch( architecture.Begin());
//      for
//      (
//        storage::Vector< linal::Vector< float> >::iterator itr( m_Bias.Begin()), itr_end( m_Bias.End());
//        itr != itr_end;
//        ++itr, ++itr_arch
//      )
//      {
//        for( float *itr_bias( itr->Begin()), *itr_bias_end( itr->End()); itr_bias < itr_bias_end; ++itr_bias)
//        {
//          *itr_bias = random::GetGlobalRandom().RandomGaussian( 0.0, 0.1);
//        }
//      }

      itr_arch = architecture.Begin();
      for
      (
        storage::Vector< linal::Matrix< float> >::iterator itr( m_Weight.Begin()), itr_end( m_Weight.End());
        itr != itr_end;
        ++itr, ++itr_arch
      )
      {
        float std( math::Sqrt( 1.0 / float( *itr_arch)));
        BCL_MessageStd( "std: " + util::Format()( std));
        if( m_ConnectionDensity >= float( 1.0) || itr == m_Weight.Last())
        {
          for( float *itr_weight( itr->Begin()), *itr_weight_end( itr->End()); itr_weight < itr_weight_end; ++itr_weight)
          {
            *itr_weight = random::GetGlobalRandom().RandomGaussian( 0.0, std);
          }
        }
        else
        {
          for( float *itr_weight( itr->Begin()), *itr_weight_end( itr->End()); itr_weight < itr_weight_end; ++itr_weight)
          {
            if( random::GetGlobalRandom().Double() < m_ConnectionDensity)
            {
              *itr_weight = random::GetGlobalRandom().RandomGaussian( 0.0, std);
            }
          }
        }
      }
    }

    //! @brief sets up the weight updaters and change weights
    //!        this has to be done unless the iterate is read from a file
    void ApproximatorNeuralNetwork::InitializeWeightUpdaters()
    {
      // create the architecture, excluding input neurons (for which there are no weight updaters)
      storage::Vector< size_t> architecture( m_HiddenArchitecture);

      // add the output neurons
      architecture.PushBack( m_TrainingData->GetResultSize());

      m_BiasUpdaters.Reset();
      m_WeightUpdaters.Reset();
      m_IterationWeightUpdaters.Reset();
      m_BiasUpdaters.AllocateMemory( architecture.GetSize() - 1);
      m_WeightUpdaters.AllocateMemory( architecture.GetSize() - 1);
      m_IterationWeightUpdaters.AllocateMemory( architecture.GetSize() - 1);

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      if( !m_BiasUpdateType.IsDefined())
      {
        m_BiasUpdateType = m_WeightUpdateType;
      }

      // initialize data for the given architecture
      for( size_t i( 0); i < architecture.GetSize(); ++i)
      {
        // initialize weight updaters for the given architecture
        m_BiasUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
        m_WeightUpdaters.PushBack( util::CloneToShPtr( *m_WeightUpdateType));
        m_BiasUpdaters.LastElement()->Initialize( m_Bias( i).GetSize());
        m_WeightUpdaters.LastElement()->Initialize( m_Weight( i).GetNumberOfElements());
        if( m_IterationWeightUpdateType.IsDefined())
        {
          m_IterationWeightUpdaters.PushBack( util::CloneToShPtr( *m_IterationWeightUpdateType));
        }
      }
    }

    //! @brief sets the data set ranges up for the # of threads
    void ApproximatorNeuralNetwork::SetupDataSetRanges()
    {
      BCL_Assert
      (
        m_TrainingData.IsDefined(),
        "SetupDataSetRanges requires valid training data!"
      );

      size_t features_per_weight_update( m_UpdateEveryNthFeature);
      // 0 indicates batch mode, so if the m_UpdateEveryNthFeature was 0, use the data set size
      if( m_UpdateEveryNthFeature == size_t( 0))
      {
        features_per_weight_update = m_TrainingSchedule.GetSize();
      }

      // redetermine the # of threads, since this influences how the data is split up
      m_NumberThreads = std::min( features_per_weight_update, sched::GetNumberCPUs());

      BCL_MessageStd( "Set up data set ranges with # threads: " + util::Format()( m_NumberThreads));

      // set up the training ranges for each thread
      m_DataSetRanges.Reset();

      // calculate the # of weigh updates per run through the data set
      const size_t number_epochs_per_data_set
      (
        ( m_TrainingSchedule.GetSize() - 1) / features_per_weight_update + 1
      );
      features_per_weight_update = m_TrainingSchedule.GetSize() / number_epochs_per_data_set;
      m_DataSetRanges.AllocateMemory( number_epochs_per_data_set * m_NumberThreads);

      // if m_TrainingData->GetSize() is not a multiple of features_per_weight_update, distribute the extra features
      // over the initial number_epochs_with_extra_feature epochs
      const size_t number_epochs_with_extra_feature( m_TrainingSchedule.GetSize() % features_per_weight_update);

      size_t current_index( 0);

      for( size_t epoch_number( 0); epoch_number < number_epochs_per_data_set; ++epoch_number)
      {
        // determine the # of features that will be examined in this epoch
        const size_t features_in_epoch( features_per_weight_update + size_t( epoch_number < number_epochs_with_extra_feature));

        // # of features per thread per epoch
        const size_t min_features_per_thread( features_in_epoch / m_NumberThreads);

        // if features_per_weight_update is not evenly divisible by m_NumberThreads, then the extra features are distributed
        // over the initial number_threads_with_extra_feature threads
        const size_t number_threads_with_extra_feature( features_in_epoch % m_NumberThreads);

        // make data set ranges for each thread for this epoch
        for( size_t current( 0); current < m_NumberThreads; ++current)
        {
          const size_t current_width( min_features_per_thread + size_t( current < number_threads_with_extra_feature));

          m_DataSetRanges.PushBack
          (
            math::Range< size_t>
            (
              current_index,
              current_index + current_width
            )
          );
          current_index += current_width;
        }
      }

      m_RMSDError = linal::Vector< float>( m_NumberThreads, 0.0);

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      // add the output neurons
      const size_t result_size( m_TrainingData->GetResultSize());
      architecture.PushBack( result_size);

      // set up separate temporary vectors and matrices for each thread
      storage::Vector< linal::Vector< float> > temp_vector_prototype( architecture.GetSize() - 1);
      storage::Vector< linal::Matrix< float> > temp_matrix_prototype( architecture.GetSize() - 1);

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      // initialize data for the given architecture
      for( size_t i( 1); i < architecture.GetSize(); ++i)
      {
        temp_vector_prototype( i - 1) = linal::Vector< float>( architecture( i), 0.0);
        temp_matrix_prototype( i - 1) = linal::Matrix< float>( architecture( i), architecture( i - 1), 0.0);
      }

      // set the temporary vectors equal to each other
      if( ( !m_Dropout.IsEmpty() && size_t( m_Dropout( 0) * architecture( 0))) || m_NoiseZScore)
      {
        m_FeatureWithDropout
          = storage::Vector< linal::Vector< float> >( m_NumberThreads, linal::Vector< float>( architecture( 0), 0.0));
      }
      m_SlopesBias = m_Errors = m_HiddenInput = m_Hidden =
        storage::Vector< storage::Vector< linal::Vector< float> > >( m_NumberThreads, temp_vector_prototype);
      m_SlopesWeight =
        storage::Vector< storage::Vector< linal::Matrix< float> > >( m_NumberThreads, temp_matrix_prototype);
      m_NumberFeaturesUpdated = linal::Vector< size_t>( m_NumberThreads, size_t( 0));

      BCL_MessageStd( "Set up data set ranges with # ranges: " + util::Format()( m_DataSetRanges.GetSize()));
      BCL_MessageStd( "Set up data set ranges for updating every nth feature: " + util::Format()( features_per_weight_update));
      m_ThreadRandomNumberGenerators
        = storage::Vector< random::UniformDistribution>
          (
            m_NumberThreads,
            random::UniformDistribution( random::GetGlobalRandom().GetSeed())
          );
      m_NeuronIndices.Resize( m_NumberThreads, m_NeuronIndices( 0));
      m_ChosenDropped.Resize( m_NumberThreads, m_ChosenDropped( 0));
    } // SetupDataSetRanges

    //! @brief helper function to initialize dropout-related vectors
    void ApproximatorNeuralNetwork::InitializeDropout()
    {
      // every hidden layer and the input layer can incorporate dropout
      const size_t number_potential_dropout_layers( m_HiddenArchitecture.GetSize() + 1);
      // handle dropout
      if( m_Dropout.IsEmpty())
      {
        m_Dropout = linal::Vector< float>( number_potential_dropout_layers, float( 0));
      }
      else if( m_Dropout.GetSize() > number_potential_dropout_layers)
      {
        BCL_Exit( "Too many dropout layers specified!", -1);
      }
      else if( m_Dropout.GetSize() < number_potential_dropout_layers)
      {
        BCL_MessageCrt( "Assuming layers > " + util::Format()( m_Dropout.GetSize()) + " have no dropout!");
        linal::Vector< float> tmp( number_potential_dropout_layers, float( 0));
        std::copy( m_Dropout.Begin(), m_Dropout.End(), tmp.Begin());
        m_Dropout = tmp;
      }

      // the only thing special about dropout-immune neurons is that they cannot occur in the visible layer, since
      // that really does not make any sense
      if( m_DropoutPartitions.IsEmpty())
      {
        m_DropoutPartitions = linal::Vector< size_t>( m_HiddenArchitecture.GetSize(), size_t( 0));
      }
      else if( m_DropoutPartitions.GetSize() > m_HiddenArchitecture.GetSize())
      {
        BCL_Exit( "Too many dropout-immune layers specified!", -1);
      }
      else if( m_DropoutPartitions.GetSize() < m_HiddenArchitecture.GetSize())
      {
        linal::Vector< size_t> tmp( m_HiddenArchitecture.GetSize() - 1, size_t( 0));
        std::copy( m_DropoutPartitions.Begin(), m_DropoutPartitions.End(), tmp.Begin());
        m_DropoutPartitions = tmp;
      }

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      m_NumberToDrop = linal::Vector< size_t>( number_potential_dropout_layers, size_t( 0));
      m_NumberToDrop( 0) = architecture( 0) * m_Dropout( 0);
      for( size_t layer( 1), n_layers( architecture.GetSize()); layer < n_layers; ++layer)
      {
        const size_t n_dropout_blocks_this_layer( m_DropoutPartitions( layer - 1) + 1);
        m_NumberToDrop( layer) = architecture( layer) * m_Dropout( layer) / float( n_dropout_blocks_this_layer);
        if( m_Dropout( layer) > 0.0 && m_NumberToDrop( layer) == 0)
        {
          BCL_MessageCrt
          (
            "Warning; a dropout value was specified it is too small for at least one neuron to be dropped from every "
            "partition of layer " + util::Format()( layer)
          );
        }
      }
      storage::Vector< storage::Vector< storage::Vector< size_t> > > neuron_indices_tmpl( number_potential_dropout_layers);
      storage::Vector< storage::Vector< size_t> > chosen_dropped_tmpl( number_potential_dropout_layers);
      for( size_t layer( 0), n_layers( architecture.GetSize()); layer < n_layers; ++layer)
      {
        const size_t dropout_prone( architecture( layer));
        const size_t n_dropout_blocks( layer ? m_DropoutPartitions( layer - 1) + 1 : 1);
        neuron_indices_tmpl( layer).Resize( n_dropout_blocks, storage::Vector< size_t>( dropout_prone));

        chosen_dropped_tmpl( layer).Resize( m_NumberToDrop( layer) * n_dropout_blocks);
        const size_t base_neurons_per_block( dropout_prone / n_dropout_blocks);
        const size_t nr_blocks_extra_neuron( dropout_prone % n_dropout_blocks);
        for( size_t block_nr( 0), neuron( 0); block_nr < n_dropout_blocks; ++block_nr)
        {
          const size_t nr_neurons_this_block( base_neurons_per_block + ( block_nr < nr_blocks_extra_neuron));
          for( size_t block_neuron( 0); block_neuron < nr_neurons_this_block; ++block_neuron, ++neuron)
          {
            neuron_indices_tmpl( layer)( block_nr)( block_neuron) = neuron;
          }
        }
      }
      m_NeuronIndices.Reset();
      m_NeuronIndices.Resize( size_t( 1), neuron_indices_tmpl);
      m_ChosenDropped.Reset();
      m_ChosenDropped.Resize( size_t( 1), chosen_dropped_tmpl);
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_neural_network_selective.h"

// includes from bcl - sorted alphabetically
#include "iterate/bcl_iterate_reflecting.h"
#include "linal/bcl_linal_vector_operations.h"
#include "linal/bcl_linal_vector_reference.h"
#include "math/bcl_math_statistics.h"
#include "model/bcl_model_neural_network_selective_backpropagation_default.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorNeuralNetworkSelective::s_IterateInstance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorNeuralNetworkSelective( false))
    );
    const util::SiPtr< const util::ObjectInterface> ApproximatorNeuralNetworkSelective::s_PretrainInstance
    (
      util::Enumerated< PretrainNeuralNetworkInterface>::AddInstance( new ApproximatorNeuralNetworkSelective( true))
    );

    //! @brief InputLayerDropoutType as string
    //! @param TYPE the type
    //! @return the string for the type
    const std::string &ApproximatorNeuralNetworkSelective::GetInputLayerDropoutTypeName( const InputLayerDropoutType &TYPE)
    {
      static const std::string s_names[] =
      {
        "Zero",
        "Noise",
        "CopySingleRandomFeature",
        "CopyEachRandomFeature",
        "CopySingleRandomPeerFeature",
        "CopyEachRandomPeerFeature",
        "CopySingleBalancedFeature",
        "CopyEachBalancedFeature",
        GetStaticClassName< ApproximatorNeuralNetworkSelective::InputLayerDropoutType>()
      };
      return s_names[ size_t( TYPE)];
    }

    //! @brief default constructor
    ApproximatorNeuralNetworkSelective::ApproximatorNeuralNetworkSelective( const bool &PRETRAIN) :
      m_UpdateEveryNthFeature( 0),
      m_IterationsPerRMSDMessage( 1),
      m_Shuffle( false),
      m_ConnectionDensity( 1.0),
      m_AlignCutoff( false),
      m_RescaleOutputDynamicRange( true),
      m_DataSetRangePosition( 0),
      m_TransferFunction(),
      m_DataSelector( NeuralNetworkSelectiveBackpropagationDefault()),
      m_IsPretrainer( PRETRAIN),
      m_NumIterations( 0),
      m_Balance( false),
      m_BalanceMaxRepeatedFeatures( 1000000),
      m_BalanceMaxOversampling( 1.0),
      m_NoiseZScore( 0.0),
      m_InputDropoutType( e_Zero),
      m_RescaleType( RescaleFeatureDataSet::e_AveStd)
    {
    }

    //! @brief constructor from training data, transfer, rescale, and objective functions
    //! @param TRAINING_DATA data to train the NeuralNetwork on
    //! @param UPDATE_EVERY_NTH_FEATURE how often the weights get updated
    //! @param ARCHITECTURE the # of neurons in each hidden layer of the network
    //! @param TRANSFER_FUNCTION ShPtr to the transfer function between input and output of each neuron
    //! @param OBJECTIVE_FUNCTION ShPtr to objective function
    //! @param WEIGHT_UPDATE_FUNCTION method by which to update the weights
    //! @param ITERATIONS_PER_RMSD_REPORT # iterations per report of the rmsd
    ApproximatorNeuralNetworkSelective::ApproximatorNeuralNetworkSelective
    (
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t UPDATE_EVERY_NTH_FEATURE,
      const storage::Vector< size_t> &ARCHITECTURE,
      const util::Implementation< TransferFunctionInterface> &TRANSFER_FUNCTION,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const util::Implementation< NeuralNetworkUpdateWeightsInterface> &WEIGHT_UPDATE_FUNCTION,
      const size_t &ITERATIONS_PER_RMSD_REPORT,
      const RescaleFeatureDataSet::TypeEnum RESCALE_TYPE
    ) :
      ApproximatorBase( OBJECTIVE_FUNCTION),
      m_UpdateEveryNthFeature( UPDATE_EVERY_NTH_FEATURE),
      m_IterationsPerRMSDMessage( ITERATIONS_PER_RMSD_REPORT),
      m_Shuffle( false),
      m_ConnectionDensity( 1.0),
      m_AlignCutoff( false),
      m_RescaleOutputDynamicRange( true),
      m_DataSetRangePosition( 0),
      m_HiddenArchitecture( ARCHITECTURE),
      m_TransferFunction( TRANSFER_FUNCTION),
      m_WeightUpdateType( WEIGHT_UPDATE_FUNCTION),
      m_DataSelector( NeuralNetworkSelectiveBackpropagationDefault()),
      m_IsPretrainer( false),
      m_NumIterations( 0),
      m_Balance( false),
      m_BalanceMaxRepeatedFeatures( 1000000),
      m_BalanceMaxOversampling( 1.0),
      m_NoiseZScore( 0.0),
      m_InputDropoutType( e_Zero),
      m_RescaleType( RESCALE_TYPE)
    {
      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new ApproximatorNeuralNetworkSelective copied from this instance
    ApproximatorNeuralNetworkSelective *ApproximatorNeuralNetworkSelective::Clone() const
    {
      return new ApproximatorNeuralNetworkSelective( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorNeuralNetworkSelective::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorNeuralNetworkSelective::GetAlias() const
    {
      static const std::string s_Name( "NeuralNetworkSelective");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorNeuralNetworkSelective::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_TrainingData = DATA;
      if( !DATA->GetFeaturesPtr()->IsRescaled())
      {
        DATA->GetFeatures().Rescale( NeuralNetwork::s_DefaultInputRange, m_RescaleType);
      }
      if( !DATA->GetResultsPtr()->IsRescaled())
      {
        DATA->GetResults().Rescale
        (
          m_RescaleOutputDynamicRange
          ? m_TransferFunction->GetDynamicOutputRange()
          : m_TransferFunction->GetOutputRange()
        );
      }
      m_RescaleOutputLastRound = DATA->GetResultsPtr()->GetScaling();

      m_Order.Resize( m_TrainingData->GetSize());
      for( size_t i( 0), n_features( m_TrainingData->GetSize()); i < n_features; ++i)
      {
        m_Order( i) = i;
      }
      if( m_Balance)
      {
        BalanceFeatures( m_BalanceMaxRepeatedFeatures, m_BalanceMaxOversampling);
      }
      if( int( m_InputDropoutType) >= int( s_InputLayerDropoutFirstClassBasedMethod))
      {
        if( m_Dropout.IsEmpty() || !m_Dropout( 0))
        {
          BCL_MessageCrt
          (
            m_InputDropoutType.GetString() + " has no effect because no neurons are to be dropped in the input layer"
          );
        }
        if( !util::IsDefined( m_ObjectiveFunction->GetThreshold()))
        {
          const std::string &old_input_dropout_type( m_InputDropoutType.GetString());
          m_InputDropoutType
            = (
                m_InputDropoutType == e_CopyEachRandomPeerFeature || m_InputDropoutType == e_CopyEachBalancedFeature
                ? e_CopyEachRandomFeature
                : e_CopySingleRandomFeature
               );
          BCL_MessageCrt
          (
            old_input_dropout_type
            + " has no effect because the objective function does not have a cutoff, switching to "
            + m_InputDropoutType.GetString()
          );
        }
        DetermineResultClasses();
      }

      BCL_MessageStd
      (
        "Setting up training data with " + util::Format()( m_TrainingData->GetSize()) + " points"
      );

      // initialize the dropout, if necessary
      InitializeDropout();

      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Iterate properly
      if( !IsTrainingContinued())
      {
        if( m_Pretrainer.IsDefined())
        {
          m_Pretrainer->SetIds( ApproximatorBase::GetIdCode());
          m_Pretrainer->SetFeatures( ApproximatorBase::GetFeatureCode());
          m_Pretrainer->SetResults( ApproximatorBase::GetResultCode());

          BCL_MessageStd( "Pretraining with method: " + m_Pretrainer->GetString());
          util::ShPtr< NeuralNetwork> initial_network( m_Pretrainer->PretrainNetwork( DATA, m_ObjectiveFunction));
          // ensure that a model was pretrained
          if( !initial_network.IsDefined())
          {
            BCL_Exit( "Pretraining failed!", -1);
          }

          // copy the bias and weight from the model
          m_Bias = initial_network->GetBias();
          m_Weight = initial_network->GetWeight();

          // copy the hidden architecture from the model
          linal::Vector< size_t> old_hidden_architecture( m_HiddenArchitecture);
          m_HiddenArchitecture = initial_network->GetArchitecture();
          // remove the first and last elements, which are the input and output layer sizes, respectively
          m_HiddenArchitecture.RemoveElements( 0, 1);
          m_HiddenArchitecture.RemoveElements( m_HiddenArchitecture.GetSize() - 1, 1);

          // need to adjust weights if performing dropout in this layer
          if( m_NumberToDrop.Sum())
          {
            // must multiply m_Weight( i) by (1-m_Dropout( i))
            for( size_t layer( 0), n_layers( m_Weight.GetSize()); layer < n_layers; ++layer)
            {
              if( !m_NumberToDrop( layer))
              {
                continue;
              }
              const size_t neurons_this_layer( m_Weight( layer).GetNumberCols());
              const float kept_fraction( float( neurons_this_layer) / float( neurons_this_layer - m_NumberToDrop( layer)));
              m_Weight( layer) *= kept_fraction;
            }
          }

          // make sure that either no hidden architecture was specified or that it agreed with the architecture implicit
          // in the model
          if
          (
            !old_hidden_architecture.IsEmpty()
            && linal::Vector< size_t>( m_HiddenArchitecture) != old_hidden_architecture
          )
          {
            BCL_MessageCrt
            (
              "Ignoring hidden architecture given in iterate; using implicit architecture from initial network file"
            );
          }
        }
      }

      // set the data ranges up for threading (if applicable)
      SetupDataSetRanges();

      m_DataSelector->Initialize( *m_TrainingData, *m_ObjectiveFunction->GetImplementation(), m_NumberThreads);
      m_DataSelector->ConstitutionMapping( m_Order);

      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Iterate properly
      if( !IsTrainingContinued())
      {
        // if the initial weights and bias were not read in from a file, they need to be initialized
        if( !m_Pretrainer.IsDefined())
        {
          // set the architecture (m_Bias, m_Weights, etc) up using m_HiddenArchitecture
          SetupArchitecture();
        }
        else
        {
          // destroy the pretrainer to free up memory
          m_Pretrainer = m_Pretrainer.GetLabel();
        }
        // weight updaters need to be initialized regardless
        InitializeWeightUpdaters();
      }

      // update neurons for dropout
      UpdateDroppedNeurons();

    } // SetTrainingData

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorNeuralNetworkSelective::GetCurrentModel() const
    {
      if( m_NumberToDrop.Sum())
      {
        // must multiply m_Weight( i) by (1-m_Dropout( i)).  To avoid unnecessary creation/destruction of matrices,
        // use m_SlopesWeight
        for( size_t layer( 0), n_layers( m_Weight.GetSize()); layer < n_layers; ++layer)
        {
          m_SlopesWeight( 0)( layer) = m_Weight( layer);
          if( m_NumberToDrop( layer))
          {
            const float n_dropped( m_ChosenDropped( layer).GetSize());
            if( layer || m_InputDropoutType == e_Noise || m_InputDropoutType == e_Zero || m_InputDropoutType == e_CopyEachRandomFeature)
            {
              const float kept_fraction( 1.0 - n_dropped / float( m_Weight( layer).GetNumberCols()));
              m_SlopesWeight( 0)( layer) *= kept_fraction;
            }
//            else if( m_InputDropoutType == e_CopyEachBalancedFeature || m_InputDropoutType == e_CopySingleBalancedFeature)
//            {
//              const float n_classes( m_PeerFeatures.GetSize());
//              const float kept_fraction( 1.0 - n_dropped / float( m_Weight( layer).GetNumberCols() * n_classes));
//              m_SlopesWeight( 0)( layer) *= kept_fraction;
//            }
          }
        }
        return util::ShPtr< Interface>
               (
                 new NeuralNetwork
                 (
                   GetRescaleFeatureDataSet(),
                   m_RescaleOutputLastRound,
                   m_Bias,
                   m_SlopesWeight( 0),
                   m_TransferFunction
                 )
               );
      }
      // make a new model out of the current data members
      return util::ShPtr< Interface>
      (
        new NeuralNetwork
        (
          GetRescaleFeatureDataSet(),
          m_RescaleOutputLastRound,
          m_Bias,
          m_Weight,
          m_TransferFunction
        )
      );
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorNeuralNetworkSelective::GetCurrentApproximation() const
    {
      util::ShPtr< Interface> model( GetCurrentModel());
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>
          (
            model,
            m_ObjectiveFunction->operator ()( model)
          )
        );
    }

    //! @brief the main operation, pretrains a neural network
    //! @param DATA the data for use in pretraining
    //! @param OBJECTIVE ShPtr to the objective function for the network
    util::ShPtr< NeuralNetwork> ApproximatorNeuralNetworkSelective::PretrainNetwork
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE
    )
    {
      this->SetObjectiveFunction( OBJECTIVE);
      this->SetTrainingData( DATA);
      while( this->GetTracker().GetIteration() < m_NumIterations)
      {
        this->Next();
      }
      return GetCurrentModel();
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorNeuralNetworkSelective::Next()
    {
      // handle shuffling
      if( m_Shuffle)
      {
        m_Order.Shuffle();
      }

      util::ShPtrVector< sched::JobInterface> all_jobs( m_NumberThreads);

      const size_t group_id( 0);

      // store the id of each thread for use by the jobs and reset all results tallying objects.
      storage::Vector< size_t> thread_ids( m_NumberThreads);
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
        m_RMSDError( thread_number) = float( 0.0);
      }

      // run through the data set once, updating weights after every epoch (m_UpdateEveryNthFeature features, data set size by default)
      // launch jobs to train the NN with the features for this epoch
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        all_jobs( thread_number)
          = util::ShPtr< sched::JobInterface>
            (
              new sched::UnaryFunctionJobWithData
              <
                const size_t,
                void,
                ApproximatorNeuralNetworkSelective
              >
              (
                group_id,
                *this,
                &ApproximatorNeuralNetworkSelective::TrainThread,
                thread_ids( thread_number),
                sched::JobInterface::e_READY,
                NULL
              )
            );
      }

      m_DataSetRangePosition = 0;
      size_t n_updated = 0;
      while( m_DataSetRangePosition < m_DataSetRanges.GetSize())
      {
        if( m_NumberThreads > size_t( 1))
        {
          for( size_t thread_number( 0); thread_number < m_NumberThreads; thread_number++)
          {
            sched::GetScheduler().SubmitJob( all_jobs( thread_number));
          }

          sched::GetScheduler().Join( all_jobs( 0));
        }
        else
        {
          // just run the job directly to avoid the scheduler's overhead
          all_jobs( 0)->Run();
        }

        size_t n_updated_this_time( 0);
        n_updated_this_time += m_NumberFeaturesUpdated( 0);
        m_NumberFeaturesUpdated( 0) = 0;
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          sched::GetScheduler().Join( all_jobs( thread_number));
          n_updated_this_time += m_NumberFeaturesUpdated( thread_number);

          if( m_NumberFeaturesUpdated( thread_number))
          {
            // accumulate the slopes from the newly joined jobs with the slopes from the earlier jobs
            for
            (
              size_t hidden_layer_number( 0), number_hidden_layers( m_HiddenArchitecture.GetSize());
              hidden_layer_number < number_hidden_layers;
              ++hidden_layer_number
            )
            {
              m_SlopesBias( 0)( hidden_layer_number) += m_SlopesBias( thread_number)( hidden_layer_number);
              m_SlopesWeight( 0)( hidden_layer_number) += m_SlopesWeight( thread_number)( hidden_layer_number);
            }
            m_NumberFeaturesUpdated( thread_number) = 0;
          }
        }

        if( n_updated_this_time)
        {
          UpdateWeights();
          n_updated += n_updated_this_time;
        }
        else
        {
          UpdateDroppedNeurons();
        }

        // move on the the next epoch
        m_DataSetRangePosition += m_NumberThreads;
      }

      if( ( this->GetTracker().GetIteration() % m_IterationsPerRMSDMessage) == size_t( 0))
      {
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          m_RMSDError( 0) += m_RMSDError( thread_number);
        }

        m_RMSDError( 0) /= m_TrainingData->GetResultSize();
        m_RMSDError( 0) /= m_TrainingData->GetSize();
        m_RMSDError( 0) = std::max( m_RMSDError( 0), float( 0.0));
        BCL_MessageStd
        (
          "Training relative RMSD for iteration " + util::Format()( this->GetTracker().GetIteration())
          + ( n_updated != m_Order.GetSize() ? ", " + util::Format()( n_updated) + " features trained" : std::string())
          + ": " + util::Format()( m_RMSDError( 0))
        );
      }

      m_NumberFeaturesUpdated = 0;

      m_DataSelector->FinalizeRound();

      m_DataSetRangePosition = 0;

      // get current model
      util::ShPtr< NeuralNetwork> current_model( GetCurrentModel());
      current_model->SetRescaleOutput( GetRescaleResultDataSet());

      // determine inaccuracy counts
      if( m_IterationWeightUpdateType.IsDefined())
      {
        linal::Matrix< float> old_weight( m_Weight( 0));
        ( *m_IterationWeightUpdaters( 0))( m_Weight( 0));
        float total_change( 0), total_weight( 0);
        for
        (
          const float *itr_old( old_weight.Begin()), *itr_old_end( old_weight.End()), *itr_new( m_Weight( 0).Begin());
          itr_old != itr_old_end;
          ++itr_old, ++itr_new
        )
        {
          total_change += math::Absolute( *itr_old - *itr_new);
          total_weight += math::Absolute( *itr_new);
        }
        BCL_MessageStd
        (
          "Average weight change: " + util::Format()( total_change / float( m_Weight( 0).GetNumberOfElements()))
          + " Average weight now : " + util::Format()( total_weight / float( m_Weight( 0).GetNumberOfElements()))
        );
      }

      if( m_IsPretrainer)
      {
        this->GetTracker().Track
        (
          util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
          (
            new storage::Pair< util::ShPtr< Interface>, float>
            (
              util::ShPtr< Interface>(),
              float( 0.0)
            )
          )
        );
        // act as a pretrainer for the specified number of steps, no need to create a model
        return;
      }

      // compute results on monitoring dataset
      FeatureDataSet< float> predicted_results( m_ObjectiveFunction->Predict( util::ShPtr< Interface>( current_model)));

      const float objective_result( m_ObjectiveFunction->Evaluate( predicted_results));
      if( m_AlignCutoff)
      {
        // optimize the rescaling function
        m_RescaleOutputLastRound = m_ObjectiveFunction->OptimizeRescalingFunction( GetRescaleResultDataSet(), predicted_results);
        current_model->SetRescaleOutput( m_RescaleOutputLastRound);
      }

      // combine it with the objective function evaluation
      this->GetTracker().Track
      (
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( current_model, objective_result)
        )
      );
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorNeuralNetworkSelective::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "trains a neural network (see http://en.wikipedia.org/wiki/Artificial_neural_network)"
      );

      parameters.AddInitializer
      (
        "transfer function",
        "function that translates input from neurons in the prior layer into the output of each hidden layer",
        io::Serialization::GetAgent( &m_TransferFunction),
        "Sigmoid"
      );
      parameters.AddInitializer
      (
        "weight update",
        "algorithm used to update the weights",
        io::Serialization::GetAgent( &m_WeightUpdateType),
        "Resilient"
      );
      parameters.AddOptionalInitializer
      (
        "bias update",
        "algorithm used to update the biases; if omitted, the weight update type is used",
        io::Serialization::GetAgent( &m_BiasUpdateType)
      );
      parameters.AddOptionalInitializer
      (
        "iteration weight update",
        "algorithm used to update the weights after every run through the data",
        io::Serialization::GetAgent( &m_IterationWeightUpdateType)
      );
      parameters.AddInitializer
      (
        "steps per update",
        "# of features seen between each update of weights (set to 0 to use the size of the training data set)",
        io::Serialization::GetAgent( &m_UpdateEveryNthFeature),
        "0"
      );
      parameters.AddOptionalInitializer
      (
        "hidden architecture",
        "# of neurons in each hidden layer, e.g. hidden architecture(100) declares a single hidden layer w/ 100 neurons. "
        "This variable can be omitted to train a single layer perception (logistic regression)",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_HiddenArchitecture,
          io::Serialization::GetAgentWithMin( size_t( 1)),
          0
        )
      );
      parameters.AddInitializer
      (
        "rmsd report frequency",
        "# of iterations between reports of rmsd on the last training batch",
        io::Serialization::GetAgentWithMin( &m_IterationsPerRMSDMessage, size_t( 1)),
        "1"
      );
      parameters.AddOptionalInitializer
      (
        "initial network",
        "method by which to create the initial network, if not randomly",
        io::Serialization::GetAgent( &m_Pretrainer)
      );
      parameters.AddInitializer
      (
        "shuffle",
        "primarily for non-batch update; if true, shuffle the order or data points between each run through the data",
        io::Serialization::GetAgent( &m_Shuffle),
        "False"
      );
      parameters.AddInitializer
      (
        "data selector",
        "method to use to select which data to backpropagate",
        io::Serialization::GetAgent( &m_DataSelector),
        "All"
      );
      parameters.AddInitializer
      (
        "connection density",
        "Fraction of connections made between input and hidden layers",
        io::Serialization::GetAgentWithRange( &m_ConnectionDensity, 0.0, 1.0),
        "1"
      );
      parameters.AddInitializer
      (
        "balance",
        "Whether to automatically balance each class (as defined by the objective function's cutoff, if applicable)",
        io::Serialization::GetAgent( &m_Balance),
        "False"
      );
      parameters.AddInitializer
      (
        "balance max repeats",
        "Applies only if balance=True; absolute maximum number of times that a feature can be repeated in order to reach"
        "the targeted ratio of positives to negatives",
        io::Serialization::GetAgent( &m_BalanceMaxRepeatedFeatures),
        "1000000"
      );
      parameters.AddInitializer
      (
        "balance target ratio",
        "Applies only if balance=True; target ratio between most-common and underrepresented class in the dataset "
        "achieved by data replication; to simulate normal balancing; this should be 1, but smaller values may yield "
        "more general models",
        io::Serialization::GetAgentWithRange( &m_BalanceMaxOversampling, float( 0.0), float( 1.0)),
        "1"
      );
      parameters.AddOptionalInitializer
      (
        "dropout",
        "fraction of neurons to have \"dropout\" (set to 0) for an entire weight-update session, per layer",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_Dropout,
          io::Serialization::GetAgentWithRange( float( 0.0), float( 1.0)),
          0
        )
      );
      parameters.AddOptionalInitializer
      (
        "dropout partitions",
        "# of partitions between neurons in each layer.  The number of neurons dropped within each partition will always"
        "be the same, so this allows difference regions of hidden neurons to learn different functionalities, which is"
        "particularly useful for multi-output ANNs",
        io::Serialization::GetAgentWithSizeLimits( &m_DropoutPartitions, size_t( 0))
      );
      parameters.AddInitializer
      (
        "scaling",
        "Type of input scaling. Normally AveStd works best, but MinMax and None may also be used in some circumstances",
        io::Serialization::GetAgent( &m_RescaleType),
        "AveStd"
      );
      parameters.AddInitializer
      (
        "input noise",
        "Amount of noise to apply to input features, in units of z-score",
        io::Serialization::GetAgentWithRange( &m_NoiseZScore, 0.0, 3.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "rescale output dynamic range",
        "if true, output will be rescaled to the dynamic range of the transfer function. Useful for "
        "regression, where the output of the function being predicted could feasibly extend beyond the range seen in the "
        "the training set by a marginal amount. It may also result in smaller weights. "
        "Classification problems often benefit from using the False setting",
        io::Serialization::GetAgent( &m_RescaleOutputDynamicRange),
        "True"
      );
      parameters.AddInitializer
      (
        "input dropout type",
        "Method of applying dropout to the input layer. Descriptions for each method:\n"
        "Zero                         <-- Set the dropped neurons to zero; fast, but biased\n"
        "Noise,                       <-- Set the neurons to a random gaussian value sampled according to the input mean/std\n"
        "CopySingleRandomFeature      <-- Copy dropped values from a single randomly-selected feature\n"
        "CopyEachRandomFeature        <-- Copy each dropped value from a randomly-selected feature (training example)\n"
        "CopySingleRandomPeerFeature  <-- Copy values from a single, randomly-selected peer feature\n"
        "CopyEachRandomPeerFeature    <-- Copy each value from a randomly-selected peer feature\n"
        "A peer is a training example that has the same class for every result column.  Use of input dropout types that "
        "have Peer in the name requires that a classification-based objective function is used (typically any objective "
        "function with a cutoff). The output class is a binary value of whether the column is above or below the cutoff"
        ". The balanced variants (CopySingleBalancedFeature, CopyEachBalancedFeature) select a random feature of a "
        " random class.",
        io::Serialization::GetAgent( &m_InputDropoutType),
        "Zero"
      );

      if( !m_IsPretrainer)
      {
        parameters.AddInitializer
        (
          "align cutoff",
          "Adjust output scaling automatically according to the objective functions.  This will only have an effect for "
          "objective functions such as enrichment and fpp vs ppv that have an internal FPR or similar cutoff.",
          io::Serialization::GetAgent( &m_AlignCutoff),
          "False"
        );
        parameters.AddInitializer
        (
          "objective function",
          "function that evaluates the model after each batch step",
          io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
          "RMSD"
        );
      }
      else
      {
        // pretraining-specific parameters
        parameters.AddInitializer
        (
          "iterations",
          "Number of complete iterations through the data to pretrain the network",
          io::Serialization::GetAgent( &m_NumIterations)
        );
      }

      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorNeuralNetworkSelective::Read( std::istream &ISTREAM)
    {
      descriptor::Dataset::s_Instance.IsDefined();

      // read members
      io::Serialize::Read( m_UpdateEveryNthFeature, ISTREAM);
      io::Serialize::Read( m_HiddenArchitecture, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Weight, ISTREAM);
      io::Serialize::Read( m_TransferFunction, ISTREAM);
      io::Serialize::Read( m_WeightUpdateType, ISTREAM);
      io::Serialize::Read( m_IterationWeightUpdateType, ISTREAM);
      io::Serialize::Read( m_BiasUpdaters, ISTREAM);
      io::Serialize::Read( m_WeightUpdaters, ISTREAM);
      io::Serialize::Read( m_IterationWeightUpdaters, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorNeuralNetworkSelective::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_UpdateEveryNthFeature, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_HiddenArchitecture, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Weight, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_TransferFunction, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightUpdateType, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_IterationWeightUpdateType, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_IterationWeightUpdaters, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

    //! run forward through ANN and compute hidden terms m_Hidden (test)
    void ApproximatorNeuralNetworkSelective::CalcHiddenTerms
    (
      const FeatureReference< float> &FEATURE,
      const size_t &THREAD_ID,
      const size_t &FEATURE_ID
    )
    {
      // get the thread-local errors, hidden, and hidden input arrays
      storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      storage::Vector< linal::Vector< float> > &hidden_input( m_HiddenInput( THREAD_ID));

      // input layer
      // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
      hidden_input( 0) = m_Bias( 0);
      if( m_NumberToDrop( 0) || m_NoiseZScore)
      {
        linal::Vector< float> &feature_with_dropout( m_FeatureWithDropout( THREAD_ID));
        std::copy( FEATURE.Begin(), FEATURE.End(), feature_with_dropout.Begin());
        linal::MatrixConstReference< float> allfeatures( m_TrainingData->GetFeaturesReference());
        if( m_NoiseZScore)
        {
          random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
          for( float *itr( feature_with_dropout.Begin()), *itr_end( feature_with_dropout.End()); itr != itr_end; ++itr)
          {
            *itr += rng.RandomGaussian( 0.0, m_NoiseZScore);
          }
        }
        if( m_NumberToDrop( 0))
        {
          const storage::Vector< size_t> &drop_ids( m_ChosenDropped( 0));
          if( m_InputDropoutType == e_Noise)
          {
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = rng.RandomGaussian( 0.0, 0.5);
            }
          }
          else if( m_InputDropoutType == e_Zero)
          {
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = 0.0;
            }
          }
          else if( m_InputDropoutType == e_CopySingleRandomFeature || m_InputDropoutType == e_CopyEachRandomFeature)
          {
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            const size_t max_feature_to_pick( m_TrainingData->GetSize() - 1);
            if( m_InputDropoutType == e_CopySingleRandomFeature)
            {
              const size_t random_feature( rng.Random( max_feature_to_pick));
              linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature));
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = selected_feature( *itr_drop);
              }
            }
            else
            {
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = allfeatures( rng.Random( max_feature_to_pick), *itr_drop);
              }
            }
          }
          else if
          (
            m_InputDropoutType == e_CopySingleRandomPeerFeature
            || m_InputDropoutType == e_CopyEachRandomPeerFeature
          )
          {
            // peer-based dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            const storage::Vector< size_t> &peers( m_PeerFeatures( m_ResultClass( FEATURE_ID)));
            const size_t max_feature_to_pick( peers.GetSize() - 1);
            if( m_InputDropoutType == e_CopySingleRandomPeerFeature)
            {
              const size_t random_feature( peers( rng.Random( max_feature_to_pick)));
              linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature));
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) += selected_feature( *itr_drop);
              }
            }
            else // if( m_InputDropoutType == e_CopyEachRandomPeerFeature)
            {
              for
              (
                storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
                itr_drop != itr_drop_end;
                ++itr_drop
              )
              {
                feature_with_dropout( *itr_drop) = allfeatures( peers( rng.Random( max_feature_to_pick)), *itr_drop);
              }
            }
          }
          else if( m_InputDropoutType == e_CopySingleBalancedFeature)
          {
            // single feature, selected randomly from randomly selected class dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            // choose which class to select the feature from
            const storage::Vector< size_t> &chosen_class( m_PeerFeatures( rng.Random( m_PeerFeatures.GetSize() - 1)));
            const size_t random_feature_id( chosen_class( rng.Random( chosen_class.GetSize() - 1)));
            linal::VectorConstReference< float> selected_feature( allfeatures.GetRow( random_feature_id));
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = selected_feature( *itr_drop);
            }
          }
          else if( m_InputDropoutType == e_CopyEachBalancedFeature)
          {
            // single feature, selected randomly from randomly selected class dropout types
            random::UniformDistribution &rng( m_ThreadRandomNumberGenerators( THREAD_ID));
            // choose which class to select the feature from
            const size_t max_class( m_PeerFeatures.GetSize() - 1);
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              const storage::Vector< size_t> &chosen_class( m_PeerFeatures( rng.Random( max_class)));
              const size_t random_feature_id( chosen_class( rng.Random( chosen_class.GetSize() - 1)));
              feature_with_dropout( *itr_drop) = allfeatures( random_feature_id, *itr_drop);
            }
          }
        }
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), feature_with_dropout);
      }
      else
      {
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), FEATURE);
      }

      // run hidden_input through the transfer functions to get the 1st hidden layer output
      m_TransferFunction->F( hidden( 0), hidden_input( 0));

      // remaining layers
      for( size_t k( 1), number_hidden_layers( hidden.GetSize()); k < number_hidden_layers; ++k)
      {
        // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
        hidden_input( k) = m_Bias( k);
        if( m_NumberToDrop( k))
        {
          linal::Vector< float> &hidden_dropped( hidden( k - 1));
          const storage::Vector< size_t> &drop_ids( m_ChosenDropped( k));
          for
          (
            storage::Vector< size_t>::const_iterator itr_dropped( drop_ids.Begin()), itr_dropped_end( drop_ids.End());
            itr_dropped != itr_dropped_end;
            ++itr_dropped
          )
          {
            hidden_dropped( *itr_dropped) = 0.0;
          }
        }
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( k), m_Weight( k), hidden( k - 1));

        // run hidden_input through the transfer functions to get the k+1th hidden layer output
        m_TransferFunction->F( hidden( k), hidden_input( k));

      }
    } // CalcHiddenTerms

    //! run backward through ANN and compute err terms m_Errors (train)
    //! @return true if the error should be backpropagated
    bool ApproximatorNeuralNetworkSelective::CalcErrorTerms
    (
      const FeatureReference< float> &RESULT,
      const size_t &THREAD_ID,
      const size_t &FEATURE_ID
    )
    {
      // get the thread-local errors, hidden, and hidden input arrays
      storage::Vector< linal::Vector< float> > &errors( m_Errors( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden_input( m_HiddenInput( THREAD_ID));

      // compute difference
      const linal::Vector< float> &predicted( hidden.LastElement());
      linal::Vector< float> &storage( errors.LastElement());
      storage = RESULT;
      storage -= predicted;
      const float error( storage.Norm());
      m_RMSDError( THREAD_ID) += error;

      // decide whether or not to backprop this feature
      // may also decide not to backprop some of the oclumns
      if( !m_DataSelector->ShouldBackpropagate( predicted, storage, FEATURE_ID, THREAD_ID))
      {
        return false;
      }

      // output layer

      // the following code performs this operation
      // errors.LastElement() = ( RESULT - hidden.LastElement())
      //                          * m_TransferFunction->dF( hidden_input.LastElement(), hidden.LastElement())
      // without creating any (expensive) temporary vectors
      m_TransferFunction->MultiplyBydF
      (
        errors.LastElement(),
        hidden_input.LastElement(),
        hidden.LastElement()
      );

      // all other layers
      for( size_t i( hidden.GetSize() - 1); i > 0; --i)
      {
        // errors( i - 1) = errors( i) * m_Weight( i)
        linal::GetDefaultOperations< float>().VectorEqualsVectorTimesMatrix( errors( i - 1), errors( i), m_Weight( i));
        m_TransferFunction->MultiplyBydF
        (
          errors( i - 1),
          hidden_input( i - 1),
          hidden( i - 1)
        );
      }
      return true;
    } // CalcErrorTerms

    //! compute changes m_SlopeBias/Weight to be applied on ANN (train)
    void ApproximatorNeuralNetworkSelective::CalcChangeTerms( const FeatureReference< float> &FEATURE, const size_t &THREAD_ID)
    {
      // get the thread-local arrays
      const storage::Vector< linal::Vector< float> > &errors( m_Errors( THREAD_ID));
      const storage::Vector< linal::Vector< float> > &hidden( m_Hidden( THREAD_ID));
      storage::Vector< linal::Vector< float> > &slopes_bias( m_SlopesBias( THREAD_ID));
      storage::Vector< linal::Matrix< float> > &slopes_weight( m_SlopesWeight( THREAD_ID));

      // first layer
      // m_SlopesWeight( THREAD_ID)( 0) += math::OuterProduct( errors( 0), FEATURE)
      linal::AddOuterProductToMatrix( slopes_weight( 0), errors( 0), FEATURE);

      // all other layers
      for( size_t i( 1), number_hidden_layers( hidden.GetSize()); i < number_hidden_layers; ++i)
      {
        // m_SlopesWeight( THREAD_ID)( i) += math::OuterProduct( errors( i), FEATURE)
        linal::AddOuterProductToMatrix( slopes_weight( i), errors( i), hidden( i - 1));
      }

      // add the errors to the slopes for the biases directly
      for( size_t i( 0), number_hidden_layers( hidden.GetSize()); i < number_hidden_layers; ++i)
      {
        slopes_bias( i) += errors( i);
      }
    } // CalcChangeTerms

    //! train ANN with a feature
    void ApproximatorNeuralNetworkSelective::TrainThread( const size_t &THREAD_ID)
    {
      // zero out the slopes vectors / matrices
      for
      (
        size_t layer_number( 0), number_hidden_layers( m_SlopesBias( THREAD_ID).GetSize());
        layer_number < number_hidden_layers;
        ++layer_number
      )
      {
        m_SlopesBias( THREAD_ID)( layer_number) = float( 0);
        m_SlopesWeight( THREAD_ID)( layer_number) = float( 0);
      }

      // only run the data for this range if it actually existed
      // it may not if m_DataSetRanges.GetSize() is not a multiple of the number of threads
      if( THREAD_ID + m_DataSetRangePosition < m_DataSetRanges.GetSize())
      {
        // the data position is influenced by the data set range position and the thread id
        const math::Range< size_t> &data( m_DataSetRanges( THREAD_ID + m_DataSetRangePosition));
        for
        (
          size_t feature_id( data.GetMin()), feature_end( data.GetMax());
          feature_id < feature_end;
          ++feature_id
        )
        {
          // training step
          const size_t eff_id( m_Order( feature_id));
          const FeatureReference< float> &feature( m_TrainingData->GetFeaturesPtr()->operator()( eff_id));
          CalcHiddenTerms( feature, THREAD_ID, eff_id);
          if( CalcErrorTerms( m_TrainingData->GetResultsPtr()->operator()( eff_id), THREAD_ID, eff_id))
          {
            CalcChangeTerms( feature, THREAD_ID);
            ++m_NumberFeaturesUpdated( THREAD_ID);
          }
        }
        // TODO: Test what happens if we always backpropagate
        m_DataSelector->FinalizeConformation();
        for
        (
          size_t feature_id( data.GetMin()), feature_end( data.GetMax());
          feature_id < feature_end;
          ++feature_id
        )
        {
          // training step
          const size_t eff_id( m_Order( feature_id));
          const FeatureReference< float> &feature( m_TrainingData->GetFeaturesPtr()->operator()( eff_id));
          CalcHiddenTerms( feature, THREAD_ID, eff_id);
          if( CalcErrorTerms( m_TrainingData->GetResultsPtr()->operator()( eff_id), THREAD_ID, eff_id))
          {
            CalcChangeTerms( feature, THREAD_ID);
            ++m_NumberFeaturesUpdated( THREAD_ID);
          }
        }
        m_DataSelector->FinalizeConformation();
      }
    } // TrainThread

    void ApproximatorNeuralNetworkSelective::UpdateWeights()
    {
      // handle dropout for all hidden neurons (except the output layer)
      for
      (
        size_t layer_number( 1), output_layer_number( m_NumberToDrop.GetSize());
        layer_number < output_layer_number;
        ++layer_number
      )
      {
        if( !m_NumberToDrop( layer_number))
        {
          // nothing to drop, continue
          continue;
        }
        // shuffle the first n-indices in the m_NeuronIndices array
        storage::Vector< size_t>::const_iterator
          itr_chosen( m_ChosenDropped( layer_number).Begin()), itr_chosen_end( m_ChosenDropped( layer_number).End());
        linal::Matrix< float> &weight_matrix( m_SlopesWeight( 0)( layer_number - 1));
        linal::Vector< float> &bias_vector( m_SlopesBias( 0)( layer_number - 1));
        for( ; itr_chosen != itr_chosen_end; ++itr_chosen)
        {
          weight_matrix.GetRow( *itr_chosen) = util::GetUndefined< float>();
          bias_vector( *itr_chosen) = util::GetUndefined< float>();
        }
      }
      for
      (
        size_t layer_number( 0), output_layer_number( m_NumberToDrop.GetSize());
        layer_number < output_layer_number;
        ++layer_number
      )
      {
        if( !m_NumberToDrop( layer_number))
        {
          // nothing to drop, continue
          continue;
        }
        // shuffle the first n-indices in the m_NeuronIndices array
        storage::Vector< size_t>::const_iterator
          itr_chosen( m_ChosenDropped( layer_number).Begin()), itr_chosen_end( m_ChosenDropped( layer_number).End());
        linal::Matrix< float> &weight_matrix( m_SlopesWeight( 0)( layer_number));
        for( ; itr_chosen != itr_chosen_end; ++itr_chosen)
        {
          weight_matrix.SetCol( *itr_chosen, util::GetUndefined< float>());
        }
      }

      for
      (
        size_t layer_number( 0), output_layer_number( m_Weight.GetSize());
        layer_number < output_layer_number;
        layer_number++
      )
      {
        // update the bias using the desired propagation method
        ( *m_WeightUpdaters( layer_number))
        (
          m_Weight( layer_number).Begin(),
          m_SlopesWeight( 0)( layer_number).Begin()
        );

        // update the bias using the desired propagation method
        ( *m_BiasUpdaters( layer_number))
        (
          m_Bias( layer_number).Begin(),
          m_SlopesBias( 0)( layer_number).Begin()
        );
      }
      UpdateDroppedNeurons();
    } // UpdateWeights

    void ApproximatorNeuralNetworkSelective::UpdateDroppedNeurons()
    {
      for
      (
        size_t layer_number( 0), output_layer_number( m_Weight.GetSize());
        layer_number < output_layer_number;
        layer_number++
      )
      {
        if( !m_NumberToDrop( layer_number))
        {
          // nothing to drop, continue
          continue;
        }
        storage::Vector< size_t> &chosen_array( m_ChosenDropped( layer_number));
        const size_t nr_blocks( layer_number ? m_DropoutPartitions( layer_number - 1) + 1 : 1);
        const size_t n_to_drop_per_block( m_NumberToDrop( layer_number));
        for( size_t block_number( 0), neuron( 0); block_number < nr_blocks; ++block_number)
        {
          // shuffle the first n-indices in the m_NeuronIndices array
          storage::Vector< size_t> &indices_array( m_NeuronIndices( layer_number)( block_number));
          const size_t layer_size( indices_array.GetSize());
          for( size_t block_neuron( 0); block_neuron < n_to_drop_per_block; ++block_neuron, ++neuron)
          {
            std::swap( indices_array( block_neuron), indices_array( random::GetGlobalRandom().Random( block_neuron, layer_size - 1)));
            chosen_array( neuron) = indices_array( block_neuron);
          }
        }
        if( m_UpdateEveryNthFeature == 0 || m_UpdateEveryNthFeature > chosen_array.GetSize())
        {
          chosen_array.Sort( std::less< size_t>());
        }
      }
    } // UpdateWeights

    //! @brief sets up the neural network with a particular architecture, after training data was set
    void ApproximatorNeuralNetworkSelective::SetupArchitecture()
    {
      BCL_Assert
      (
        m_TrainingData.IsDefined(),
        "SetupArchitecture requires valid training data!"
      );

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      // add the output neurons
      architecture.PushBack( m_TrainingData->GetResultSize());

      m_Bias.Reset();
      m_Weight.Reset();
      m_Bias.AllocateMemory( architecture.GetSize() - 1);
      m_Weight.AllocateMemory( architecture.GetSize() - 1);

      if( m_TrainingData->GetSize() == 0)
      {
        // no training data, can't set the architecture up yet
        return;
      }

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      // initialize data for the given architecture
      for( size_t i( 1); i < architecture.GetSize(); ++i)
      {
        m_Bias.PushBack( linal::Vector< float>( architecture( i), 0.0));
        m_Weight.PushBack( linal::Matrix< float>( architecture( i), architecture( i - 1), 0.0));
      }

      // randomize the initial values of bias and weight
      storage::Vector< size_t>::const_iterator itr_arch( architecture.Begin());
      for
      (
        storage::Vector< linal::Vector< float> >::iterator itr( m_Bias.Begin()), itr_end( m_Bias.End());
        itr != itr_end;
        ++itr, ++itr_arch
      )
      {
        for( float *itr_bias( itr->Begin()), *itr_bias_end( itr->End()); itr_bias < itr_bias_end; ++itr_bias)
        {
          *itr_bias = random::GetGlobalRandom().RandomGaussian( 0.0, 0.1);
        }
      }

      itr_arch = architecture.Begin();
      for
      (
        storage::Vector< linal::Matrix< float> >::iterator itr( m_Weight.Begin()), itr_end( m_Weight.End());
        itr != itr_end;
        ++itr, ++itr_arch
      )
      {
        float std( 1.0 / math::Sqrt( float( *itr_arch)));
        BCL_MessageStd( "std: " + util::Format()( std));
        if( m_ConnectionDensity >= float( 1.0) || itr == m_Weight.Last())
        {
          for( float *itr_weight( itr->Begin()), *itr_weight_end( itr->End()); itr_weight < itr_weight_end; ++itr_weight)
          {
            *itr_weight = random::GetGlobalRandom().RandomGaussian( 0.0, std);
          }
        }
        else
        {
          for( float *itr_weight( itr->Begin()), *itr_weight_end( itr->End()); itr_weight < itr_weight_end; ++itr_weight)
          {
            if( random::GetGlobalRandom().Double() < m_ConnectionDensity)
            {
              *itr_weight = random::GetGlobalRandom().RandomGaussian( 0.0, std);
            }
          }
        }
      }
    }

    //! @brief sets up the weight updaters and change weights
    //!        this has to be done unless the iterate is read from a file
    void ApproximatorNeuralNetworkSelective::InitializeWeightUpdaters()
    {
      // create the architecture, excluding input neurons (for which there are no weight updaters)
      storage::Vector< size_t> architecture( m_HiddenArchitecture);

      // add the output neurons
      architecture.PushBack( m_TrainingData->GetResultSize());

      m_BiasUpdaters.Reset();
      m_WeightUpdaters.Reset();
      m_IterationWeightUpdaters.Reset();
      m_BiasUpdaters.AllocateMemory( architecture.GetSize() - 1);
      m_WeightUpdaters.AllocateMemory( architecture.GetSize() - 1);
      m_IterationWeightUpdaters.AllocateMemory( architecture.GetSize() - 1);

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      if( !m_BiasUpdateType.IsDefined())
      {
        m_BiasUpdateType = m_WeightUpdateType;
      }

      // initialize data for the given architecture
      for( size_t i( 0); i < architecture.GetSize(); ++i)
      {
        // initialize weight updaters for the given architecture
        m_BiasUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
        m_WeightUpdaters.PushBack( util::CloneToShPtr( *m_WeightUpdateType));
        m_BiasUpdaters.LastElement()->Initialize( m_Bias( i).GetSize());
        m_WeightUpdaters.LastElement()->Initialize( m_Weight( i).GetNumberOfElements());
        if( m_IterationWeightUpdateType.IsDefined())
        {
          m_IterationWeightUpdaters.PushBack( util::CloneToShPtr( *m_IterationWeightUpdateType));
        }
      }
    }

    //! @brief sets the data set ranges up for the # of threads
    void ApproximatorNeuralNetworkSelective::SetupDataSetRanges()
    {
      BCL_Assert
      (
        m_TrainingData.IsDefined(),
        "SetupDataSetRanges requires valid training data!"
      );

      size_t features_per_weight_update( m_UpdateEveryNthFeature);
      // 0 indicates batch mode, so if the m_UpdateEveryNthFeature was 0, use the data set size
      if( m_UpdateEveryNthFeature == size_t( 0))
      {
        features_per_weight_update = m_Order.GetSize();
      }

      // redetermine the # of threads, since this influences how the data is split up
      m_NumberThreads = std::min( features_per_weight_update, sched::GetNumberCPUs());

      BCL_MessageStd( "Set up data set ranges with # threads: " + util::Format()( m_NumberThreads));

      // set up the training ranges for each thread
      m_DataSetRanges.Reset();

      // calculate the # of weigh updates per run through the data set
      const size_t number_epochs_per_data_set
      (
        ( m_Order.GetSize() - 1) / features_per_weight_update + 1
      );
      features_per_weight_update = m_Order.GetSize() / number_epochs_per_data_set;
      m_DataSetRanges.AllocateMemory( number_epochs_per_data_set * m_NumberThreads);

      // if m_TrainingData->GetSize() is not a multiple of features_per_weight_update, distribute the extra features
      // over the initial number_epochs_with_extra_feature epochs
      const size_t number_epochs_with_extra_feature( m_Order.GetSize() % features_per_weight_update);

      size_t current_index( 0);

      for( size_t epoch_number( 0); epoch_number < number_epochs_per_data_set; ++epoch_number)
      {
        // determine the # of features that will be examined in this epoch
        const size_t features_in_epoch( features_per_weight_update + size_t( epoch_number < number_epochs_with_extra_feature));

        // # of features per thread per epoch
        const size_t min_features_per_thread( features_in_epoch / m_NumberThreads);

        // if features_per_weight_update is not evenly divisible by m_NumberThreads, then the extra features are distributed
        // over the initial number_threads_with_extra_feature threads
        const size_t number_threads_with_extra_feature( features_in_epoch % m_NumberThreads);

        // make data set ranges for each thread for this epoch
        for( size_t current( 0); current < m_NumberThreads; ++current)
        {
          const size_t current_width( min_features_per_thread + size_t( current < number_threads_with_extra_feature));

          m_DataSetRanges.PushBack
          (
            math::Range< size_t>
            (
              current_index,
              current_index + current_width
            )
          );
          current_index += current_width;
        }
      }

      m_RMSDError = linal::Vector< float>( m_NumberThreads, 0.0);

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      // add the output neurons
      const size_t result_size( m_TrainingData->GetResultSize());
      architecture.PushBack( result_size);

      // set up separate temporary vectors and matrices for each thread
      storage::Vector< linal::Vector< float> > temp_vector_prototype( architecture.GetSize() - 1);
      storage::Vector< linal::Matrix< float> > temp_matrix_prototype( architecture.GetSize() - 1);

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( architecture.Begin(), architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      // initialize data for the given architecture
      for( size_t i( 1); i < architecture.GetSize(); ++i)
      {
        temp_vector_prototype( i - 1) = linal::Vector< float>( architecture( i), 0.0);
        temp_matrix_prototype( i - 1) = linal::Matrix< float>( architecture( i), architecture( i - 1), 0.0);
      }

      // set the temporary vectors equal to each other
      if( ( !m_Dropout.IsEmpty() && size_t( m_Dropout( 0) * architecture( 0))) || m_NoiseZScore)
      {
        m_FeatureWithDropout
          = storage::Vector< linal::Vector< float> >( m_NumberThreads, linal::Vector< float>( architecture( 0), 0.0));
      }
      m_SlopesBias = m_Errors = m_HiddenInput = m_Hidden =
        storage::Vector< storage::Vector< linal::Vector< float> > >( m_NumberThreads, temp_vector_prototype);
      m_SlopesWeight =
        storage::Vector< storage::Vector< linal::Matrix< float> > >( m_NumberThreads, temp_matrix_prototype);
      m_NumberFeaturesUpdated = linal::Vector< size_t>( m_NumberThreads, size_t( 0));

      BCL_MessageStd( "Set up data set ranges with # ranges: " + util::Format()( m_DataSetRanges.GetSize()));
      BCL_MessageStd( "Set up data set ranges for updating every nth feature: " + util::Format()( features_per_weight_update));
      m_ThreadRandomNumberGenerators
        = storage::Vector< random::UniformDistribution>
          (
            m_NumberThreads,
            random::UniformDistribution( random::GetGlobalRandom().GetSeed())
          );
    } // SetupDataSetRanges

    //! @brief balance features based on the objective function
    //! @param MAX_REPEATS the maximum # of repeats allowed for any given feature; used for multi-column outputs, when
    //!        otherwise an output column with extremely few positives would see severe over-representation of certain
    //!        features
    //! @param MAX_BALANCE_RATIO 0-1; indicates the maximum ratio that the under-represented features will be balanced
    //! If underepresented features are already balanced to at least MAX_BALANCE_RATIO before calling this function,
    //! no features will be removed.  If the value of MAX_BALANCE_RATIO is greater than MAX_REPEATS would allow for the
    //! column; then each feature will just be replicated MAX_REPEATS times
    void ApproximatorNeuralNetworkSelective::BalanceFeatures
    (
      const size_t &MAX_REPEATS,
      const float &MAX_BALANCE_RATIO
    )
    {
      m_Order.Resize( m_TrainingData->GetSize());
      for( size_t i( 0), n_features( m_TrainingData->GetSize()); i < n_features; ++i)
      {
        m_Order( i) = i;
      }
      // get scaling and cutoff information
      const float cutoff( m_ObjectiveFunction->GetThreshold());
      if( !util::IsDefined( cutoff))
      {
        BCL_MessageCrt( "Cannot balance a dataset when a non-classification type objective is used!");
        return;
      }

      // determine the class of each training example result
      DetermineResultClasses();

      // create reflecting iterators for each class and determine the size of the most populated class
      const size_t n_classes( m_PeerFeatures.GetSize());
      BCL_Assert( n_classes > 1, "Only a single class of features was found; balancing is impossible!");
      size_t max_class_size( m_PeerFeatures.FirstElement().GetSize());
      storage::Vector< size_t> class_iterators( n_classes, size_t( 0));
      for( size_t class_id( 0); class_id < n_classes; ++class_id)
      {
        max_class_size = std::max( max_class_size, m_PeerFeatures( class_id).GetSize());
      }
      size_t second_most_common_class_size( 0);
      for( size_t class_id( 0); class_id < n_classes; ++class_id)
      {
        if( m_PeerFeatures( class_id).GetSize() != max_class_size)
        {
          second_most_common_class_size = std::max( second_most_common_class_size, m_PeerFeatures( class_id).GetSize());
        }
      }

      size_t max_target_size( std::min( size_t( max_class_size * m_BalanceMaxOversampling + 1), max_class_size));
      storage::Vector< size_t> max_this_class( n_classes, max_class_size);
      float oversampling_factor
      (
        std::min( float( m_BalanceMaxRepeatedFeatures), float( max_target_size) / float( second_most_common_class_size))
      );
      for( size_t class_id( 0); class_id < n_classes; ++class_id)
      {
        if( m_PeerFeatures( class_id).GetSize() < max_class_size)
        {
          max_this_class( class_id) =
            std::min( m_PeerFeatures( class_id).GetSize() * oversampling_factor, float( max_target_size));
        }
      }
      m_Order.Reset();
      m_Order.AllocateMemory( n_classes * max_class_size);
      for( size_t class_feature( 0); class_feature < max_class_size; ++class_feature)
      {
        for( size_t class_id( 0); class_id < n_classes; ++class_id)
        {
          if( class_feature < max_this_class( class_id))
          {
            const storage::Vector< size_t> &peers( m_PeerFeatures( class_id));
            m_Order.PushBack( peers( class_iterators( class_id) % peers.GetSize()));
            ++class_iterators( class_id);
          }
        }
      }
      m_Order.Shuffle();
    }

    //! @brief helper function to initialize dropout-related vectors
    void ApproximatorNeuralNetworkSelective::InitializeDropout()
    {
      // every hidden layer and the input layer can incorporate dropout
      const size_t number_potential_dropout_layers( m_HiddenArchitecture.GetSize() + 1);
      // handle dropout
      if( m_Dropout.IsEmpty())
      {
        m_Dropout = linal::Vector< float>( number_potential_dropout_layers, float( 0));
      }
      else if( m_Dropout.GetSize() > number_potential_dropout_layers)
      {
        BCL_Exit( "Too many dropout layers specified!", -1);
      }
      else if( m_Dropout.GetSize() < number_potential_dropout_layers)
      {
        BCL_MessageCrt( "Assuming layers > " + util::Format()( m_Dropout.GetSize()) + " have no dropout!");
        linal::Vector< float> tmp( number_potential_dropout_layers, float( 0));
        std::copy( m_Dropout.Begin(), m_Dropout.End(), tmp.Begin());
        m_Dropout = tmp;
      }

      // the only thing special about dropout-immune neurons is that they cannot occur in the visible layer, since
      // that really does not make any sense
      if( m_DropoutPartitions.IsEmpty())
      {
        m_DropoutPartitions = linal::Vector< size_t>( m_HiddenArchitecture.GetSize(), size_t( 0));
      }
      else if( m_DropoutPartitions.GetSize() > m_HiddenArchitecture.GetSize())
      {
        BCL_Exit( "Too many dropout-immune layers specified!", -1);
      }
      else if( m_DropoutPartitions.GetSize() < m_HiddenArchitecture.GetSize())
      {
        linal::Vector< size_t> tmp( m_HiddenArchitecture.GetSize() - 1, size_t( 0));
        std::copy( m_DropoutPartitions.Begin(), m_DropoutPartitions.End(), tmp.Begin());
        m_DropoutPartitions = tmp;
      }

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      m_NumberToDrop = linal::Vector< size_t>( number_potential_dropout_layers, size_t( 0));
      m_NumberToDrop( 0) = architecture( 0) * m_Dropout( 0);
      for( size_t layer( 1), n_layers( architecture.GetSize()); layer < n_layers; ++layer)
      {
        const size_t n_dropout_blocks_this_layer( m_DropoutPartitions( layer - 1) + 1);
        m_NumberToDrop( layer) = architecture( layer) * m_Dropout( layer) / float( n_dropout_blocks_this_layer);
        if( m_Dropout( layer) > 0.0 && m_NumberToDrop( layer) == 0)
        {
          BCL_MessageCrt
          (
            "Warning; a dropout value was specified it is too small for at least one neuron to be dropped from every "
            "partition of layer " + util::Format()( layer)
          );
        }
      }
      m_NeuronIndices = storage::Vector< storage::Vector< storage::Vector< size_t> > >( number_potential_dropout_layers);
      m_ChosenDropped = storage::Vector< storage::Vector< size_t> >( number_potential_dropout_layers);
      for( size_t layer( 0), n_layers( architecture.GetSize()); layer < n_layers; ++layer)
      {
        const size_t dropout_prone( architecture( layer));
        const size_t n_dropout_blocks( layer ? m_DropoutPartitions( layer - 1) + 1 : 1);
        m_NeuronIndices( layer).Resize( n_dropout_blocks, storage::Vector< size_t>( dropout_prone));

        m_ChosenDropped( layer).Resize( m_NumberToDrop( layer) * n_dropout_blocks);
        const size_t base_neurons_per_block( dropout_prone / n_dropout_blocks);
        const size_t nr_blocks_extra_neuron( dropout_prone % n_dropout_blocks);
        for( size_t block_nr( 0), neuron( 0); block_nr < n_dropout_blocks; ++block_nr)
        {
          const size_t nr_neurons_this_block( base_neurons_per_block + ( block_nr < nr_blocks_extra_neuron));
          for( size_t block_neuron( 0); block_neuron < nr_neurons_this_block; ++block_neuron, ++neuron)
          {
            m_NeuronIndices( layer)( block_nr)( block_neuron) = neuron;
          }
        }
      }
    }

    //! @brief determine result classes; only called if input dropout type requires peers
    void ApproximatorNeuralNetworkSelective::DetermineResultClasses()
    {
      if( !m_ResultClass.IsEmpty())
      {
        return;
      }
      const float cutoff( m_ObjectiveFunction->GetThreshold());
      const linal::MatrixConstReference< float> results( m_TrainingData->GetResultsReference());

      const size_t results_size( m_TrainingData->GetResultSize());
      // Rescaled cutoffs, one for each result
      linal::Vector< float> scaled_cutoffs( results_size, cutoff);

      // compute the rescaled cutoffs
      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( m_TrainingData->GetResultsPtr()->GetScaling());
      for( size_t result( 0); result < results_size; ++result)
      {
        scaled_cutoffs( result) = results_scaling->RescaleValue( result, cutoff);
      }

      const size_t bitsize( sizeof( size_t) * 8);
      const size_t n_sizets( ( results_size - 1) / bitsize + 1);
      storage::Vector< size_t> result_class( n_sizets, size_t( 0));
      m_ResultClass = storage::Vector< size_t>( m_TrainingData->GetSize(), size_t( 0));
      // for each result, compute the result class
      size_t n_classes_so_far( 0);
      storage::Map< storage::Vector< size_t>, size_t> class_to_index;
      storage::Vector< storage::Vector< size_t> > classes;
      for( size_t feature( 0), n_features( m_TrainingData->GetSize()); feature < n_features; ++feature)
      {
        result_class.SetAllElements( 0);
        linal::VectorConstReference< float> result_row( results.GetRow( feature));
        for( size_t result_index( 0); result_index < results_size; ++result_index)
        {
          if( result_row( result_index) >= scaled_cutoffs( result_index))
          {
            result_class( result_index / bitsize) |= ( 1 << ( result_index % bitsize));
          }
        }
        storage::Map< storage::Vector< size_t>, size_t>::const_iterator itr_map( class_to_index.Find( result_class));
        if( itr_map == class_to_index.End())
        {
          class_to_index[ result_class] = m_ResultClass( feature) = n_classes_so_far++;
          classes.PushBack( result_class);
        }
        else
        {
          m_ResultClass( feature) = itr_map->second;
        }
      }
      // accumulate all classes
      m_PeerFeatures = storage::Vector< storage::Vector< size_t> >( n_classes_so_far);
      for( size_t feature( 0), n_features( m_TrainingData->GetSize()); feature < n_features; ++feature)
      {
        m_PeerFeatures( m_ResultClass( feature)).PushBack( feature);
      }
      std::stringstream info_stream;
      info_stream << "Found " << n_classes_so_far << " classes of training points. Counts per class: ";
      for( size_t class_id( 0); class_id < n_classes_so_far; ++class_id)
      {
        info_stream << m_PeerFeatures( class_id).GetSize() << ' ';
      }
      info_stream << ".  Class Binary IDs: ";
      for( size_t class_id( 0); class_id < n_classes_so_far; ++class_id)
      {
        for( size_t result_index( 0); result_index < results_size; ++result_index)
        {
          info_stream << int( bool( classes( class_id)( result_index / bitsize) & ( 1 << ( result_index % bitsize))));
        }
        info_stream << ' ';
      }
      BCL_MessageStd( info_stream.str());
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_restricted_boltzmann_machine.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_statistics.h"
#include "model/bcl_model_transfer_sigmoid.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorRestrictedBoltzmannMachine::s_IterateInstance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorRestrictedBoltzmannMachine( false))
    );
    const util::SiPtr< const util::ObjectInterface> ApproximatorRestrictedBoltzmannMachine::s_PretrainInstance
    (
      util::Enumerated< PretrainNeuralNetworkInterface>::AddInstance( new ApproximatorRestrictedBoltzmannMachine( true))
    );

    //! @brief default constructor
    //! @param PRETRAIN whether this object is being constructed as a pretrainer or an iterate
    ApproximatorRestrictedBoltzmannMachine::ApproximatorRestrictedBoltzmannMachine( const bool &PRETRAIN) :
      m_UpdateEveryNthFeature( 0),
      m_NumberThreads(),
      m_IterationsPerRMSDMessage( 1),
      m_Type( RestrictedBoltzmannMachineLayer::e_StochasticSigmoid),
      m_Shuffle( false),
      m_NumberStochasticSteps( 3),
      m_DataSetRangePosition( 0),
      m_IsPretrainer( PRETRAIN),
      m_NumIterations( 0)
    {
    }

    //! @brief constructor from training data, transfer, rescale, and objective functions
    //! @param TRAINING_DATA data to train the NeuralNetwork on
    //! @param UPDATE_EVERY_NTH_FEATURE how often the weights get updated
    //! @param ARCHITECTURE the # of neurons in each hidden layer of the network
    //! @param TRANSFER_FUNCTION ShPtr to the transfer function between input and output of each neuron
    //! @param OBJECTIVE_FUNCTION ShPtr to objective function
    //! @param WEIGHT_UPDATE_FUNCTION method by which to update the weights
    //! @param BIAS_UPDATE_FUNCTION method by which to update the biases
    //! @param ITERATIONS_PER_RMSD_REPORT # iterations per report of the rmsd
    ApproximatorRestrictedBoltzmannMachine::ApproximatorRestrictedBoltzmannMachine
    (
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t UPDATE_EVERY_NTH_FEATURE,
      const storage::Vector< size_t> &ARCHITECTURE,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE_FUNCTION,
      const util::Implementation< NeuralNetworkUpdateWeightsInterface> &WEIGHT_UPDATE_FUNCTION,
      const util::Implementation< NeuralNetworkUpdateWeightsInterface> &BIAS_UPDATE_FUNCTION,
      const size_t &ITERATIONS_PER_RMSD_REPORT,
      const RestrictedBoltzmannMachineLayer::Type &TYPE
    ) :
      ApproximatorBase( OBJECTIVE_FUNCTION),
      m_UpdateEveryNthFeature( UPDATE_EVERY_NTH_FEATURE),
      m_IterationsPerRMSDMessage( ITERATIONS_PER_RMSD_REPORT),
      m_Type( TYPE),
      m_Shuffle( UPDATE_EVERY_NTH_FEATURE), // Always shuffle except in batch-update mode
      m_NumberStochasticSteps( 3),
      m_DataSetRangePosition( 0),
      m_HiddenArchitecture( ARCHITECTURE),
      m_WeightUpdateType( WEIGHT_UPDATE_FUNCTION),
      m_BiasUpdateType( BIAS_UPDATE_FUNCTION),
      m_WeightDecay( 0.0001),
      m_IsPretrainer( false),
      m_NumIterations( 0)
    {
      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new ApproximatorRestrictedBoltzmannMachine copied from this instance
    ApproximatorRestrictedBoltzmannMachine *ApproximatorRestrictedBoltzmannMachine::Clone() const
    {
      return new ApproximatorRestrictedBoltzmannMachine( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorRestrictedBoltzmannMachine::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorRestrictedBoltzmannMachine::GetAlias() const
    {
      static const std::string s_Name( "RestrictedBoltzmannMachine");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorRestrictedBoltzmannMachine::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      m_TrainingData = DATA;
      DATA->GetFeatures().DeScale();
      DATA->GetFeatures().Rescale( RestrictedBoltzmannMachineLayer::s_DefaultInputRange, RescaleFeatureDataSet::e_MinMax);
      DATA->GetResults().DeScale();
      DATA->GetResults().Rescale( TransferSigmoid().GetDynamicOutputRange());

      BCL_MessageStd
      (
        "Setting up training data with " + util::Format()( DATA->GetSize()) + " points"
      );

      m_Order.Resize( DATA->GetSize());
      for( size_t i( 0), n_features( DATA->GetSize()); i < n_features; ++i)
      {
        m_Order( i) = i;
      }

      // set the data ranges up for threading (if applicable)
      SetupDataSetRanges();

      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Iterate properly
      if( !IsTrainingContinued())
      {
        // weight updaters need to be initialized regardless
        InitializeWeightUpdaters();
      }
    } // SetTrainingData

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorRestrictedBoltzmannMachine::GetCurrentModel() const
    {
      // make a new model out of the current data members
      const size_t number_layers( m_Layers.GetSize()), number_vis_layers_no_output( number_layers - 1);
      storage::Vector< linal::Vector< float> > hidden_biases( number_layers + 1);
      storage::Vector< linal::Matrix< float> > weights( number_layers + 1);
      for( size_t layer( 0); layer < number_vis_layers_no_output; ++layer)
      {
        hidden_biases( layer) = m_Layers( layer).GetHiddenBias();
        weights( layer) = m_Layers( layer).GetWeight().Transposed();
      }
      hidden_biases( number_vis_layers_no_output) = m_Layers( number_vis_layers_no_output).GetHiddenBias();

      // get the number of real hidden neurons in the next-to-last layer
      const size_t number_real_visible_neurons
      (
        m_HiddenArchitecture.GetSize() > size_t( 1)
        ? m_HiddenArchitecture( m_HiddenArchitecture.GetSize() - 2)
        : m_TrainingData->GetFeatureSize()
      );

      // number of final hidden neurons
      const size_t number_final_hidden_neurons( m_HiddenArchitecture.LastElement());

      // determine the number of output neurons
      const size_t number_output_neurons( m_TrainingData->GetResultSize());

      // get the biases from the final visible layer for the output neurons
      hidden_biases( number_layers)
        = linal::VectorConstReference< float>
          (
            number_output_neurons,
            m_Layers( number_vis_layers_no_output).GetVisibleBias().Begin() + number_real_visible_neurons
          );

      // create a reference to the weights that are hidden vs.
      weights( number_vis_layers_no_output)
        = linal::MatrixConstReference< float>
          (
            number_real_visible_neurons,
            number_final_hidden_neurons,
            m_Layers( number_vis_layers_no_output).GetWeight().Begin()
          ).Transposed();

      weights( number_layers)
        = linal::MatrixConstReference< float>
          (
            number_output_neurons,
            number_final_hidden_neurons,
            m_Layers( number_vis_layers_no_output).GetWeight()[ number_real_visible_neurons]
          );

      util::ShPtr< Interface> model
      (
        new NeuralNetwork
        (
          GetRescaleFeatureDataSet(),
          GetRescaleResultDataSet(),
          hidden_biases,
          weights,
          util::Implementation< TransferFunctionInterface>( TransferSigmoid())
        )
      );
      return model;
    }

    //! @brief the main operation, pretrains a neural network
    //! @param DATA the data for use in pretraining
    //! @param OBJECTIVE ShPtr to the objective function for the network
    util::ShPtr< NeuralNetwork> ApproximatorRestrictedBoltzmannMachine::PretrainNetwork
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE
    )
    {
      this->SetObjectiveFunction( OBJECTIVE);
      this->SetTrainingData( DATA);
      while( this->GetTracker().GetIteration() < m_NumIterations)
      {
        this->Next();
      }
      return GetCurrentModel();
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorRestrictedBoltzmannMachine::GetCurrentApproximation() const
    {
      util::ShPtr< Interface> model( GetCurrentModel());
      return
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>
          (
            model,
            m_ObjectiveFunction->operator ()( model)
          )
        );
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorRestrictedBoltzmannMachine::Next()
    {
      // handle shuffling
      if( m_Shuffle)
      {
        m_Order.Shuffle();
      }

      util::ShPtrVector< sched::JobInterface> all_jobs( m_NumberThreads);

      const size_t group_id( 0);

      // store the id of each thread for use by the jobs and reset all results tallying objects.
      storage::Vector< size_t> thread_ids( m_NumberThreads);
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
      }

      // run through the data set once, updating weights after every epoch (m_UpdateEveryNthFeature features, data set size by default)
      // launch jobs to train the NN with the features for this epoch
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        all_jobs( thread_number)
          = util::ShPtr< sched::JobInterface>
            (
              new sched::UnaryFunctionJobWithData
              <
                const size_t,
                void,
                ApproximatorRestrictedBoltzmannMachine
              >
              (
                group_id,
                *this,
                &ApproximatorRestrictedBoltzmannMachine::TrainThread,
                thread_ids( thread_number),
                sched::JobInterface::e_READY,
                NULL
              )
            );
      }

      m_DataSetRangePosition = 0;
      while( m_DataSetRangePosition < m_DataSetRanges.GetSize())
      {
        for( size_t thread_number( 0); thread_number < m_NumberThreads; thread_number++)
        {
          sched::GetScheduler().RunJob( all_jobs( thread_number));
        }

        sched::GetScheduler().Join( all_jobs( 0));
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          sched::GetScheduler().Join( all_jobs( thread_number));
          // accumulate the slopes from the newly joined jobs with the slopes from the earlier jobs
          for
          (
            size_t hidden_layer_number( 0), number_hidden_layers( m_Trainers( thread_number).GetSize());
            hidden_layer_number < number_hidden_layers;
            ++hidden_layer_number
          )
          {
            m_Trainers( 0)( hidden_layer_number).AccumulateChangesFrom( m_Trainers( thread_number)( hidden_layer_number));
          }
        }

        UpdateWeights();

        // move on the the next epoch
        m_DataSetRangePosition += m_NumberThreads;
      }

      if( ( this->GetTracker().GetIteration() % m_IterationsPerRMSDMessage) == size_t( 0))
      {
        for( size_t thread_number( 1); thread_number < m_NumberThreads; thread_number++)
        {
          // accumulate errors
          m_RMSDError( 0) += m_RMSDError( thread_number);
          m_ReconstructionError( 0) += m_ReconstructionError( thread_number);
        }
        m_RMSDError( 0) /= m_TrainingData->GetSize();
        m_RMSDError( 0) = std::max( m_RMSDError( 0), float( 0.0));
        m_ReconstructionError( 0) /= m_TrainingData->GetSize();
        BCL_MessageStd
        (
          "Training relative RMSD for iteration " + util::Format()( this->GetTracker().GetIteration())
          + ": " + util::Format()( m_RMSDError( 0))
          + " reconstruction error in 1st layer: " + util::Format()( m_ReconstructionError( 0))
        );
      }

      m_DataSetRangePosition = 0;

      if( m_IsPretrainer)
      {
        this->GetTracker().Track
        (
          util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
          (
            new storage::Pair< util::ShPtr< Interface>, float>
            (
              util::ShPtr< Interface>(),
              float( 0.0)
            )
          )
        );
        // act as a pretrainer for the specified number of steps, no need to create a model
        return;
      }

      // get current model
      util::ShPtr< Interface> current_model( GetCurrentModel());

      const float objective_result( m_ObjectiveFunction->operator()( current_model));

      // combine it with the objective function evaluation
      this->GetTracker().Track
      (
        util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( current_model, objective_result)
        )
      );
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorRestrictedBoltzmannMachine::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "trains a neural network (see http://en.wikipedia.org/wiki/Artificial_neural_network)"
      );

      parameters.AddInitializer
      (
        "type",
        "Type of RBM to train; StochasticSigmoid is a noisy sigmoid approx for continous data, see "
        "http://www.ee.nthu.edu.tw/~hchen/pubs/iee2003.pdf . Binomial is from http://www.cs.toronto.edu/~hinton/science.pdf",
        io::Serialization::GetAgent( &m_Type),
        "StochasticSigmoid"
      );
      parameters.AddInitializer
      (
        "weight update",
        "algorithm used to update the weights",
        io::Serialization::GetAgent( &m_WeightUpdateType),
        "Resilient"
      );
      parameters.AddOptionalInitializer
      (
        "bias update",
        "algorithm used to update the biases; if omitted, the weight update type is used",
        io::Serialization::GetAgent( &m_BiasUpdateType)
      );
      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        "RMSD"
      );
      parameters.AddInitializer
      (
        "steps per update",
        "# of features seen between each update of weights (set to 0 to use the size of the training data set)",
        io::Serialization::GetAgent( &m_UpdateEveryNthFeature),
        "0"
      );
      parameters.AddInitializer
      (
        "hidden architecture",
        "# of neurons in each hidden layer, e.g. (100) declares that there is 1 hidden layer w/ 100 neurons",
        io::Serialization::GetAgentContainerWithCheck
        (
          &m_HiddenArchitecture,
          io::Serialization::GetAgentWithMin( size_t( 1))
        )
      );
      parameters.AddInitializer
      (
        "rmsd report frequency",
        "# of iterations between reports of rmsd on the last training batch",
        io::Serialization::GetAgentWithMin( &m_IterationsPerRMSDMessage, size_t( 1)),
        "1"
      );
      parameters.AddInitializer
      (
        "shuffle",
        "primarily for non-batch update; if true, shuffle the order or data points between each run through the data",
        io::Serialization::GetAgent( &m_Shuffle),
        "False"
      );
      parameters.AddInitializer
      (
        "stochastic steps",
        "0 gives fast convergence but poor generalizability, 3 gives the most generality but can result in slow "
        "convergence, especially for binomial networks.",
        io::Serialization::GetAgentWithRange( &m_NumberStochasticSteps, 0, 3),
        "3"
      );
      parameters.AddInitializer
      (
        "weight decay",
        "Weight decay parameter; large values (say 0.001) may result in better networks but slower training times"
        ". This number should be scaled with # steps per update (batch size); so if multiplying batch size by X, multiply "
        "this number by X too for consistency",
        io::Serialization::GetAgentWithRange( &m_WeightDecay, 0.0, 0.1),
        "0.00001"
      );
      if( m_IsPretrainer)
      {
        // pretraining-specific parameters
        parameters.AddInitializer
        (
          "iterations",
          "Number of complete iterations through the data to pretrain the network",
          io::Serialization::GetAgent( &m_NumIterations)
        );
      }
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ApproximatorRestrictedBoltzmannMachine::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_IsPretrainer, ISTREAM);
      util::ObjectDataLabel label;
      io::Serialize::Read( label, ISTREAM);
      BCL_Assert( ApproximatorBase::TryRead( label, util::GetLogger()), "Could not read iterate");

      // read members
      io::Serialize::Read( m_Layers,               ISTREAM);
      io::Serialize::Read( m_Trainers,             ISTREAM);
      io::Serialize::Read( m_HiddenArchitecture,   ISTREAM);
      io::Serialize::Read( m_HiddenBiasUpdaters,   ISTREAM);
      io::Serialize::Read( m_VisibleBiasUpdaters,  ISTREAM);
      io::Serialize::Read( m_HiddenNoiseUpdaters,  ISTREAM);
      io::Serialize::Read( m_VisibleNoiseUpdaters, ISTREAM);
      io::Serialize::Read( m_WeightUpdaters,       ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ApproximatorRestrictedBoltzmannMachine::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members.  Note that this is probably insufficient;
      io::Serialize::Write( m_IsPretrainer, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( ApproximatorBase::GetLabel(), OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Layers, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Trainers, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_HiddenArchitecture, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_HiddenBiasUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_VisibleBiasUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_HiddenNoiseUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_VisibleNoiseUpdaters, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightUpdaters, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

    //! train ANN with a feature
    void ApproximatorRestrictedBoltzmannMachine::TrainThread( const size_t &THREAD_ID)
    {
      // get a reference to the array of trainers
      storage::Vector< TrainRestrictedBoltzmannMachineLayer> &trainers( m_Trainers( THREAD_ID));

      // determine the # of layers
      const size_t number_layers( m_Layers.GetSize());

      // reset the trainers to prepare for the next round
      for( size_t layer_number( 0); layer_number < number_layers; ++layer_number)
      {
        trainers( layer_number).Reset();
      }

      // only run the data for this range if it actually existed
      // it may not if m_DataSetRanges.GetSize() is not a multiple of the number of threads
      if( THREAD_ID + m_DataSetRangePosition >= m_DataSetRanges.GetSize())
      {
        return;
      }

      // determine the # of layers prior to the output layer
      const size_t number_layers_before_output( number_layers - 1);

      // get the feature matrix
      const linal::MatrixConstReference< float> features( m_TrainingData->GetFeaturesPtr()->GetMatrix());
      const linal::MatrixConstReference< float> results( m_TrainingData->GetResultsPtr()->GetMatrix());

      // create temporary vectors
      linal::Vector< float> first_hidden_tmp( m_HiddenArchitecture( 0));
      linal::Vector< float> first_visible_tmp( m_TrainingData->GetFeatureSize());

      // the data position is influenced by the data set range position and the thread id
      const math::Range< size_t> &data( m_DataSetRanges( THREAD_ID + m_DataSetRangePosition));
      float &rmsd_location( m_RMSDError( THREAD_ID));
      float &reconstruction_location( m_ReconstructionError( THREAD_ID));
      for
      (
        size_t feature_id( data.GetMin()), feature_end( data.GetMax());
        feature_id < feature_end;
        ++feature_id
      )
      {
        // training step
        const size_t eff_id( m_Order( feature_id));
        linal::VectorConstReference< float> feature( m_TrainingData->GetFeaturesPtr()->operator()( eff_id));

        // get the 1st layer error
        reconstruction_location += m_Layers( 0).ComputeReconstructionError( feature, first_hidden_tmp, first_visible_tmp);

        linal::VectorConstReference< float> result( results.GetRow( eff_id));
        const linal::VectorConstReference< float> original_result( result);

        // propagate the feature through the various layers
        for( size_t layer_number( 0); layer_number < number_layers_before_output; ++layer_number)
        {
          feature = trainers( layer_number).Train( feature);
        }
        // propagate through to the result
        result = trainers( number_layers_before_output).Train( feature, result);

        // track the RMSD between the reconstructed points
        float err_tracker( 0);
        // add the reconstruction error
        for
        (
          const float *itr_reconstructed( result.Begin()), *itr_original( original_result.Begin()), *itr_reconstructed_end( result.End());
          itr_reconstructed != itr_reconstructed_end;
          ++itr_reconstructed, ++itr_original
        )
        {
          err_tracker += math::Sqr( *itr_reconstructed - *itr_original);
        }
        rmsd_location += math::Sqrt( err_tracker / float( result.GetSize()));
      }
    } // TrainThread

    void ApproximatorRestrictedBoltzmannMachine::UpdateWeights()
    {
      for( size_t layer_number( 0), number_layers( m_Layers.GetSize()); layer_number < number_layers; ++layer_number)
      {
        // Update all features
        m_Trainers( 0)( layer_number).UpdateLayer
        (
          m_WeightDecay,
          *m_WeightUpdaters( layer_number),
          *m_VisibleBiasUpdaters( layer_number),
          *m_HiddenBiasUpdaters( layer_number),
          *m_VisibleNoiseUpdaters( layer_number),
          *m_HiddenNoiseUpdaters( layer_number)
        );
      }
    } // UpdateWeights

    //! @brief sets up the weight updaters and change weights
    //!        this has to be done unless the iterate is read from a file
    void ApproximatorRestrictedBoltzmannMachine::InitializeWeightUpdaters()
    {
      // create the architecture, excluding input neurons (for which there are no weight updaters)
      storage::Vector< size_t> visible_architecture( size_t( 1), m_TrainingData->GetFeatureSize());
      visible_architecture.Append( m_HiddenArchitecture);
      visible_architecture.PopBack();
      visible_architecture.LastElement() += m_TrainingData->GetResultSize();

      m_HiddenBiasUpdaters.Reset();
      m_HiddenNoiseUpdaters.Reset();
      m_WeightUpdaters.Reset();
      m_VisibleBiasUpdaters.Reset();
      m_VisibleNoiseUpdaters.Reset();
      const size_t number_layers( m_HiddenArchitecture.GetSize());
      m_HiddenBiasUpdaters.AllocateMemory( number_layers);
      m_HiddenNoiseUpdaters.AllocateMemory( number_layers);
      m_VisibleBiasUpdaters.AllocateMemory( number_layers);
      m_VisibleNoiseUpdaters.AllocateMemory( number_layers);
      m_WeightUpdaters.AllocateMemory( number_layers);

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( visible_architecture.Begin(), visible_architecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );
      BCL_Assert
      (
        math::Statistics::MinimumValue( m_HiddenArchitecture.Begin(), m_HiddenArchitecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      if( !m_BiasUpdateType.IsDefined())
      {
        m_BiasUpdateType = m_WeightUpdateType;
      }

      // initialize data for the given architecture
      for( size_t i( 0); i < number_layers; ++i)
      {
        // initialize weight updaters for the given architecture
        m_VisibleBiasUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
        m_HiddenBiasUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
        m_WeightUpdaters.PushBack( util::CloneToShPtr( *m_WeightUpdateType));
        m_VisibleBiasUpdaters.LastElement()->Initialize( visible_architecture( i));
        m_HiddenBiasUpdaters.LastElement()->Initialize( m_HiddenArchitecture( i));
        m_WeightUpdaters.LastElement()->Initialize( visible_architecture( i) * m_HiddenArchitecture( i));
        if( m_Type == RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
        {
          m_VisibleNoiseUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
          m_HiddenNoiseUpdaters.PushBack( util::CloneToShPtr( *m_BiasUpdateType));
          m_VisibleNoiseUpdaters.LastElement()->Initialize( visible_architecture( i));
          m_HiddenNoiseUpdaters.LastElement()->Initialize( m_HiddenArchitecture( i));
        }
      }
      // for binomial types, it is still necessary to have updaters for the noise, even though they are never used
      if( m_Type != RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
      {
        m_VisibleNoiseUpdaters = m_VisibleBiasUpdaters;
        m_HiddenNoiseUpdaters = m_HiddenBiasUpdaters;
      }
    }

    //! @brief sets the data set ranges up for the # of threads
    void ApproximatorRestrictedBoltzmannMachine::SetupDataSetRanges()
    {
      BCL_Assert
      (
        m_TrainingData.IsDefined(),
        "SetupDataSetRanges requires valid training data!"
      );

      size_t features_per_weight_update( std::min( m_TrainingData->GetSize(), m_UpdateEveryNthFeature));
      // 0 indicates batch mode, so if the m_UpdateEveryNthFeature was 0, use the data set size
      if( m_UpdateEveryNthFeature == size_t( 0))
      {
        features_per_weight_update = m_TrainingData->GetSize();
      }

      // redetermine the # of threads, since this influences how the data is split up
      m_NumberThreads = std::min( features_per_weight_update, sched::GetNumberCPUs());

      BCL_MessageStd( "Set up data set ranges with # threads: " + util::Format()( m_NumberThreads));

      // set up the training ranges for each thread
      m_DataSetRanges.Reset();

      // calculate the # of weigh updates per run through the data set
      const size_t number_epochs_per_data_set
      (
        ( m_TrainingData->GetSize() - 1) / features_per_weight_update + 1
      );
      features_per_weight_update = m_TrainingData->GetSize() / number_epochs_per_data_set;
      m_DataSetRanges.AllocateMemory( number_epochs_per_data_set * m_NumberThreads);

      // if m_TrainingData->GetSize() is not a multiple of features_per_weight_update, distribute the extra features
      // over the initial number_epochs_with_extra_feature epochs
      const size_t number_epochs_with_extra_feature( m_TrainingData->GetSize() % features_per_weight_update);

      size_t current_index( 0);

      for( size_t epoch_number( 0); epoch_number < number_epochs_per_data_set; ++epoch_number)
      {
        // determine the # of features that will be examined in this epoch
        const size_t features_in_epoch( features_per_weight_update + size_t( epoch_number < number_epochs_with_extra_feature));

        // # of features per thread per epoch
        const size_t min_features_per_thread( features_in_epoch / m_NumberThreads);

        // if features_per_weight_update is not evenly divisible by m_NumberThreads, then the extra features are distributed
        // over the initial number_threads_with_extra_feature threads
        const size_t number_threads_with_extra_feature( features_in_epoch % m_NumberThreads);

        // make data set ranges for each thread for this epoch
        for( size_t current( 0); current < m_NumberThreads; ++current)
        {
          const size_t current_width( min_features_per_thread + size_t( current < number_threads_with_extra_feature));

          m_DataSetRanges.PushBack
          (
            math::Range< size_t>
            (
              current_index,
              current_index + current_width
            )
          );
          current_index += current_width;
        }
      }

      m_RMSDError = linal::Vector< float>( m_NumberThreads, 0.0);
      m_ReconstructionError = linal::Vector< float>( m_NumberThreads, 0.0);

      // create the complete architecture, including input and output neurons
      // start by making a vector with just the input neurons
      storage::Vector< size_t> architecture( 1, m_TrainingData->GetFeatureSize());

      // append the hidden neurons
      if( !m_HiddenArchitecture.IsEmpty())
      {
        architecture.Append( m_HiddenArchitecture);
      }

      // create the architecture, excluding input neurons (for which there are no weight updaters)
      storage::Vector< size_t> visible_architecture( size_t( 1), m_TrainingData->GetFeatureSize());
      visible_architecture.Append( m_HiddenArchitecture);
      visible_architecture.PopBack();
      visible_architecture.LastElement() += m_TrainingData->GetResultSize();

      // ensure that there are no empty layers
      BCL_Assert
      (
        math::Statistics::MinimumValue( visible_architecture.Begin(), visible_architecture.End()) > 0
        && math::Statistics::MinimumValue( m_HiddenArchitecture.Begin(), m_HiddenArchitecture.End()) > 0,
        "Each layer must have at least one neuron!"
      );

      // initialize data for the given architecture
      m_Layers.Reset();
      const size_t number_layers( visible_architecture.GetSize());
      m_Layers.Resize( number_layers);
      m_Trainers.Resize( m_NumberThreads);
      for( size_t thread( 0); thread < m_NumberThreads; ++thread)
      {
        m_Trainers( thread).Reset();
        m_Trainers( thread).Resize( number_layers);
      }
      for( size_t i( 0); i < number_layers; ++i)
      {
        m_Layers( i) = RestrictedBoltzmannMachineLayer( visible_architecture( i), m_HiddenArchitecture( i), m_Type);
        for( size_t thread( 0); thread < m_NumberThreads; ++thread)
        {
          m_Trainers( thread)( i) = TrainRestrictedBoltzmannMachineLayer( m_Layers( i), m_NumberStochasticSteps);
        }
      }

      BCL_MessageStd( "Set up data set ranges with # ranges: " + util::Format()( m_DataSetRanges.GetSize()));
      BCL_MessageStd( "Set up data set ranges for updating every nth feature: " + util::Format()( features_per_weight_update));
    } // SetupDataSetRanges

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_support_vector_machine.h"

// includes from bcl - sorted alphabetically
#include "model/bcl_model_objective_function_constant.h"
#include "model/bcl_model_support_vector_machine.h"
#include "util/bcl_util_stopwatch.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    const size_t ApproximatorSupportVectorMachine::s_Lower_Bound( 0); //!<  0 indicates lower boundary
    const size_t ApproximatorSupportVectorMachine::s_Upper_Bound( 1); //!<  1 indicates upper boundary
    const size_t ApproximatorSupportVectorMachine::s_Free( 2);        //!<  2 indicates no boundary
    const float ApproximatorSupportVectorMachine::m_EPS_A( 1e-12);
    const float ApproximatorSupportVectorMachine::m_P( 0.1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorSupportVectorMachine::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorSupportVectorMachine())
    );

    //! @brief default constructor
    ApproximatorSupportVectorMachine::ApproximatorSupportVectorMachine() :
      m_CostParameterC( 0.0),
      m_Status(),
      m_Alpha(),
      m_Gradient(),
      m_GradientBar(),
      m_Bias(),
      m_Signs(),
      m_Labels(),
      m_ActiveSize(),
      m_ProbLength(),
      m_Model( new SupportVectorMachine),
      m_NumberIterations( 0),
      m_NumberCurrentSupportVectors( 0),
      m_OptimizationGapThreshold( 0.1),
      m_OptimizationGap( 1.0),
      m_LastObjectiveFunctionValue( 0.0)
    {
      ApproximatorBase::SetObjectiveFunction( GetDefaultObjectiveFunction());
    }

    //! @brief Iterate for Sequential Minimal Optimization Learning Algorithm
    //! @param COST_PARAMETER_C penalty parameter c for svm regression training
    //! @param MODEL initial support vector model
    //! @param TRAINING_DATA training data set of choice
    //! @param NUMBER_ITERATIONS
    ApproximatorSupportVectorMachine::ApproximatorSupportVectorMachine
    (
      const float COST_PARAMETER_C,
      const util::ShPtr< SupportVectorMachine> &MODEL,
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t NUMBER_ITERATIONS
    ) :
      m_CostParameterC( COST_PARAMETER_C),
      m_Status( storage::Vector< size_t>()),
      m_Alpha( storage::Vector< float>()),
      m_Gradient( storage::Vector< float>()),
      m_GradientBar( storage::Vector< float>()),
      m_Bias( storage::Vector< float>()),
      m_Signs( storage::Vector< int>()),
      m_Labels( storage::Vector< float>()),
      m_ActiveSize( 0),
      m_ProbLength( 2 * TRAINING_DATA->GetSize()),
      m_Model( MODEL.IsDefined() ? MODEL : util::ShPtr< SupportVectorMachine>( new SupportVectorMachine())),
      m_NumberIterations( NUMBER_ITERATIONS),
      m_NumberCurrentSupportVectors( 0),
      m_OptimizationGapThreshold( 0.1),
      m_OptimizationGap( 1.0),
      m_LastObjectiveFunctionValue( 0.0)
    {
      ApproximatorBase::SetObjectiveFunction( GetDefaultObjectiveFunction());

      // only used in conjunction with IteratateFromFile
      SetTrainingContinued( false);

      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new ApproximatorSupportVectorMachine copied from this instance
    ApproximatorSupportVectorMachine *ApproximatorSupportVectorMachine::Clone() const
    {
      return new ApproximatorSupportVectorMachine( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorSupportVectorMachine::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorSupportVectorMachine::GetAlias() const
    {
      static const std::string s_Name( "SupportVectorMachine");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorSupportVectorMachine::SetTrainingData( util::ShPtr< descriptor::Dataset> &DATA)
    {
      // if the number of possible support vectors added in every iteration exceeds the number of training data
      // note: in every iteration can be 2 support vectors added
      if( 2 * m_NumberIterations > DATA->GetSize())
      {
        // set number of internal iterations to 1
        m_NumberIterations = 1;

        BCL_MessageDbg
        (
          "Reset number of internal iterations since one iterations adds "
          "more support vectors then training data available"
        );
      }

      m_TrainingData = DATA;
      DATA->GetFeatures().Rescale( SupportVectorMachine::s_DefaultInputRange, RescaleFeatureDataSet::e_AveStd);
      DATA->GetResults().Rescale( SupportVectorMachine::s_DefaultInputRange, RescaleFeatureDataSet::e_AveStd);

      // set rescale functions and kernel svm model used in iterative training process
      m_Model = util::ShPtr< SupportVectorMachine>
      (
        new SupportVectorMachine( m_Model->GetKernel(), GetRescaleFeatureDataSet(), GetRescaleResultDataSet())
      );

      BCL_MessageStd( "before InitializeMemberVectorsForTraining");
      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Interate properly
      if( !IsTrainingContinued())
      {
        BCL_MessageStd( "InitializeMemberVectorsForTraining");
        // initialize model
        InitializeMemberVectorsForTraining();
      }
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorSupportVectorMachine::GetCurrentModel() const
    {
      return m_Model;
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorSupportVectorMachine::GetCurrentApproximation() const
    {
      util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > current_model;

      if
      (
        !m_ObjectiveFunction->GetImplementation().IsDefined()
        || m_ObjectiveFunction->GetImplementation().GetAlias() == "Constant"
      )
      {
        // create final pair with model and objective function evaluation
        current_model = util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_Model.HardCopy(), m_OptimizationGap)
        );
      }
      else
      {
        // create final pair with model and objective function evaluation
        current_model = util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_Model.HardCopy(), m_LastObjectiveFunctionValue)
        );
      }

      return current_model;
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorSupportVectorMachine::Next()
    {
      util::Stopwatch timer;

      if
      (
        m_OptimizationGap > m_OptimizationGapThreshold
        && m_NumberCurrentSupportVectors < float( 0.9) * m_TrainingData->GetSize()
      )
      {
        // iterate for a number of internal iterations
        for( size_t counter( 0); counter < m_NumberIterations; ++counter)
        {
          m_OptimizationGap = IterationStep();
        }

        // postprocess model and determine final support vectors
        FinalizeSupportVectorModel();

        m_NumberCurrentSupportVectors = m_Model->GetNumberSupportVectors();
        m_LastObjectiveFunctionValue = m_ObjectiveFunction->operator ()( util::ShPtr< Interface>( m_Model));
      }

      this->Track( GetCurrentApproximation());

      BCL_MessageStd
      (
        + " #SV: " + util::Format()( m_NumberCurrentSupportVectors)
        + " gap: " + util::Format()( m_OptimizationGap)
        + " threshold: " + util::Format()( m_OptimizationGapThreshold)
        + " time: " + util::Format()( timer.GetProcessDuration().GetTimeAsHourMinuteSecondMilliSeconds())
      );
    }

    //! @brief iterates one cycle and returns ShPtr to pair of resultant argument and corresponding score
    float ApproximatorSupportVectorMachine::IterationStep()
    {
      size_t first_vector_index( 0);
      size_t second_vector_index( 0);

      // determine index i and j for the two data vectors for which the quadratic problem has to be solved
      const float optimization_gap
      (
        DetermineFeatureVectorCombination( first_vector_index, second_vector_index)
      );

      // Solving Quadratic Problem sub problems for two given data vectors
      SolveQuadraticProblemSubProblem( first_vector_index, second_vector_index);

      // return optimization difference to error threshold epsilon
      return optimization_gap;
    }

    //! @brief initialize a default objective function constant as default objective function
    util::ShPtr< ObjectiveFunctionWrapper> ApproximatorSupportVectorMachine::GetDefaultObjectiveFunction()
    {
      return util::ShPtr< ObjectiveFunctionWrapper>
      (
        new ObjectiveFunctionWrapper
        (
          util::Implementation< ObjectiveFunctionInterface>
          (
            new ObjectiveFunctionConstant
            (
              std::numeric_limits< float>::max(),
              opti::e_SmallerEqualIsBetter
            )
          )
        )
      );
    }

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorSupportVectorMachine::CanContinue() const
    {
      return
        m_OptimizationGap > m_OptimizationGapThreshold
        && m_NumberCurrentSupportVectors < float( 0.9) * m_TrainingData->GetSize();
    }

    //! @brief finalize support vector model and determine final SVs, Alphas and Bias
    void ApproximatorSupportVectorMachine::FinalizeSupportVectorModel()
    {
      // reconstruct the whole gradient
      ReconstructGradient();

      // resort alpha vector to original length
      size_t alpha_count( 0);

      // counter for bound alphas
      size_t bound_alpha_count( 0);

      // counter for iteration
      size_t vector_counter( 0);

      // final vector with all alphas of support vectors
      storage::Vector< float> alpha_final;

      storage::List< size_t> sv_indices;

      // assembly of the final alpha vector of the lagrange multipliers
      for
      (
        storage::Vector< float>::const_iterator iter_alpha_begin( m_Alpha.Begin()),
        iter_alpha_middle( m_Alpha.Begin() + m_TrainingData->GetSize()),
        iter_alpha_middle_const( m_Alpha.Begin() + m_TrainingData->GetSize());
        iter_alpha_begin != iter_alpha_middle_const;
        ++iter_alpha_begin, ++iter_alpha_middle
      )
      {
        // individual lagrange multiplier value alpha
        // creating final alphas
        const float alpha_value( *iter_alpha_begin - *iter_alpha_middle);

        // if alpha is not zero then correspondent vector is a support vector
        if( alpha_value != 0)
        {
          alpha_final.PushBack( alpha_value);

          sv_indices.PushBack( vector_counter);

          ++alpha_count;
        }

        // increase iteration counter
        ++vector_counter;

        // if alpha lays has a value of penalty parameter C (boundary)
        if( alpha_value == m_CostParameterC)
        {
          ++bound_alpha_count;
        }
      }

      // matrix containing support vectors
      linal::Matrix< float> sv_matrix
      (
        sv_indices.GetSize(),                             // rows
        m_TrainingData->GetFeatureSize(),                 // cols
        float( 0)                                         // default value
      );

      // reset counter
      vector_counter = 0;

      // fill support vector matrix with vectors by support vector index
      for
      (
        storage::List< size_t>::const_iterator itr_sv( sv_indices.Begin()), itr_sv_end( sv_indices.End());
        itr_sv != itr_sv_end;
        ++itr_sv, ++vector_counter
      )
      {
        // get current support vector
        const FeatureReference< float> &support_vector( m_TrainingData->GetFeaturesPtr()->operator ()( *itr_sv));
        // copy support vector to support vector matrix
        std::copy( support_vector.Begin(), support_vector.End(), sv_matrix[ vector_counter]);
      }

      // assemble SV Model
      m_Model->SetAlpha( alpha_final);
      m_Model->SetBias( CalculateBias());
      m_Model->SetSupportVectors( FeatureDataSet< float>( sv_matrix));
      m_Model->SetNumberSupportVectors( m_Model->GetSupportVectors().GetNumberFeatures());
      m_Model->SetNumberBoundSupportVectors( bound_alpha_count);
    }

    //! read NeuralNetwork from std::istream
    std::istream &ApproximatorSupportVectorMachine::Read( std::istream &ISTREAM)
    {
      storage::Vector< float>::s_Instance.IsDefined();

      // read members
      io::Serialize::Read( m_CostParameterC, ISTREAM);
      io::Serialize::Read( m_Status, ISTREAM);
      io::Serialize::Read( m_Alpha, ISTREAM);
      io::Serialize::Read( m_Gradient, ISTREAM);
      io::Serialize::Read( m_GradientBar, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Signs, ISTREAM);
      io::Serialize::Read( m_Labels, ISTREAM);
      io::Serialize::Read( m_ActiveSize, ISTREAM);
      io::Serialize::Read( m_ProbLength, ISTREAM);
      io::Serialize::Read( m_Model, ISTREAM);
      io::Serialize::Read( m_NumberIterations, ISTREAM);
      io::Serialize::Read( m_OptimizationGapThreshold, ISTREAM);
      io::Serialize::Read( m_OptimizationGap, ISTREAM);
      io::Serialize::Read( m_LastObjectiveFunctionValue, ISTREAM);

      // return
      return ISTREAM;
    }

    //! write NeuralNetwork into std::ostream
    std::ostream &ApproximatorSupportVectorMachine::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_CostParameterC, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Status, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Alpha, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Gradient, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_GradientBar, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Signs, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Labels, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_ActiveSize, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_ProbLength, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Model, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_NumberIterations, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_OptimizationGapThreshold, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_OptimizationGap, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_LastObjectiveFunctionValue, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief method checks whether a LaGrange Multiplier reached a certain boundary or not
    //! @param ALPHA a LaGrange Multiplier
    //! @return const int that indicates whether ALPHA reached a certain boundary or not
    int ApproximatorSupportVectorMachine::AlphaToStatus( const float &ALPHA) const
    {
      if( ALPHA >= m_CostParameterC - m_EPS_A)
      {
        return s_Upper_Bound;
      }
      else if( ALPHA <= m_EPS_A)
      {
        return s_Lower_Bound;
      }
      else
      {
        return s_Free;
      }
    }

    //! @brief
    void ApproximatorSupportVectorMachine::UpdateAlphaStatus( const int &FEATURE_VECTOR_I)
    {
      if( m_Alpha( FEATURE_VECTOR_I) >= m_CostParameterC)
      {
        m_Status( FEATURE_VECTOR_I) = s_Upper_Bound;
      }
      else if( m_Alpha( FEATURE_VECTOR_I) <= 0)
      {
        m_Status( FEATURE_VECTOR_I) = s_Lower_Bound;
      }
      else
      {
        m_Status( FEATURE_VECTOR_I) = s_Free;
      }
    }

    //! @brief method checks whether a feature_vector i reached the upper boundary
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached upper boundary
    inline bool ApproximatorSupportVectorMachine::IsUpperBound( const size_t &FEATURE_VECTOR_I) const
    {
      return m_Status( FEATURE_VECTOR_I) == s_Upper_Bound;
    }

    //! @brief checks whether a feature_vector i reached the lower boundary
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached lower boundary
    inline bool ApproximatorSupportVectorMachine::IsLowerBound( const size_t &FEATURE_VECTOR_I) const
    {
      return m_Status( FEATURE_VECTOR_I) == s_Lower_Bound;
    }

    //! @brief checks whether a feature_vector i is not bound and between the boundaries
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached no boundary
    inline bool ApproximatorSupportVectorMachine::IsFree( const size_t &FEATURE_VECTOR_I) const
    {
      return m_Status( FEATURE_VECTOR_I) == s_Free;
    }

    //! @brief Initialize member vectors and variables to prepare SVR training
    void ApproximatorSupportVectorMachine::InitializeMemberVectorsForTraining()
    {
      // Initialization of variables and vectors
      const size_t number_feature_vectors( m_TrainingData->GetSize());

      // has to be set in case of this class is read from file and constructor was not explicitely applied
      m_ActiveSize = 0;
      m_ProbLength = 2 * number_feature_vectors;

      // initialize vector for lagrange multipliers
      m_Alpha = storage::Vector< float>( m_ProbLength, 0.0);

      // initialize vector for labeling of
      m_Labels = storage::Vector< float>();
      m_Labels.InsertElements( 0, 1.0, number_feature_vectors);
      m_Labels.InsertElements( number_feature_vectors, -1.0, number_feature_vectors);

      // initialize vector for signs of every label
      m_Signs = storage::Vector< int>();
      m_Signs.InsertElements( 0, 1, number_feature_vectors);
      m_Signs.InsertElements( number_feature_vectors, -1, number_feature_vectors);

      // status indication of KKT determination for every vector
      m_Status = storage::Vector< size_t>( m_ProbLength, size_t( 0));

      //
      m_GradientBar = storage::Vector< float>( m_ProbLength, 0.0);

      // initialization of bias for support vector model
      m_Bias = storage::Vector< float>( 2 * number_feature_vectors, m_P);

      // initialization of gradients for support vector model
      m_Gradient = storage::Vector< float>( 2 * number_feature_vectors);

      // initializing
      for( size_t progress( 0); progress < number_feature_vectors; ++progress)
      {
        const float result( m_TrainingData->GetResultsPtr()->operator ()( progress)( 0));
        m_Bias( progress) -= result;
        m_Bias( number_feature_vectors + progress) += result;
      }

      m_Gradient = m_Bias;

      // update alpha_status
      for( size_t progress( 0); progress < m_ProbLength; ++progress)
      {
        UpdateAlphaStatus( progress);
      }

      m_ActiveSize = m_ProbLength;

      linal::Vector< float> Q_i;

      // initializing Gradients
      for( size_t progress( 0); progress < m_ActiveSize; ++progress)
      {
        if( !IsLowerBound( progress))
        {
          Q_i = m_Model->GetKernel()->GetInputVectorIKernelMatrix( *( m_TrainingData->GetFeaturesPtr()), progress, m_Signs, m_ProbLength);

          for( size_t progress_internal( 0); progress_internal < m_ActiveSize; ++progress_internal)
          {
            m_Gradient( progress_internal) += m_Alpha( progress) * Q_i( progress_internal);
          }

          if( IsUpperBound( progress))
          {
            for( size_t progress_internal( 0); progress_internal < m_ActiveSize; ++progress_internal)
            {
              m_GradientBar( progress_internal) += m_CostParameterC * Q_i( progress_internal);
            }
          }
        }
      }

      BCL_MessageDbg( " Initialization.. complete");
    }

    //! @brief determine the bias value for Support Vector Regression Model
    //! @return calculated bias value
    float ApproximatorSupportVectorMachine::CalculateBias() const
    {
      float bias;
      int nr_free( 0);
      float sum_free( 0);

      // initialize upper and lower boundaries
      float upper_bound(  std::numeric_limits< float>::infinity());
      float lower_bound( -std::numeric_limits< float>::infinity());

      size_t progress( 0);
      // iterate over all labes and gradients to update upper and lower boundaries
      for
      (
        storage::Vector< float>::const_iterator iter_begin_labels( m_Labels.Begin()),
        iter_begin_gradient( m_Gradient.Begin());
        iter_begin_labels != m_Labels.End();
        ++iter_begin_labels, ++iter_begin_gradient, ++progress
      )
      {
        // compute gradient of particular label
        const float label_gradient( *iter_begin_labels * ( *iter_begin_gradient));

        // check for boundary conditions and update upper or lower boundaries accordingly
        if( IsLowerBound( progress))
        {
          if( m_Labels( progress) > 0)
          {
            upper_bound = std::min( upper_bound, label_gradient);
          }
          else
          {
            lower_bound = std::max( lower_bound, label_gradient);
          }
        }
        else if( IsUpperBound( progress))
        {
          if( m_Labels( progress) < 0)
          {
            upper_bound = std::min( upper_bound, label_gradient);
          }
          else
          {
            lower_bound = std::max( lower_bound, label_gradient);
          }
        }
        else
        {
          // case where no boundary was hit
          ++nr_free;
          sum_free += label_gradient;
        }
      }

      // calculate bias
      if( nr_free > 0)
      {
        bias = sum_free / nr_free;
      }
      else
      {
        bias = ( upper_bound + lower_bound) / 2;
      }

      // return calculated bias value
      return bias;
    }

    //! @brief heuristic approach to find a feature_vector i and j
    //! @brief depending on m_Gradients for examination of SMO Classification
    //! @param TRAINING_DATA data set of feature vectors inclusive labels
    //! @param SVR_MODEL
    //! @param FIRST_VECTOR_INDEX
    //! @param SECOND_VECTOR_INDEX
    //! @return pair of two indices for feature_vector i and j
    float ApproximatorSupportVectorMachine::DetermineFeatureVectorCombination
    (
      size_t &FIRST_VECTOR_INDEX,
      size_t &SECOND_VECTOR_INDEX
    ) const
    {
      // return i,j such that
      // i: maximizes -y_i * grad(f)_i, i in I_up(\alpha)
      // j: minimizes the decrease of obj value
      //    (if quadratic coefficient <= 0, replace it with tau)
      //    -y_j*grad(f)_j < -y_i*grad(f)_i, j in I_low(\alpha)

      float maximum_gradient( -1 * std::numeric_limits< float>::infinity()); //-INFINITY;
      float Gmax2( -1 * std::numeric_limits< float>::infinity()); //-INFINITY;

      int maximum_gradient_index( -1);
      int Gmin_idx( -1);

      size_t process( 0);

      for
      (
        storage::Vector< float>::const_iterator
          iter_begin_gradient( m_Gradient.Begin()),
          iter_end_gradient( m_Gradient.End()),
          iter_begin_labels( m_Labels.Begin());
        iter_begin_gradient != iter_end_gradient;
        ++iter_begin_gradient, ++iter_begin_labels, ++process
      )
      {
        if( *iter_begin_labels == +1)
        {
          if( !IsUpperBound( process))
          {
            if( -( *iter_begin_gradient) >= maximum_gradient)
            {
              maximum_gradient = -( *iter_begin_gradient);
              maximum_gradient_index = process;
            }
          }
        }
        else
        {
          if( !IsLowerBound( process))
          {
            if( *iter_begin_gradient >= maximum_gradient)
            {
              maximum_gradient = *iter_begin_gradient;
              maximum_gradient_index = process;
            }
          }
        }
      }

      const int index_i( maximum_gradient_index);

      linal::Vector< float> Q_i;

      if( index_i != -1) // null Q_i not accessed: Gmax=-INF if index_i=-1
      {
        Q_i = m_Model->GetKernel()->GetInputVectorIKernelMatrix
        (
          *( m_TrainingData->GetFeaturesPtr()),
          index_i,
          m_Signs,
          m_ProbLength
        );
      }

      linal::Vector< float> QD( m_ProbLength, float( 0.0));

      float kernel_value( 0.0);
      size_t progress( 0);

      for
      (
        linal::Vector< float>::iterator iter_begin_QD( QD.Begin()),
        iter_middle_QD( QD.Begin() + QD.GetSize() / 2),
        iter_end_QD( QD.End());
        iter_middle_QD != iter_end_QD;
        ++iter_begin_QD, ++iter_middle_QD, ++progress
      )
      {

        const FeatureReference< float> &item_ref_at_position( m_TrainingData->GetFeaturesPtr()->operator ()( progress));

        kernel_value = m_Model->GetKernel()->operator()
        (
          item_ref_at_position,
          item_ref_at_position
        );

        *iter_begin_QD  = kernel_value;
        *iter_middle_QD = kernel_value;
      }

      float obj_diff_min( std::numeric_limits< float>::infinity()); //+INFINITY;

      float grad_diff( 0);
      float obj_diff( 0);
      float quad_coef( 0);

      size_t index_j( 0);

      linal::Vector< float>::const_iterator iter_begin_q_i( Q_i.Begin());

      for
      (
        storage::Vector< float>::const_iterator
          iter_begin_labels( m_Labels.Begin()),
          iter_end_labels( m_Labels.End()),
          iter_begin_gradient( m_Gradient.Begin());
        iter_begin_labels != iter_end_labels;
        ++iter_begin_labels, ++iter_begin_gradient, ++iter_begin_q_i, ++index_j
      )
      {
        // if label is positive
        if( *iter_begin_labels == +1)
        {
          if( !IsLowerBound( index_j))
          {
            grad_diff = maximum_gradient + *iter_begin_gradient;

            if( -*iter_begin_gradient >= Gmax2)
            {
              Gmax2 = *iter_begin_gradient;
            }

            if( grad_diff > 0)
            {
              // QD (k) = QD ( k + l)
              quad_coef = Q_i( index_i) + QD( index_j) - 2 * m_Labels( index_i) * ( *iter_begin_q_i);

              if( quad_coef > 0)
              {
                obj_diff = -( grad_diff * grad_diff) / quad_coef;
              }
              else
              {
                obj_diff = -( grad_diff * grad_diff) / m_EPS_A;
              }

              if( obj_diff <= obj_diff_min)
              {
                Gmin_idx = index_j;
                obj_diff_min = obj_diff;
              }
            }
          }
        }
        else
        {
          // if label is negative
          if( !IsUpperBound( index_j))
          {
            grad_diff = maximum_gradient - *iter_begin_gradient;

            if( -*iter_begin_gradient >= Gmax2)
            {
              Gmax2 = -( *iter_begin_gradient);
            }

            if( grad_diff > 0)
            {
              quad_coef = Q_i( index_i) + QD( index_j) + 2 * m_Labels( index_i) * ( *iter_begin_q_i);

              if( quad_coef > 0)
              {
                obj_diff = -( grad_diff * grad_diff) / quad_coef;
              }
              else
              {
                obj_diff = -( grad_diff * grad_diff) / m_EPS_A;
              }

              if( obj_diff <= obj_diff_min)
              {
                Gmin_idx = index_j;
                obj_diff_min = obj_diff;
              }
            }
          }
        }
      }

      FIRST_VECTOR_INDEX  = maximum_gradient_index;
      SECOND_VECTOR_INDEX = Gmin_idx;

      return maximum_gradient + Gmax2; // optimization gap
    }

    //! @brief solve sub problem of quadratic problem according two feature vectors
    //! @param SVR_MODEL support vector machine model of interest
    //! @param FEATURE_VECTOR_I index of feature vector in MATRIX
    //! @param FEATURE_VECTOR_J index of feature vector in MATRIX
    //! @return a pair of vector of float with computed values of kernel function
    void ApproximatorSupportVectorMachine::SolveQuadraticProblemSubProblem
    (
      const int &FEATURE_VECTOR_I,
      const int &FEATURE_VECTOR_J
    )
    {
      const float old_m_alpha_i( m_Alpha( FEATURE_VECTOR_I));
      const float old_m_alpha_j( m_Alpha( FEATURE_VECTOR_J));

      // using Kernel Q_i(x) = K(i,x)
      linal::Vector< float> Q_i
      (
        m_Model->GetKernel()->GetInputVectorIKernelMatrix( *( m_TrainingData->GetFeaturesPtr()), FEATURE_VECTOR_I, m_Signs, m_ProbLength)
      );

      // using Kernel Q_j(x) = K(j,x)
      linal::Vector< float> Q_j
      (
        m_Model->GetKernel()->GetInputVectorIKernelMatrix( *( m_TrainingData->GetFeaturesPtr()), FEATURE_VECTOR_J, m_Signs, m_ProbLength)
      );

      float quad_coef( Q_i( FEATURE_VECTOR_I) + Q_j( FEATURE_VECTOR_J) + 2 * Q_i( FEATURE_VECTOR_J));

      if( quad_coef <= 0)
      {
        quad_coef = m_EPS_A;
      }

      // if one of the two classLabels is negative
      // computing LaGrange Multipliers m_Alpha
      if( m_Labels( FEATURE_VECTOR_I) * m_Labels( FEATURE_VECTOR_J) < 0)
      {
        const float delta( ( -1 * m_Gradient( FEATURE_VECTOR_I) - m_Gradient( FEATURE_VECTOR_J)) / quad_coef);

        const float diff( m_Alpha( FEATURE_VECTOR_I) - m_Alpha( FEATURE_VECTOR_J));

        m_Alpha( FEATURE_VECTOR_I) += delta;
        m_Alpha( FEATURE_VECTOR_J) += delta;

        if( diff > 0)
        {
          if( m_Alpha( FEATURE_VECTOR_J) < 0)
          {
            m_Alpha( FEATURE_VECTOR_J) = 0;
            m_Alpha( FEATURE_VECTOR_I) = diff;
          }
          if( m_Alpha( FEATURE_VECTOR_I) > m_CostParameterC)
          {
            m_Alpha( FEATURE_VECTOR_I) = m_CostParameterC;
            m_Alpha( FEATURE_VECTOR_J) = m_CostParameterC - diff;
          }
        }
        else
        {
          if( m_Alpha( FEATURE_VECTOR_I) < 0)
          {
            m_Alpha( FEATURE_VECTOR_I) = 0;
            m_Alpha( FEATURE_VECTOR_J) = -diff;
          }
          if( m_Alpha( FEATURE_VECTOR_J) > m_CostParameterC)
          {
            m_Alpha( FEATURE_VECTOR_J) = m_CostParameterC;
            m_Alpha( FEATURE_VECTOR_I) = m_CostParameterC + diff;
          }
        }
      }
      else // if both classLabels are positive
      {
        const float delta( ( m_Gradient( FEATURE_VECTOR_I) - m_Gradient( FEATURE_VECTOR_J)) / quad_coef);
        const float sum( m_Alpha( FEATURE_VECTOR_I) + m_Alpha( FEATURE_VECTOR_J));

        m_Alpha( FEATURE_VECTOR_I) -= delta;
        m_Alpha( FEATURE_VECTOR_J) += delta;

        if( sum > m_CostParameterC)
        {
          if( m_Alpha( FEATURE_VECTOR_I) > m_CostParameterC)
          {
            m_Alpha( FEATURE_VECTOR_I) = m_CostParameterC;
            m_Alpha( FEATURE_VECTOR_J) = sum - m_CostParameterC;
          }
          if( m_Alpha( FEATURE_VECTOR_J) > m_CostParameterC)
          {
            m_Alpha( FEATURE_VECTOR_J) = m_CostParameterC;
            m_Alpha( FEATURE_VECTOR_I) = sum - m_CostParameterC;
          }
        }
        else
        {
          if( m_Alpha( FEATURE_VECTOR_J) < 0)
          {
            m_Alpha( FEATURE_VECTOR_J) = 0;
            m_Alpha( FEATURE_VECTOR_I) = sum;
          }
          if( m_Alpha( FEATURE_VECTOR_I) < 0)
          {
            m_Alpha( FEATURE_VECTOR_I) = 0;
            m_Alpha( FEATURE_VECTOR_J) = sum;
          }
        }
      }

      // update Gradient
      const float delta_alpha_i( m_Alpha( FEATURE_VECTOR_I) - old_m_alpha_i);
      const float delta_alpha_j( m_Alpha( FEATURE_VECTOR_J) - old_m_alpha_j);

      auto iter_begin_Qi( Q_i.Begin()), iter_begin_Qj( Q_j.Begin());
      for
      (
        storage::Vector< float>::iterator
          iter_begin_gradient( m_Gradient.Begin()),
          iter_end_gradient( m_Gradient.End());
        iter_begin_gradient != iter_end_gradient;
        ++iter_begin_gradient, ++iter_begin_Qi, ++iter_begin_Qj
      )
      {
        *iter_begin_gradient += *iter_begin_Qi * delta_alpha_i + *iter_begin_Qj * delta_alpha_j;
      }

      // update alpha_status and m_GradientBar
      const bool feature_i_is_upper_bound( IsUpperBound( FEATURE_VECTOR_I));
      const bool feature_j_is_upper_bound( IsUpperBound( FEATURE_VECTOR_J));

      UpdateAlphaStatus( FEATURE_VECTOR_I);
      UpdateAlphaStatus( FEATURE_VECTOR_J);

      short cost_multiplier( 0);

      if( feature_i_is_upper_bound != IsUpperBound( FEATURE_VECTOR_I))
      {
        cost_multiplier += ( feature_i_is_upper_bound ? -1 : 1);
      }
      if( feature_j_is_upper_bound != IsUpperBound( FEATURE_VECTOR_J))
      {
        cost_multiplier += ( feature_j_is_upper_bound ? -1 : 1);
      }

      if( cost_multiplier != 0)
      {
        iter_begin_Qi = Q_i.Begin();
        const double effective_cost( cost_multiplier * m_CostParameterC);
        for
        (
          storage::Vector< float>::iterator
            iter_begin_gradient_bar( m_GradientBar.Begin()),
            iter_end_gradient_bar( m_GradientBar.End());
          iter_begin_gradient_bar != iter_end_gradient_bar;
          ++iter_begin_gradient_bar, ++iter_begin_Qi
        )
        {
          *iter_begin_gradient_bar += effective_cost * ( *iter_begin_Qi);
        }
      }

    }

    //! @brief reconstruct the gradients for the final model
    void ApproximatorSupportVectorMachine::ReconstructGradient()
    {
      // reconstruct inactive elements of m_Gradient from m_GradientBar and free variables
      if( m_ActiveSize == m_ProbLength)
      {
        return;
      }

      for
      (
        storage::Vector< float>::iterator iter_begin_gradient( m_Gradient.Begin()),
        iter_end_gradient( m_Gradient.End()),
        iter_begin_gradient_bar( m_GradientBar.Begin()),
        iter_begin_bias( m_Bias.Begin());
        iter_begin_gradient != iter_end_gradient;
        ++iter_begin_gradient, ++iter_begin_gradient_bar, ++iter_begin_bias
      )
      {
        *iter_begin_gradient = *iter_begin_gradient_bar + *iter_begin_bias;
      }

      // vector of kernel values of vector i and all other vectors in training set
      linal::Vector< float> Q_i;

      // progress counter accessing elements at a certain position in vector
      size_t progress( 0);

      // iterate over all LaGrange multipliers
      for
      (
        storage::Vector< float>::iterator
          iter_begin_alpha( m_Alpha.Begin()),
          iter_end_alpha( m_Alpha.End());
        iter_begin_alpha != iter_end_alpha;
        ++iter_begin_alpha, ++progress
      )
      {
        // if correspondent status values indicates that it is not between upper or lower bound
        if( IsFree( progress))
        {
          // compute kernel vector for specific input vector i
          Q_i = m_Model->GetKernel()->GetInputVectorIKernelMatrix( *( m_TrainingData->GetFeaturesPtr()), progress, m_Signs, m_ProbLength);

          linal::Vector< float>::iterator iter_begin_Qi( Q_i.Begin());

          // iterate over all feature vector gradients
          for
          (
            storage::Vector< float>::iterator
              iter_begin_gradient( m_Gradient.Begin() + m_ActiveSize),
              iter_end_gradient( m_Gradient.End());
            iter_begin_gradient != iter_end_gradient;
            ++iter_begin_gradient, ++iter_begin_Qi
          )
          {
            // add next alpha * kernel(i,i) to gradient
            *iter_begin_gradient += *iter_begin_alpha * ( *iter_begin_Qi);
          }
        }
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorSupportVectorMachine::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "trains a support vector machine using sequential-minimal-optimization "
        "(see http://en.wikipedia.org/wiki/Sequential_Minimal_Optimization)"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        GetDefaultObjectiveFunction()->GetImplementation().GetString()
      );
      parameters.AddInitializer
      (
        "kernel",
        "kernel used to map pairs of features onto a hyperplane",
        io::Serialization::GetAgent( &m_Model->GetKernel()),
        "RBF(gamma=0.5)"
      );

      parameters.AddInitializer
      (
        "iterations",
        "# of iterations used internally to improve the optimization gap",
        io::Serialization::GetAgent( &m_NumberIterations),
        "1"
      );
      parameters.AddInitializer
      (
        "cost",
        "controls the trade off between allowing training errors and forcing rigid margins; high values may lead to better training values, but risks overtraining",
        io::Serialization::GetAgent( &m_CostParameterC),
        "0.0"
      );
      parameters.AddInitializer
      (
        "gap_threshold",
        "optimization gap threshold - the training process will stop when threshold is reached, higher values lead to more training cycles ",
        io::Serialization::GetAgent( &m_OptimizationGapThreshold),
        "0.1"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_approximator_support_vector_machine_multi_output.h"

// includes from bcl - sorted alphabetically
#include "command/bcl_command_parameter_check_ranged.h"
#include "linal/bcl_linal_symmetric_eigensolver.h"
#include "model/bcl_model_kappa_nearest_neighbor.h"
#include "model/bcl_model_objective_function_constant.h"
#include "model/bcl_model_support_vector_machine_multi_output.h"
#include "util/bcl_util_stopwatch.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    const size_t ApproximatorSupportVectorMachineMultiOutput::s_Lower_Bound( 0); //!<  0 indicates lower boundary
    const size_t ApproximatorSupportVectorMachineMultiOutput::s_Upper_Bound( 1); //!<  1 indicates upper boundary
    const size_t ApproximatorSupportVectorMachineMultiOutput::s_Free( 2);        //!<  2 indicates no boundary
    const float ApproximatorSupportVectorMachineMultiOutput::m_EPS_A( 1e-12);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ApproximatorSupportVectorMachineMultiOutput::s_Instance
    (
      util::Enumerated< ApproximatorBase>::AddInstance( new ApproximatorSupportVectorMachineMultiOutput())
    );

    //! @brief default constructor
    ApproximatorSupportVectorMachineMultiOutput::ApproximatorSupportVectorMachineMultiOutput() :
      m_CostParameterC( 0.0),
      m_Status(),
      m_Alpha(),
      m_Gradient(),
      m_Bias(),
      m_Model( new SupportVectorMachineMultiOutput),
      m_ObjectiveFunction( GetDefaultObjectiveFunction()),
      m_NumberIterations( 0),
      m_NumberCurrentSupportVectors( 0),
      m_OptimizationGapThreshold( 0.1),
      m_OptimizationGap( 1.0),
      m_LastObjectiveFunctionValue( 0.0),
      m_Kappa( 1),
      m_CoordinateSystems(),
      m_CoordinateSystemsTransposed(),
      m_Epsilon( 0.001),
      m_SelfKernelValues()
    {
      ApproximatorBase::SetObjectiveFunction( GetDefaultObjectiveFunction());
    }

    //! @brief Iterate for Sequential Minimal Optimization Learning Algorithm
    //! @param COST_PARAMETER_C penalty parameter c for svm regression training
    //! @param MODEL initial support vector model
    //! @param TRAINING_DATA training data set of choice
    //! @param NUMBER_ITERATIONS
    ApproximatorSupportVectorMachineMultiOutput::ApproximatorSupportVectorMachineMultiOutput
    (
      const float COST_PARAMETER_C,
      const util::ShPtr< SupportVectorMachineMultiOutput> &MODEL,
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t NUMBER_ITERATIONS
    ) :
      m_CostParameterC( COST_PARAMETER_C),
      m_Status(),
      m_Alpha(),
      m_Gradient(),
      m_Bias(),
      m_Model( MODEL.IsDefined() ? MODEL : util::ShPtr< SupportVectorMachineMultiOutput>( new SupportVectorMachineMultiOutput())),
      m_ObjectiveFunction( GetDefaultObjectiveFunction()),
      m_NumberIterations( NUMBER_ITERATIONS),
      m_NumberCurrentSupportVectors( 0),
      m_OptimizationGapThreshold( 0.1),
      m_OptimizationGap( 1.0),
      m_LastObjectiveFunctionValue( 0.0),
      m_Kappa( 1),
      m_CoordinateSystems(),
      m_CoordinateSystemsTransposed(),
      m_Epsilon( 0.001),
      m_SelfKernelValues()
    {
      ApproximatorBase::SetObjectiveFunction( GetDefaultObjectiveFunction());

      // only used in conjunction with IteratateFromFile
      SetTrainingContinued( false);

      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new ApproximatorSupportVectorMachineMultiOutput copied from this instance
    ApproximatorSupportVectorMachineMultiOutput *ApproximatorSupportVectorMachineMultiOutput::Clone() const
    {
      return new ApproximatorSupportVectorMachineMultiOutput( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ApproximatorSupportVectorMachineMultiOutput::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ApproximatorSupportVectorMachineMultiOutput::GetAlias() const
    {
      static const std::string s_Name( "MultiOutputSVM");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void ApproximatorSupportVectorMachineMultiOutput::SetTrainingData( util::ShPtr< descriptor::Dataset> &DATA)
    {
      // if the number of possible support vectors added in every iteration exceeds the number of training data
      // note: in every iteration can be 2 support vectors added
      if( 2 * m_NumberIterations > DATA->GetSize())
      {
        // set number of internal iterations to 1
        m_NumberIterations = 1;

        BCL_MessageDbg
        (
          "Reset number of internal iterations since one iterations adds "
          "more support vectors then training data available"
        );
      }

      m_TrainingData = DATA;
      DATA->GetFeatures().Rescale( SupportVectorMachineMultiOutput::s_DefaultInputRange, RescaleFeatureDataSet::e_AveStd);
      DATA->GetResults().Rescale( SupportVectorMachineMultiOutput::s_DefaultInputRange, RescaleFeatureDataSet::e_AveStd);

      // set rescale functions and kernel svm model used in iterative training process
      m_Model = util::ShPtr< SupportVectorMachineMultiOutput>
      (
        new SupportVectorMachineMultiOutput( m_Model->GetKernel(), GetRescaleFeatureDataSet(), GetRescaleResultDataSet())
      );

      // assemble SV Model
      m_Bias = linal::Vector< float>( m_TrainingData->GetResultSize(), float( 0.0));
      m_Model->SetBias( m_Bias);

      BCL_MessageStd( "before InitializeMemberVectorsForTraining");
      // if the current instance is NOT used through IterateInterfaceFromFile
      // then initialize the Interate properly
      if( !IsTrainingContinued())
      {
        BCL_MessageStd( "InitializeMemberVectorsForTraining");
        // initialize model and LLT coordinate systems
        InitializeMemberVectorsForTraining();
        ComputeCoordinateSystems();
      }
    }

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> ApproximatorSupportVectorMachineMultiOutput::GetCurrentModel() const
    {
      return m_Model;
    }

    //! @brief returns the current approximation
    //! @return current argument result pair
    const util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
      ApproximatorSupportVectorMachineMultiOutput::GetCurrentApproximation() const
    {
      util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> > current_model;

      if
      (
        !m_ObjectiveFunction->GetImplementation().IsDefined()
        || m_ObjectiveFunction->GetImplementation().GetAlias() == "Constant"
      )
      {
        // create final pair with model and objective function evaluation
        current_model = util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_Model.HardCopy(), m_OptimizationGap)
        );
      }
      else
      {
        // create final pair with model and objective function evaluation
        current_model = util::ShPtr< storage::Pair< util::ShPtr< Interface>, float> >
        (
          new storage::Pair< util::ShPtr< Interface>, float>( m_Model.HardCopy(), m_LastObjectiveFunctionValue)
        );
      }

      return current_model;
    }

    //! @brief conducts the next approximation step and stores the approximation
    void ApproximatorSupportVectorMachineMultiOutput::Next()
    {
      util::Stopwatch timer;

      if
      (
        m_OptimizationGap > m_OptimizationGapThreshold
        && m_NumberCurrentSupportVectors < float( 0.9) * m_TrainingData->GetSize()
      )
      {
        // iterate for a number of internal iterations
        for( size_t counter( 0); counter < m_NumberIterations; ++counter)
        {
           IterationStep();
        }

        // postprocess model and determine final support vectors
        m_OptimizationGap = FinalizeSupportVectorModel();

        m_NumberCurrentSupportVectors = m_Model->GetNumberSupportVectors();
        m_LastObjectiveFunctionValue = m_ObjectiveFunction->operator ()( util::ShPtr< Interface>( m_Model));
      }

      this->Track( GetCurrentApproximation());

      BCL_MessageStd
      (
        + " #SV: " + util::Format()( m_NumberCurrentSupportVectors)
        + " gap: " + util::Format()( m_OptimizationGap)
        + " threshold: " + util::Format()( m_OptimizationGapThreshold)
        + " time: " + util::Format()( timer.GetProcessDuration().GetTimeAsHourMinuteSecondMilliSeconds())
      );
    }

    //! @brief iterates one cycle and returns ShPtr to pair of resultant argument and corresponding score
    float ApproximatorSupportVectorMachineMultiOutput::IterationStep()
    {
      size_t first_vector_index( 0);
      size_t second_vector_index( 0);

      // determine index i and j for the two data vectors for which the quadratic problem has to be solved
      DetermineFeatureVectorCombination( first_vector_index, second_vector_index);

      // Solving Quadratic Problem sub problems for two given data vectors
      return SolveQuadraticProblemSubProblem( first_vector_index, second_vector_index, true);
    }

    //! @brief initialize a default objective function constant as default objective function
    util::ShPtr< ObjectiveFunctionWrapper> ApproximatorSupportVectorMachineMultiOutput::GetDefaultObjectiveFunction()
    {
      return util::ShPtr< ObjectiveFunctionWrapper>
      (
        new ObjectiveFunctionWrapper
        (
          util::Implementation< ObjectiveFunctionInterface>
          (
            new ObjectiveFunctionConstant
            (
              std::numeric_limits< float>::max(),
              opti::e_SmallerEqualIsBetter
            )
          )
        )
      );
    }

    //! @brief evaluates whether the approximation can continue
    //! @return true, if the approximation can continue - otherwise false
    bool ApproximatorSupportVectorMachineMultiOutput::CanContinue() const
    {
      return
        m_OptimizationGap > m_OptimizationGapThreshold
        && m_NumberCurrentSupportVectors < float( 0.9) * m_TrainingData->GetSize();
    }

    //! @brief finalize support vector model and determine final SVs, Alphas and Bias
    float ApproximatorSupportVectorMachineMultiOutput::FinalizeSupportVectorModel()
    {
      // counter of support vectors
      size_t beta_count( 0);

      // counter for iteration
      size_t current_vector_index( 0);

      //calculate bias
      const linal::Vector< float> bias( CalculateBias());
      m_Bias = bias;

      // determine optimization gap = sqrt( sum( (error - bias)^2) / ( number_outputs * number_features))
      // this is similar to standard derivation and measures the deviation of predictions and outputs
      // sum of error vectors squares
      float error_sum( 0.0);

      // go over all error/gradient vectors and square them using dot product
      for
      (
          storage::Vector< linal::Vector< float> >::const_iterator itr_error( m_Gradient.Begin()),
          itr_error_end( m_Gradient.End());
          itr_error != itr_error_end;
          ++itr_error
      )
      {
        const linal::Vector< float> reduced_gradient( *itr_error - bias);
        error_sum += reduced_gradient * reduced_gradient;
      }

      // to get a standard-derivation-like value divide by the number of outputs and take the sqrt
      // it is independent of the number of outputs
      const float optimization_gap
      (
        math::Sqrt( error_sum / float( m_TrainingData->GetSize() * m_TrainingData->GetResultSize()))
      );

      // count alpha vector elements that hit C boundary
      // there should be a reasonable number of bound alphas if there are too much or too less adjust m_CostParameterC
      size_t bound_alphas( 0);
      for( size_t index_alpha( 0); index_alpha < m_TrainingData->GetFeatureSize(); ++index_alpha)
      {
        for( size_t alpha_element( 0); alpha_element < m_TrainingData->GetResultSize(); ++alpha_element)
        {
          if( !IsFree( index_alpha, alpha_element))
          {
            ++bound_alphas;
          }
        }
      }

      BCL_Message( util::Message::e_Standard, "Bound Alphas: " + util::Format()( bound_alphas));

      // final vector with all betas of support vectors. beta = transposed coordinate system * alpha
      storage::Vector< linal::Vector< float> > betas;

      // list of indizes of the support vectors
      storage::List< size_t> sv_indices;

      // assembly of the final beta vector of the lagrange multipliers
      storage::Vector< linal::Matrix< float> >::const_iterator iter_coord_sys( m_CoordinateSystems.Begin());

      // go over all alpha vectors
      for
      (
        storage::Vector< linal::Vector< float> >::const_iterator iter_alpha( m_Alpha.Begin()),
        iter_alpha_end( m_Alpha.End());
        iter_alpha != iter_alpha_end;
        ++iter_alpha, ++iter_coord_sys
      )
      {
        // iterate over all alpha elements and set them to zero if their value is smaller than the threshold m_Epsilon
        linal::Vector< float> current_alpha( *iter_alpha);
        for
        (
            linal::Vector< float>::iterator iter_alpha_element( current_alpha.Begin()),
            iter_alpha_element_end( current_alpha.End());
            iter_alpha_element != iter_alpha_element_end;
            ++iter_alpha_element
        )
        {
          // check whether alpha element is in epsilon tube
          if( *iter_alpha_element < m_Epsilon && *iter_alpha_element > -m_Epsilon)
          {
            *iter_alpha_element = 0;
          }
        }
        // compute the beta vector
        const linal::Vector< float> current_beta( *iter_coord_sys * ( current_alpha));

        // make it support vector if at least one element is nonzero
        bool is_support_vector( false);

        // iterate through all betas
        for
        (
            linal::Vector< float>::const_iterator iter_beta( current_beta.Begin()),
            iter_beta_end( current_beta.End());
            iter_beta != iter_beta_end;
            ++iter_beta
        )
        {
          // if beta is not zero then correspondent vector is a support vector
          if( *iter_beta > m_EPS_A || *iter_beta < -m_EPS_A)
          {
            is_support_vector = true;
          }
        }
        // store beta of support vector in variable betas
        if( is_support_vector)
        {
          betas.PushBack( current_beta);
          sv_indices.PushBack( current_vector_index);
          ++beta_count;
        }

        // increase iteration counter
        ++current_vector_index;
      }

      // matrix containing support vectors
      linal::Matrix< float> sv_matrix
      (
        sv_indices.GetSize(),                             // rows
        m_TrainingData->GetFeatureSize(),                 // cols
        float( 0)                                         // default value
      );

      // reset counter
      current_vector_index = 0;

      // fill support vector matrix with vectors by support vector index
      for
      (
        storage::List< size_t>::const_iterator itr_sv( sv_indices.Begin()), itr_sv_end( sv_indices.End());
        itr_sv != itr_sv_end;
        ++itr_sv, ++current_vector_index
      )
      {
        // get current support vector
        const FeatureReference< float> &support_vector( m_TrainingData->GetFeatures()( *itr_sv));
        // copy support vector to support vector matrix
        std::copy( support_vector.Begin(), support_vector.End(), sv_matrix[ current_vector_index]);
      }

      // assemble SV Model
      m_Model->SetBeta( betas);
      m_Model->SetBias( bias);
      m_Model->SetSupportVectors( FeatureDataSet< float>( sv_matrix));
      m_Model->SetNumberSupportVectors( m_Model->GetSupportVectors().GetNumberFeatures());

      return optimization_gap;
    }

    //! @brief update status of one complete alpha vector according to its alpha vector
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    void ApproximatorSupportVectorMachineMultiOutput::UpdateAlphaStatus( const size_t &FEATURE_VECTOR_I)
    {
      const linal::Vector< float> &alpha_i( m_Alpha( FEATURE_VECTOR_I));
      storage::Vector< size_t> &status_i( m_Status( FEATURE_VECTOR_I));
      linal::Vector< float>::const_iterator itr_alpha( alpha_i.Begin());
      storage::Vector< size_t>::iterator itr_stat( status_i.Begin()), itr_stat_end( status_i.End());

      // update boundary status for every element of the alpha vector
      for( ; itr_stat != itr_stat_end; ++itr_stat, ++itr_alpha)
      {
        if( *itr_alpha >= m_CostParameterC - m_EPS_A)
        {
          *itr_stat = s_Upper_Bound;
        }
        else if( *itr_alpha <= m_EPS_A - m_CostParameterC)
        {
          *itr_stat = s_Lower_Bound;
        }
        else
        {
          *itr_stat = s_Free;
        }
      }
    }

    //! read NeuralNetwork from std::istream
    std::istream &ApproximatorSupportVectorMachineMultiOutput::Read( std::istream &ISTREAM)
    {
      storage::Vector< float>::s_Instance.IsDefined();

      // read members
      io::Serialize::Read( m_CostParameterC, ISTREAM);
      io::Serialize::Read( m_Status, ISTREAM);
      io::Serialize::Read( m_Alpha, ISTREAM);
      io::Serialize::Read( m_Gradient, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Model, ISTREAM);
      io::Serialize::Read( m_NumberIterations, ISTREAM);
      io::Serialize::Read( m_OptimizationGapThreshold, ISTREAM);
      io::Serialize::Read( m_OptimizationGap, ISTREAM);
      io::Serialize::Read( m_LastObjectiveFunctionValue, ISTREAM);
      io::Serialize::Read( m_Kappa, ISTREAM);
      io::Serialize::Read( m_CoordinateSystems, ISTREAM);
      io::Serialize::Read( m_CoordinateSystemsTransposed, ISTREAM);
      io::Serialize::Read( m_Epsilon, ISTREAM);
      io::Serialize::Read( m_SelfKernelValues, ISTREAM);

      // return
      return ISTREAM;
    }

    //! write NeuralNetwork into std::ostream
    std::ostream &ApproximatorSupportVectorMachineMultiOutput::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_CostParameterC, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Status, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Alpha, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Gradient, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Model, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_NumberIterations, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_OptimizationGapThreshold, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_OptimizationGap, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_LastObjectiveFunctionValue, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Kappa, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_CoordinateSystems, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_CoordinateSystemsTransposed, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_Epsilon, OSTREAM, INDENT) << std::endl;
      io::Serialize::Write( m_SelfKernelValues, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief method checks whether a feature_vector i reached the upper boundary
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @param ELEMENT position of the element - corresponds to the output column
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached upper boundary
    inline bool ApproximatorSupportVectorMachineMultiOutput::IsUpperBound( const size_t &FEATURE_VECTOR_I, const size_t &ELEMENT) const
    {
      return m_Status( FEATURE_VECTOR_I)( ELEMENT) == s_Upper_Bound;
    }
    //! @brief checks whether a feature_vector i reached the lower boundary
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @param ELEMENT position of the element - corresponds to the output column
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached lower boundary
    inline bool ApproximatorSupportVectorMachineMultiOutput::IsLowerBound( const size_t &FEATURE_VECTOR_I, const size_t &ELEMENT) const
    {
      return m_Status( FEATURE_VECTOR_I)( ELEMENT) == s_Lower_Bound;
    }

    //! @brief checks whether a feature_vector i is not bound and between the boundaries
    //! @param FEATURE_VECTOR_I a position of a feature vector in m_Status
    //! @param ELEMENT position of the element - corresponds to the output column
    //! @return const bool that indicates whether FEATURE_VECTOR_I reached no boundary
    inline bool ApproximatorSupportVectorMachineMultiOutput::IsFree( const size_t &FEATURE_VECTOR_I, const size_t &ELEMENT) const
    {
      return m_Status( FEATURE_VECTOR_I)( ELEMENT) == s_Free;
    }

    //! @brief Initialize member vectors and variables to prepare SVR training
    void ApproximatorSupportVectorMachineMultiOutput::InitializeMemberVectorsForTraining()
    {
      // Initialization of variables and vectors
      const size_t number_feature_vectors( m_TrainingData->GetSize());
      const size_t result_size( m_TrainingData->GetResultSize());

      // Initialize lagrange multiplier vectors
      m_Alpha = storage::Vector< linal::Vector< float> >
      (
        number_feature_vectors,
        linal::Vector< float>( result_size, float( 0.0))
      );

      //initialize alpha status
      m_Status = storage::Vector< storage::Vector< size_t> >
      (
        number_feature_vectors, storage::Vector< size_t>( result_size, size_t( 0))
      );

      //initialize alpha status
      for( size_t alpha_vector_index( 0); alpha_vector_index < number_feature_vectors; ++alpha_vector_index)
      {
        UpdateAlphaStatus( alpha_vector_index);
      }

      //compute the kernel value of each input vector with itself and store them in m_SelfKernelValues
      float kernel_value( 0.0);
      size_t progress( 0);
      m_SelfKernelValues = linal::Vector< float>( number_feature_vectors, float( 0.0));
      for
      (
        linal::Vector< float>::iterator iter_begin( m_SelfKernelValues.Begin()),
        iter_end( m_SelfKernelValues.End());
        iter_begin != iter_end;
        ++iter_begin, ++progress
      )
      {
        // get current feature
        const FeatureReference< float> &item_ref_at_position( m_TrainingData->GetFeatures()( progress));

        // compute self kernel value
        kernel_value = m_Model->GetKernel()->operator()( item_ref_at_position, item_ref_at_position);

        *iter_begin = kernel_value;
      }

      //initialize Error Vectors
      //Because all alphas are 0 in the beginning, m_Gradient = result
      //bias doesn't matter during training and gradients must not contain bias for bias calculation
      m_Gradient.AllocateMemory( number_feature_vectors);

      for( size_t i( 0); i < number_feature_vectors; ++i)
      {
        m_Gradient.PushBack( m_TrainingData->GetResults()( i));
      }

      BCL_Message( util::Message::e_Debug, " Initialization.. complete");
    }

    //! @brief determine the bias value for Support Vector Regression Model
    //! @return calculated bias value
    linal::Vector< float> ApproximatorSupportVectorMachineMultiOutput::CalculateBias() const
    {
      // initialization
      const size_t &result_size( m_TrainingData->GetResultSize());
      linal::Vector< float> bias( result_size, float( 0.0));
      int nr_free_gradients( 0);
      linal::Vector< float> sum_free_gradients( result_size, float( 0.0));

      // initialization of progress counter
      size_t progress( 0);

      // iterate over all gradients and sum gradients of non bound alphas
      for
      (
        storage::Vector< linal::Vector< float> >::const_iterator iter_gradient( m_Gradient.Begin()),
        iter_end_gradient( m_Gradient.End());
        iter_gradient != iter_end_gradient;
        ++iter_gradient, ++progress
      )
      {
        bool is_free( true);

        for( size_t element_id( 0); element_id < result_size; ++element_id)
        {
          // check for boundary conditions and update upper or lower boundaries accordingly
          if( !IsFree( progress, element_id))
          {
            is_free = false;
          }
        }
        if( is_free)
        {
          ++nr_free_gradients;
          sum_free_gradients += *iter_gradient;
        }
      }

      BCL_Assert( nr_free_gradients > 0, "All alpha vectors hit bounds. Adjust Parameter C");

      // calculate bias
      // iterate over all outputs
      for
      (
          linal::Vector< float>::iterator iter_bias( bias.Begin()),
          iter_sum_free( sum_free_gradients.Begin()),
          iter_end_bias( bias.End());
          iter_bias != iter_end_bias;
          ++iter_bias, ++iter_sum_free
      )
      {
        //if there are free elements take the average bias
        *iter_bias = *iter_sum_free / float( nr_free_gradients);
      }

      // return calculated bias value
      return bias;
    }

    //! @brief heuristic approach to find a feature_vector i and j
    //! @brief depending on m_Gradients for examination of SMO Classification
    //! @param TRAINING_DATA data set of feature vectors inclusive labels
    //! @param SVR_MODEL
    //! @param FIRST_VECTOR_INDEX
    //! @param SECOND_VECTOR_INDEX
    //! @return pair of two indices for feature_vector i and j
    void ApproximatorSupportVectorMachineMultiOutput::DetermineFeatureVectorCombination
    (
      size_t &FIRST_VECTOR_INDEX,
      size_t &SECOND_VECTOR_INDEX
    )
    {
      // initialize stopwatch
      util::Stopwatch timer;
      timer.Reset();
      timer.Start();

      // get current feature
      const float &number_features( m_TrainingData->GetFeaturesPtr()->GetNumberFeatures());

      // initialize
      float gradient_vector_i_sqr( -1);
      size_t feature_index_i( 0);
      bool found_feature_i( false);
      size_t process( 0);

      // determine i such that:
      // error vector transformed in local coordinate system has a maximum value,
      // bound hitting elements are ignored
      for
      (
        storage::Vector< linal::Vector< float> >::const_iterator
          iter_gradient( m_Gradient.Begin()),
          iter_end_gradient( m_Gradient.End());
        iter_gradient != iter_end_gradient;
        ++iter_gradient, ++process
      )
      {
        // initialize
        size_t element( 0);
        float sqr_sum( 0.0);

        // transform into local coordinate system
        const linal::Vector< float> trans_grad( m_CoordinateSystemsTransposed( process) * ( *iter_gradient - m_Bias));

        //iterate over all elements of transformed gradient
        for
        (
            linal::Vector< float>::const_iterator
              iter_trans_grad_element( trans_grad.Begin()),
              iter_end_trans_grad_element( trans_grad.End());
            iter_trans_grad_element != iter_end_trans_grad_element;
            ++iter_trans_grad_element, ++element
        )
        {
          // if the alpha would be adjusted in a direction where the boundary is hit, this element doesn't count
          if( *iter_trans_grad_element > 0)
          {
            if( !IsUpperBound( process, element))
            {
              sqr_sum += math::Sqr( *iter_trans_grad_element);
            }
          }
          else
          {
            if( !IsLowerBound( process, element))
            {
              sqr_sum += math::Sqr( *iter_trans_grad_element);
            }
          }
        }

        // save the maximum transformed error vector
        if( sqr_sum > gradient_vector_i_sqr)
        {
          gradient_vector_i_sqr = sqr_sum;
          feature_index_i = process;
          found_feature_i = true;
        }
      }

      // this happens if all gradients are zero or all alphas hit bounds
      BCL_Assert( found_feature_i, "no Vector i found");

      //for choosing j we solve the problem for every vector and take the one with the largest change in
      //beta = trans_coord_system * alpha
      float best_delta_bata_value( 0);
      size_t best_j( 0);
      for( size_t j( 0); j < number_features; ++j)
      {
        //finds the solution but doesn't apply it
        float delta_beta( SolveQuadraticProblemSubProblem( feature_index_i, j, false));

        if( best_delta_bata_value < delta_beta)
        {
          best_delta_bata_value = delta_beta;
          best_j = j;
        }
      }

      const size_t index_j( best_j);

      FIRST_VECTOR_INDEX  = feature_index_i;
      SECOND_VECTOR_INDEX = index_j;

      BCL_Message
      (
        util::Message::e_Debug, "selected i:" + util::Format()( feature_index_i)
        + " selected j:" + util::Format()( index_j)
      );

      timer.Stop();
      const util::Time total_duration_so_far( timer.GetTotalTime());

      BCL_Message( util::Message::e_Debug, "Select Duration: " + util::Format()( total_duration_so_far));
    }

    //! @brief solve sub problem of quadratic problem according two feature vectors
    //! @param SVR_MODEL support vector machine model of interest
    //! @param FEATURE_VECTOR_I index of feature vector in MATRIX
    //! @param FEATURE_VECTOR_J index of feature vector in MATRIX
    //! @return a pair of vector of float with computed values of kernel function
    float ApproximatorSupportVectorMachineMultiOutput::SolveQuadraticProblemSubProblem
    (
      const size_t &FEATURE_VECTOR_I,
      const size_t &FEATURE_VECTOR_J,
      const bool &APPLY_SOLUTION
    )
    {
      //get result size
      const size_t result_size( m_TrainingData->GetResultSize());

      // compute kernel value of chosen feature vectors
      const float k_ij
      (
        m_Model->GetKernel()->operator()
        (
          m_TrainingData->GetFeaturesPtr()->operator ()( FEATURE_VECTOR_I),
          m_TrainingData->GetFeaturesPtr()->operator ()( FEATURE_VECTOR_J)
        )
      );

      // compute quad_coef eta
      float quad_coef( m_SelfKernelValues( FEATURE_VECTOR_I) - 2 * k_ij + m_SelfKernelValues( FEATURE_VECTOR_J));
      if( quad_coef <= m_EPS_A) //TDO use max()
      {
        quad_coef = m_EPS_A;
      }

      //get local coordinate_systems and the transposed matrices
      const linal::Matrix< float> &coord_sys_i( m_CoordinateSystems( FEATURE_VECTOR_I));
      const linal::Matrix< float> &coord_sys_j( m_CoordinateSystems( FEATURE_VECTOR_J));
      const linal::Matrix< float> &coord_sys_i_trans( m_CoordinateSystemsTransposed( FEATURE_VECTOR_I));
      const linal::Matrix< float> &coord_sys_j_trans( m_CoordinateSystemsTransposed( FEATURE_VECTOR_J));

      //get error vectors at i and j
      linal::Vector< float> error_i( m_Gradient( FEATURE_VECTOR_I));
      linal::Vector< float> error_j( m_Gradient( FEATURE_VECTOR_J));

      //get Alphas
      const linal::Vector< float> &alpha_i_old( m_Alpha( FEATURE_VECTOR_I));
      const linal::Vector< float> &alpha_j_old( m_Alpha( FEATURE_VECTOR_J));

      //get gamma used in the summation constraint
      const linal::Vector< float> gamma( coord_sys_i * alpha_i_old + coord_sys_j * alpha_j_old);

      //compute new unclipped alphas
      linal::Vector< float> alpha_i_new( result_size, float( 0.0));
      linal::Vector< float> alpha_j_new( result_size, float( 0.0));

      // compute the unclipped alpha_i
      alpha_i_new = alpha_i_old + coord_sys_i_trans * ( error_i - error_j) / quad_coef;

      // fit alpha_i into boundary constraints
      for
      (
        linal::Vector< float>::iterator
        itr_alpha = alpha_i_new.Begin(),
        itr_alpha_end = alpha_i_new.End();
        itr_alpha != itr_alpha_end;
        ++itr_alpha
      )
      {
        if( *itr_alpha <= -m_CostParameterC - m_EPS_A)
        {
          *itr_alpha = -m_CostParameterC;
        }
        if( *itr_alpha >= m_CostParameterC + m_EPS_A)
        {
          *itr_alpha = m_CostParameterC;
        }
      }

      // check for boundary violation and loop-ajust the alphas
      bool bounds_correct( false);

      // set a constant value for max iterations to fit alphas into bounds
      const size_t max_bound_iterations( 5);

      for( size_t bound_iterations( 0); bound_iterations < max_bound_iterations && !bounds_correct; ++bound_iterations)
      {
        // calculate alpha_j using the summation constraint
        alpha_j_new = coord_sys_j_trans * ( gamma - coord_sys_i * alpha_i_new);

        // check bounds of alpha_j
        bounds_correct = true;
        for
        (
            linal::Vector< float>::iterator
            itr = alpha_j_new.Begin(),
            itr_end = alpha_j_new.End();
            itr != itr_end;
            ++itr
        )
        {
          // alpha can be numerical unstable, that's why violations smaller than m_EPS_A are not taken care of
          if( *itr > m_CostParameterC + m_EPS_A)
          {
            *itr = m_CostParameterC;
            bounds_correct = false;
          }
          if( *itr < -m_CostParameterC - m_EPS_A)
          {
            *itr = -m_CostParameterC;
            bounds_correct = false;
          }
        }
        if( bounds_correct)
        {
          break;
        }
        // compute cliped alpha_i if bounds were violated
        alpha_i_new = coord_sys_i_trans * ( gamma - coord_sys_j * alpha_j_new);

        // check bounds of alpha_i
        bounds_correct = true;
        for
        (
            linal::Vector< float>::iterator
            itr = alpha_i_new.Begin(),
            itr_end = alpha_i_new.End();
            itr != itr_end;
            ++itr
        )
        {
          if( *itr > m_CostParameterC + m_EPS_A)
          {
            *itr = m_CostParameterC;
            bounds_correct = false;
          }
          if( *itr < -m_CostParameterC - m_EPS_A)
          {
            *itr = -m_CostParameterC;
            bounds_correct = false;
          }
        }
      }

      if( !bounds_correct)
      {
        BCL_Message( util::Message::e_Debug, "No correct bounds during BoundInterations");
        //fix bounds ignoring summation constraint
        for
        (
            linal::Vector< float>::iterator
            itr_i = alpha_i_new.Begin(),
            itr_j = alpha_j_new.Begin(),
            itr_i_end = alpha_i_new.End();
            itr_i != itr_i_end;
            ++itr_i, ++itr_j
        )
        {
          if( *itr_i > m_CostParameterC + m_EPS_A)
          {
            *itr_i = m_CostParameterC;
          }
          if( *itr_i < -m_CostParameterC - m_EPS_A)
          {
            *itr_i = -m_CostParameterC;
          }
          if( *itr_j > m_CostParameterC + m_EPS_A)
          {
            *itr_j = m_CostParameterC;
          }
          if( *itr_j < -m_CostParameterC - m_EPS_A)
          {
            *itr_j = -m_CostParameterC;
          }
        }
      }

      //calculate delta beta
      const linal::Vector< float> delta_beta( coord_sys_i * ( alpha_i_new - alpha_i_old));
      const float delta_beta_value( delta_beta * delta_beta);

      if( APPLY_SOLUTION)
      {
        // using Kernel Q_i(x) = K(bound_iterations,x)
        const linal::Vector< float> Q_i
        (
          m_Model->GetKernel()->GetInputVectorIKernelMatrixMultiOutput( *( m_TrainingData->GetFeaturesPtr()), FEATURE_VECTOR_I)
        );
        // using Kernel Q_j(x) = K(j,x)
        const linal::Vector< float> Q_j
        (
          m_Model->GetKernel()->GetInputVectorIKernelMatrixMultiOutput( *( m_TrainingData->GetFeaturesPtr()), FEATURE_VECTOR_J)
        );

        //update Error vectors
        //transform the delta_alpha into the local coordinate system
        const linal::Vector< float> delta_alpha_i( coord_sys_i * ( alpha_i_new - alpha_i_old));
        const linal::Vector< float> delta_alpha_j( coord_sys_j * ( alpha_j_new - alpha_j_old));
        linal::Vector< float>::const_iterator iter_begin_Qi( Q_i.Begin()), iter_begin_Qj( Q_j.Begin());

        for
        (
          storage::Vector< linal::Vector< float> >::iterator
            iter_begin_gradient( m_Gradient.Begin()),
            iter_end_gradient( m_Gradient.End());
          iter_begin_gradient != iter_end_gradient;
          ++iter_begin_gradient, ++iter_begin_Qi, ++iter_begin_Qj
        )
        {
          *iter_begin_gradient -= *iter_begin_Qi * delta_alpha_i + *iter_begin_Qj * delta_alpha_j;
        }
        //submit new Alphas to m_Alpha
        m_Alpha( FEATURE_VECTOR_I) = alpha_i_new;
        m_Alpha( FEATURE_VECTOR_J) = alpha_j_new;

        //update Alpha status
        UpdateAlphaStatus( FEATURE_VECTOR_I);
        UpdateAlphaStatus( FEATURE_VECTOR_J);
      }

      return delta_beta_value;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ApproximatorSupportVectorMachineMultiOutput::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "trains a support vector machine using sequential-minimal-optimization "
        "(see http://en.wikipedia.org/wiki/Sequential_Minimal_Optimization)"
      );

      parameters.AddInitializer
      (
        "objective function",
        "function that evaluates the model after each batch step",
        io::Serialization::GetAgent( &m_ObjectiveFunction->GetImplementation()),
        GetDefaultObjectiveFunction()->GetImplementation().GetString()
      );
      parameters.AddInitializer
      (
        "kernel",
        "kernel used to map pairs of features onto a hyperplane",
        io::Serialization::GetAgent( &m_Model->GetKernel()),
        "RBF(gamma=0.5)"
      );

      parameters.AddInitializer
      (
        "iterations",
        "# of iterations used internally to improve the optimization gap",
        io::Serialization::GetAgent( &m_NumberIterations),
        "1"
      );
      parameters.AddInitializer
      (
        "cost",
        "controls the trade off between allowing training errors and forcing rigid margins; high values may lead to better training values, but risks overtraining",
        io::Serialization::GetAgent( &m_CostParameterC),
        "0.0"
      );
      parameters.AddInitializer
      (
        "gap_threshold",
        "optimization gap threshold - the training process will stop when threshold is reached, higher values lead to more training cycles ",
        io::Serialization::GetAgent( &m_OptimizationGapThreshold),
        "0.1"
      );

      parameters.AddInitializer
      (
        "kappa",
        "Number of nearest neighbors used for computation of locally linear transformation coordinate system",
        io::Serialization::GetAgent( &m_Kappa),
        "1"
      );

      parameters.AddInitializer
      (
        "epsilon",
        "Range within errors are ignored. The smaller the more accurate is the prediction and the more support vectors are needed ",
        io::Serialization::GetAgent( &m_Epsilon),
        "0.0001"
      );

      return parameters;
    }

    //! @brief computes for each output vector a local coordinate system used in the multi output method
    void ApproximatorSupportVectorMachineMultiOutput::ComputeCoordinateSystems()
    {
      const size_t result_size( m_TrainingData->GetResultSize());
      const size_t train_size( m_TrainingData->GetSize());

      BCL_Assert( m_Kappa < train_size, "Kappa is not smaller than Training set");
      // If kappa is one or less the resulting coordianate systems are set to the identity matrix
      if( m_Kappa < 2)
      {
        linal::Matrix< float> identity( result_size, result_size, float( 0));
        for( size_t i( 0); i < result_size; ++i)
        {
          identity( i, i) = 1;
        }
        for( size_t i( 0); i < train_size; ++i)
        {
          m_CoordinateSystems.PushBack( identity);
          m_CoordinateSystemsTransposed.PushBack( identity);
        }
      }
      //else compute them from kappa neighbors
      else
      {
        linal::Matrix< float> neighbor_distance( result_size, m_Kappa, float( 0.0));

        // for each result vector
        for( size_t current_res_vector = 0; current_res_vector < train_size; ++current_res_vector)
        {
          //find the number of equal vectors
          size_t number_equals( 0);
          for( size_t i( 0); i < train_size; ++i)
          {
            if
            (
              math::EqualWithinTolerance
              (
                m_TrainingData->GetResults()( i),
                m_TrainingData->GetResults()( current_res_vector)
              )
            )
            {
              ++number_equals;
            }
          }

          // if all outputs are equal
          BCL_Assert( number_equals < train_size, "No different Outputs");
          size_t clipped_kappa( m_Kappa);
          if( m_Kappa + number_equals > train_size)
          {
            BCL_Message
            (
              util::Message::e_Standard,
              "On vector " + util::Format()( current_res_vector) +
              " are less than kappa different vectors"
            );
            // clipp Kappa to half the different vectors, at least one
            clipped_kappa = ( train_size - number_equals) / 2;
          }

          // get the Kappa nearest neighbors
          storage::Vector< storage::Pair< float, size_t> > neighbors
          (
            KappaNearestNeighbor::FindWithoutRescaling
            (
              m_TrainingData->GetResults()( current_res_vector),
              clipped_kappa + number_equals, m_TrainingData->GetResults()
            )
          );

          //get average
          linal::Vector< float> average( result_size, float( 0.0));

          for( size_t j( 0); j < clipped_kappa; ++j)
          {
            average += linal::Vector< float>
            (
              result_size,
              m_TrainingData->GetResults().GetMatrix()[ neighbors( j + number_equals).Second()]
            ) / float( clipped_kappa);
          }

          // fill matrix with difference to the average
          for( size_t i( 0); i < result_size; ++i)
          {
            for( size_t j( 0); j < clipped_kappa; ++j)
            {
              neighbor_distance( i, j) = m_TrainingData->GetResults()( neighbors( j + number_equals).Second())( i) - average( i);
            }
          }
          //do singular value decomposition
          //this equals getting the eigenvectors of the covariance matrix
          // We need the left-singular vectors as a square matrix. That's equal to compute the right-singular vectors of the transposed matrix
          linal::Matrix< float> u( 1, 1), v, vt( 1, 1);
          linal::SingularValueDecomposition( neighbor_distance.Transposed(), vt, u);
          v = vt.Transposed();
          //copy V into m_CoordinateSystems
          m_CoordinateSystems.PushBack( v);

          //copy transposed matrix into m_CoordinateSystemsTransposed
          m_CoordinateSystemsTransposed.PushBack( vt);
        }
      }
      BCL_Message( util::Message::e_Debug, "CoordSys:" + util::Format()( m_CoordinateSystems));
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_collect_features_above.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> CollectFeaturesAbove::s_Instance
    (
      util::Enumerated< CollectFeaturesInterface>::AddInstance( new CollectFeaturesAbove( 0.0))
    );

    //! @brief constructor from parameter
    //! @param THRESHOLD threshold above which feature values will be kept
    CollectFeaturesAbove::CollectFeaturesAbove( const float &THRESHOLD) :
      m_Threshold( THRESHOLD)
    {
    }

    //! @brief Clone function
    //! @return pointer to new CollectFeaturesAbove
    CollectFeaturesAbove *CollectFeaturesAbove::Clone() const
    {
      return new CollectFeaturesAbove( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &CollectFeaturesAbove::GetClassIdentifier() const
    {
      return GetStaticClassName< CollectFeaturesAbove>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &CollectFeaturesAbove::GetAlias() const
    {
      static const std::string s_Name( "Above");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief collect the top m_NumberToKeep value indices by score
    //! @param SCORES any values
    //! @return top m_NumberToKeep value indices
    storage::Vector< size_t> CollectFeaturesAbove::Collect( const linal::Vector< float> &SCORES) const
    {
      // find the indices of values above the given threshold
      storage::Vector< size_t> to_keep;
      to_keep.AllocateMemory( SCORES.GetSize());
      for( size_t i( 0), i_max( SCORES.GetSize()); i < i_max; ++i)
      {
        if( SCORES( i) > m_Threshold)
        {
          to_keep.PushBack( i);
        }
      }
      return to_keep;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer CollectFeaturesAbove::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription( "Selects the indices of the values above a threshold");

      member_data.AddInitializer
      (
        "",
        "threshold value",
        io::Serialization::GetAgent( &m_Threshold),
        "0.0"
      );

      return member_data;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_collect_features_top.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> CollectFeaturesTop::s_Instance
    (
      util::Enumerated< CollectFeaturesInterface>::AddInstance( new CollectFeaturesTop( util::GetUndefined< size_t>()))
    );

    //! @brief constructor from parameter
    //! @param NUMBER_TO_KEEP number of features to keep
    CollectFeaturesTop::CollectFeaturesTop( const size_t &NUMBER_TO_KEEP) :
      m_NumberToKeep( NUMBER_TO_KEEP)
    {
    }

    //! @brief Clone function
    //! @return pointer to new CollectFeaturesTop
    CollectFeaturesTop *CollectFeaturesTop::Clone() const
    {
      return new CollectFeaturesTop( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &CollectFeaturesTop::GetClassIdentifier() const
    {
      return GetStaticClassName< CollectFeaturesTop>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &CollectFeaturesTop::GetAlias() const
    {
      static const std::string s_Name( "Top");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief collect the top m_NumberToKeep value indices by score
    //! @param SCORES any values
    //! @return top m_NumberToKeep value indices
    storage::Vector< size_t> CollectFeaturesTop::Collect( const linal::Vector< float> &SCORES) const
    {
      // check if it is desired to keep more scores than were even given
      if( m_NumberToKeep >= SCORES.GetSize())
      {
        // keep all the scores
        storage::Vector< size_t> to_keep( SCORES.GetSize());
        for( size_t i( 0), i_max( to_keep.GetSize()); i < i_max; ++i)
        {
          to_keep( i) = i;
        }
        return to_keep;
      }

      // initialize a vector to hold the scores and their indices
      storage::Vector< storage::Pair< float, size_t> > score_and_index;

      // allocate enough memory to hold all the columns
      score_and_index.AllocateMemory( SCORES.GetSize());

      // add all the scores and column indices to the vector
      for( size_t feature_index( 0), feature_size( SCORES.GetSize()); feature_index < feature_size; ++feature_index)
      {
        score_and_index.PushBack( storage::Pair< float, size_t>( SCORES( feature_index), feature_index));
      }

      // sort the vector by scores
      score_and_index.Sort( std::less< storage::Pair< float, size_t> >());

      // make a vector that will hold the top m_NumberToKeep indices
      storage::Vector< size_t> chosen_scores;

      // allocate enough memory to hold the columns that will be added
      chosen_scores.AllocateMemory( m_NumberToKeep);

      size_t best_score_index( 0);
      // add the best (e.g. the last) N_HIGHEST columns to the vector
      for
      (
        storage::Vector< storage::Pair< float, size_t> >::const_reverse_iterator
          rev_itr_score( score_and_index.ReverseBegin()),
          rev_itr_score_end( score_and_index.ReverseEnd());
        best_score_index < m_NumberToKeep;
        ++rev_itr_score, ++best_score_index
      )
      {
        chosen_scores.PushBack( rev_itr_score->Second());
      }

      // sort the vector of scores by index
      chosen_scores.Sort( std::less< size_t>());

      return chosen_scores;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer CollectFeaturesTop::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription( "Selects the indices of the highest N values");

      member_data.AddInitializer
      (
        "",
        "number of highest valued indices to collect",
        io::Serialization::GetAgentWithMin( &m_NumberToKeep, size_t( 1)),
        "1"
      );

      return member_data;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_cross_validation_info.h"

// includes from bcl - sorted alphabetically
#include "descriptor/bcl_descriptor_dataset.h"
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_retrieve_data_set_from_delimited_file.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> CrossValidationInfo::s_Instance
    (
      GetObjectInstances().AddInstance( new CrossValidationInfo())
    );

    //! @brief Name for each prediction file format
    //! @param FORMAT the format to convert to a string
    //! @return string representing FORMAT
    const std::string &CrossValidationInfo::GetPredictionOutputFormatString( const PredictionOutputFormat &FORMAT)
    {
      static const std::string s_names[] =
      {
        "",
        "Matrix",
        "CSV",
        GetStaticClassName< PredictionOutputFormat>()
      };
      return s_names[ size_t( FORMAT)];
    }

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    CrossValidationInfo::CrossValidationInfo() :
      m_Result( util::GetUndefined< float>()),
      m_ImprovementType(),
      m_Independent(),
      m_Monitoring(),
      m_Training(),
      m_NumberOutputs( 0),
      m_PredictionOutputFormat( e_ToBeDetermined),
      m_Filename()
    {
    }

    //! @brief default constructor
    CrossValidationInfo::CrossValidationInfo( const std::string &FILENAME) :
      m_Result( util::GetUndefined< float>()),
      m_ImprovementType(),
      m_Independent(),
      m_Monitoring(),
      m_Training(),
      m_NumberOutputs( 0),
      m_PredictionOutputFormat( e_ToBeDetermined),
      m_Filename( FILENAME)
    {
      io::IFStream input;
      io::File::MustOpenIFStream( input, FILENAME);
      // read this object from an object data label
      io::Serialize::Read( *this, input);
      io::File::CloseClearFStream( input);

      // commonly, if the models are used later on and are moved somewhere, the predictions files are moved
      // into the same directory as the models
      if( !m_IndependentPredictions.empty() && !io::DirectoryEntry( m_IndependentPredictions).DoesExist())
      {
        const std::string basename( io::File::SplitToPathAndFileName( m_IndependentPredictions).Second());
        const std::string initial_path( io::File::SplitToPathAndFileName( m_Filename).First());
        if( io::DirectoryEntry( initial_path + "/" + basename).DoesExist())
        {
          BCL_MessageVrb
          (
            "Could not locate independent predictions at: " + m_IndependentPredictions
            + "; using file from " + initial_path + " with same name"
          );
          m_IndependentPredictions = initial_path + "/" + basename;
        }
      }
    }

    //! @brief constructor from members
    CrossValidationInfo::CrossValidationInfo
    (
      const float &RESULT,
      const opti::ImprovementTypeEnum &IMPROVEMENT_TYPE,
      const util::ObjectDataLabel &INDEPENDENT,
      const util::ObjectDataLabel &MONITORING,
      const util::ObjectDataLabel &TRAINING,
      const util::ObjectDataLabel &OBJECTIVE,
      const util::ObjectDataLabel &ITERATE,
      const std::string &INDEPENDENT_PREDICTIONS,
      const FeatureLabelSet &ID_LABELS,
      const size_t &NUMBER_OUTPUTS
    ) :
      m_Result( RESULT),
      m_ImprovementType( IMPROVEMENT_TYPE),
      m_Independent( INDEPENDENT),
      m_Monitoring( MONITORING),
      m_Training( TRAINING),
      m_Objective( OBJECTIVE),
      m_Iterate( ITERATE),
      m_IndependentPredictions( INDEPENDENT_PREDICTIONS),
      m_NumberOutputs( NUMBER_OUTPUTS),
      m_IdLabels( ID_LABELS.GetLabel()),
      m_CharsPerId( ID_LABELS.GetPropertySizes()),
      m_PredictionOutputFormat( e_Delimited) // all new files will be written out with delimited format
    {
      if( !INDEPENDENT_PREDICTIONS.empty())
      {
        m_IndependentPredictions = io::File::MakeAbsolutePath( m_IndependentPredictions);
      }
    }

    //! @brief Clone function
    //! @return pointer to new CrossValidationInfo
    CrossValidationInfo *CrossValidationInfo::Clone() const
    {
      return new CrossValidationInfo( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &CrossValidationInfo::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &CrossValidationInfo::GetAlias() const
    {
      static const std::string s_name( "CVMetadata");
      return s_name;
    }

    //! @brief get the id feature label set
    //! @return the id feature label set
    FeatureLabelSet CrossValidationInfo::GetIDsFeatureLabelSet() const
    {
      return FeatureLabelSet( "Combine", m_IdLabels.GetArguments(), m_CharsPerId);
    }

    //! @brief keep only the information in this cv-info that is held in common with OTHER
    //! @param OTHER the other cross-validation info object
    void CrossValidationInfo::KeepSharedInfo( const CrossValidationInfo &OTHER)
    {
      m_Result = m_Result == OTHER.m_Result ? m_Result : util::GetUndefined< float>();
      BCL_Assert( m_ImprovementType == OTHER.m_ImprovementType, "Cannot combine CV-ids with different improvement types");
      m_Independent = m_Independent == OTHER.m_Independent ? m_Independent : util::ObjectDataLabel();
      m_Monitoring = m_Monitoring == OTHER.m_Monitoring ? m_Monitoring : util::ObjectDataLabel();
      m_Training = m_Training == OTHER.m_Training ? m_Training : util::ObjectDataLabel();
      m_Objective = m_Objective == OTHER.m_Objective ? m_Objective : util::ObjectDataLabel();
      m_Iterate = m_Independent == OTHER.m_Iterate ? m_Iterate : util::ObjectDataLabel();
      m_IndependentPredictions = m_IndependentPredictions == OTHER.m_IndependentPredictions ? m_IndependentPredictions : std::string();
      m_NumberOutputs = m_NumberOutputs == OTHER.m_NumberOutputs ? m_NumberOutputs : util::GetUndefined< size_t>();
      m_IdLabels = m_IdLabels == OTHER.m_IdLabels ? m_IdLabels : util::ObjectDataLabel();
      m_CharsPerId = m_CharsPerId == OTHER.m_CharsPerId ? m_CharsPerId : storage::Vector< size_t>();
      m_PredictionOutputFormat = m_PredictionOutputFormat == OTHER.m_PredictionOutputFormat ? m_PredictionOutputFormat.GetEnum() : e_Delimited;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer CrossValidationInfo::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Cross-validation related data for a model");
      parameters.AddInitializer
      (
        "independent",
        "independent dataset retriever",
        io::Serialization::GetAgent( &m_Independent)
      );
      parameters.AddInitializer
      (
        "monitoring",
        "monitoring dataset retriever",
        io::Serialization::GetAgent( &m_Monitoring)
      );
      parameters.AddInitializer
      (
        "training",
        "training dataset retriever",
        io::Serialization::GetAgent( &m_Training),
        ""
      );
      parameters.AddInitializer
      (
        "result",
        "Objective function performance on the independent dataset",
        io::Serialization::GetAgent( &m_Result)
      );
      parameters.AddInitializer
      (
        "improvement type",
        "Directionality of results e.g. LargerIsBetter indicates that larger result values are better",
        io::Serialization::GetAgent( &m_ImprovementType)
      );
      parameters.AddInitializer
      (
        "objective",
        "Final objective function that was trained on",
        io::Serialization::GetAgent( &m_Objective),
        ""
      );
      parameters.AddInitializer
      (
        "iterate",
        "Actual optimization method",
        io::Serialization::GetAgent( &m_Iterate),
        ""
      );
      // Formerly, GetAgentInputFilename was used for validation, but when models are moved around the independent predictions will
      // necessarily do the same so we perform a limited search for them in ReadInitializerSuccessHook if the given path doesn't exist
      parameters.AddOptionalInitializer
      (
        "independent predictions",
        "File containing the independent predictions for this training",
        io::Serialization::GetAgent( &m_IndependentPredictions)
      );
      parameters.AddInitializer
      (
        "number outputs",
        "# of outputs (0 if unknown)",
        io::Serialization::GetAgent( &m_NumberOutputs),
        "0"
      );
      parameters.AddInitializer
      (
        "ids",
        "ID labels specified for the training",
        io::Serialization::GetAgent( &m_IdLabels),
        ""
      );
      parameters.AddInitializer
      (
        "id sizes",
        "Sizes for each id label",
        io::Serialization::GetAgentWithSizeLimits( &m_CharsPerId, 0, 10000),
        ""
      );
      parameters.AddInitializer
      (
        "prediction output format",
        "Format for the prediction output file",
        io::Serialization::GetAgent( &m_PredictionOutputFormat),
        GetPredictionOutputFormatString( e_ToBeDetermined)
      );
      return parameters;
    }

    //! @brief hook that derived classes can override, called after TryRead successfully reads a serializer containing only initializer info (no data variables)
    //! @param SERIALIZER the serializer object with initialization information
    //! @param ERR_STREAM stream to write out errors to
    //! @return true, unless there were new errors
    bool CrossValidationInfo::ReadInitializerSuccessHook( const util::ObjectDataLabel &SERIALIZER, std::ostream &ERR_STREAM)
    {
      // commonly, if the models are used later on and are moved somewhere, the predictions files are moved
      // into the same directory as the models
      if( !m_IndependentPredictions.empty() && !io::DirectoryEntry( m_IndependentPredictions).DoesExist())
      {
        const std::string basename( io::File::SplitToPathAndFileName( m_IndependentPredictions).Second());
        const std::string initial_path( io::File::SplitToPathAndFileName( m_Filename).First());
        if( io::DirectoryEntry( initial_path + "/" + basename).DoesExist())
        {
          BCL_MessageVrb
          (
            "Could not locate independent predictions at: " + m_IndependentPredictions
            + "; using file from " + initial_path + " with same name"
          );
          m_IndependentPredictions = initial_path + "/" + basename;
        }
        else
        {
          ERR_STREAM << "Could not location predictions file at: "
                     << m_IndependentPredictions << " or " << initial_path + "/" + basename << '\n';
          return false;
        }
      }
      return true;
    }

    //! @brief read predictions from any file
    //! @param FILENAME the filename
    util::ShPtr< descriptor::Dataset> CrossValidationInfo::ReadPredictions( const std::string &FILENAME)
    {
      const std::string info_filename( FILENAME + ".info");
      CrossValidationInfo info;
      if( io::DirectoryEntry( info_filename).DoesExist())
      {
        // read the info file
        io::IFStream input;
        io::File::MustOpenIFStream( input, info_filename);
        input >> info;
        io::File::CloseClearFStream( input);
      }
      info.SetIndependentPredictionsFilename( FILENAME);
      return info.ReadIndependentPredictions();
    }

    //! @brief read the independent dataset predictions into a dataset object, whose features are the predicted values
    //!        results are the same result values, and ids are the given ids
    util::ShPtr< descriptor::Dataset> CrossValidationInfo::ReadIndependentPredictions()
    {
      util::ShPtr< descriptor::Dataset> sp_dataset;
      io::DirectoryEntry directory_entry( m_IndependentPredictions);
      if
      (
        m_IndependentPredictions.empty()
        || !directory_entry.DoesExist()
        || directory_entry.IsType( io::Directory::e_Dir)
      )
      {
        // return if no output location was given
        return sp_dataset;
      }
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_IndependentPredictions);
      if( m_PredictionOutputFormat == e_ToBeDetermined)
      {
        // determine the input format
        std::string first_line;
        std::getline( input, first_line);
        if( util::TrimString( first_line) == linal::Matrix< float>().GetClassIdentifier())
        {
          m_PredictionOutputFormat = e_Matrix;
        }
        else
        {
          m_PredictionOutputFormat = e_Delimited;
          const size_t number_commas( std::count( first_line.begin(), first_line.end(), ','));
          BCL_Assert( number_commas, "Bad prediction file!");
          if( number_commas % 2)
          {
            m_IdLabels = "Combine";
            m_CharsPerId.Reset();
            m_NumberOutputs = ( number_commas + 1) / 2;
          }
          else
          {
            m_NumberOutputs = number_commas / 2;
            m_IdLabels = "Combine(UnknownIdChar)";
            m_CharsPerId.Reset();
            m_CharsPerId.PushBack( first_line.find( ','));
          }
        }

        // close and reopen file
        io::File::CloseClearFStream( input);
        io::File::MustOpenIFStream( input, m_IndependentPredictions);
      }

      if( m_PredictionOutputFormat == e_Matrix)
      {
        // matrix format
        linal::Matrix< float> experimental_predictions;
        input >> experimental_predictions;
        const size_t feature_size( experimental_predictions.GetNumberCols() / 2);
        const size_t n_training_examples( experimental_predictions.GetNumberRows());
        sp_dataset =
          util::ShPtr< descriptor::Dataset>
          (
            new descriptor::Dataset( n_training_examples, feature_size, feature_size, size_t( 0))
          );
        linal::MatrixReference< float> predictions_ref( sp_dataset->GetFeaturesReference());
        linal::MatrixReference< float> experimental_ref( sp_dataset->GetResultsReference());
        for( size_t row_id( 0); row_id < n_training_examples; ++row_id)
        {
          linal::VectorConstReference< float> ep( experimental_predictions.GetRow( row_id));
          experimental_ref.GetRow( row_id).CopyValues( ep.Reference( 0, feature_size));
          predictions_ref.GetRow( row_id).CopyValues( ep.Reference( feature_size, feature_size));
        }
      }
      else
      {
        // retrieve as a delimited file
        RetrieveDataSetFromDelimitedFile retriever
        (
          m_IndependentPredictions,
          m_NumberOutputs,
          GetIDsFeatureLabelSet().GetSize()
        );
        sp_dataset = retriever.GenerateDataSet();
        sp_dataset->GetIds().SetFeatureLabelSet( GetIDsFeatureLabelSet());
      }
      io::File::CloseClearFStream( input);
      return sp_dataset;
    }

    //! @brief write independent predictions using the current format, to the independent prediction file
    //! @param EXPERIMENTAL matrix with experimental data; non-const to allow descaling
    //! @param PREDICTIONS predicted results; non-const to allow descaling
    //! @param IDS ids matrix
    void CrossValidationInfo::WritePredictions
    (
      FeatureDataSet< float> &EXPERIMENTAL,
      FeatureDataSet< float> &PREDICTIONS,
      const FeatureDataSetInterface< char> &IDS,
      const std::string &FILENAME,
      const PredictionOutputFormat &FORMAT
    )
    {
      // descale predictions and results, just in case they are still scaled
      EXPERIMENTAL.DeScale();
      PREDICTIONS.DeScale();
      CrossValidationInfo::WritePredictions
      (
        EXPERIMENTAL.GetMatrix(),
        PREDICTIONS.GetMatrix(),
        IDS.GetMatrix(),
        FILENAME,
        FORMAT
      );
    }

    //! @brief write independent predictions using the current format, to the independent prediction file
    //! @param EXPERIMENTAL matrix with experimental data; non-const to allow descaling
    //! @param PREDICTIONS predicted results; non-const to allow descaling
    //! @param IDS ids matrix
    //! @param FILENAME file to write the predictions out to
    //! @param FORMAT the desired format to write the predictions in
    void CrossValidationInfo::WritePredictions
    (
      const linal::MatrixConstInterface< float> &EXPERIMENTAL,
      const linal::MatrixConstInterface< float> &PREDICTIONS,
      const linal::MatrixConstInterface< char> &IDS,
      const std::string &FILENAME,
      const PredictionOutputFormat &FORMAT
    )
    {
      // write experimental and predicted data
      io::OFStream output;
      io::File::MustOpenOFStream( output, FILENAME);

      if( FORMAT == e_Matrix)
      {
        // create matrix with twice the number of cols of the result vector
        linal::Matrix< float> matrix( EXPERIMENTAL.GetNumberRows(), size_t( 2) * PREDICTIONS.GetNumberCols(), float( 0.0));

        // off set to access indices of matrix for result values
        const size_t offset( PREDICTIONS.GetNumberCols());

        for( size_t data_id( 0), data_set_size( matrix.GetNumberRows()); data_id < data_set_size; ++data_id)
        {
          for( size_t row_element_id( 0), row_size( offset); row_element_id < row_size; ++row_element_id)
          {
            // get the actual result
            matrix[ data_id][ row_element_id] = EXPERIMENTAL[ data_id][ row_element_id];
            matrix[ data_id][ row_element_id + offset] = PREDICTIONS[ data_id][ row_element_id];
          }
        }
        output << matrix;
      }
      else // if( FORMAT == e_Delimited || FORMAT == e_ToBeDetermined)
      {
        const size_t id_size( IDS.GetNumberCols()), feature_size( PREDICTIONS.GetNumberCols());
        std::string rowid( id_size, ' ');
        const char delimiter( ',');
        const size_t feature_size_m1( feature_size - 1);
        for( size_t data_id( 0), data_set_size( PREDICTIONS.GetNumberRows()); data_id < data_set_size; ++data_id)
        {
          if( id_size)
          {
            // write id for this row
            linal::VectorConstReference< char> ids_row( IDS.GetRow( data_id));
            std::copy( ids_row.Begin(), ids_row.End(), rowid.begin());
            std::replace( rowid.begin(), rowid.end(), '\0', ' ');
            output << rowid << delimiter;
          }
          linal::VectorConstReference< float> prediction( PREDICTIONS.GetRow( data_id));
          linal::VectorConstReference< float> results( EXPERIMENTAL.GetRow( data_id));
          for( size_t row_element_id( 0); row_element_id < feature_size; ++row_element_id)
          {
            output << prediction( row_element_id) << delimiter;
          }
          for( size_t row_element_id( 0); row_element_id < feature_size_m1; ++row_element_id)
          {
            output << results( row_element_id) << delimiter;
          }
          output << results( feature_size_m1);
          output << '\n';
        }
      }

      io::File::CloseClearFStream( output);
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_log.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DataSetLog::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetLog())
    );

    //! @brief default constructor
    DataSetLog::DataSetLog() :
        m_Label()
    {
    }

    //! @brief constructor from the rmsd
    //! @param RMSD the rmsd to use in the clustering algorithm
    //! @param RANGE the min and max # of clusters that would be acceptable
    //! @param MAX_STEPS_TO_REACH_CLUSTER_SIZE the maximum # of attempts to change the rmsd to get the # of clusters into the range
    //! @param AUTOSCALE true if the data need to be normalized before clustering
    DataSetLog::DataSetLog
    (
      const std::string &ID_LABEL
    ) :
      m_Label( ID_LABEL)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DataSetLog
    DataSetLog *DataSetLog::Clone() const
    {
      return new DataSetLog( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetLog::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetLog>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetLog::GetAlias() const
    {
      static const std::string s_Name( "Log");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void DataSetLog::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void DataSetLog::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetLog::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetLog::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetLog::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetLog::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetLog::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      DataSetLog::GenerateDataSet()
    {
      // get the base data set and use the clustering algorithm on it
      util::ShPtr< descriptor::Dataset> changeable( m_Retriever->GenerateDataSet());
      this->operator()( changeable);
      return changeable;
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    void DataSetLog::operator ()( util::ShPtr< descriptor::Dataset> DATA) const
    {
      linal::MatrixReference< float> feature_matrix( DATA->GetFeatures().GetMatrix());
      linal::MatrixReference< float> result_matrix( DATA->GetResults().GetMatrix());
      for( linal::MatrixReference< float>::iterator itr_m( feature_matrix.Begin()), itr_m_end( feature_matrix.End());
          itr_m != itr_m_end;
          ++itr_m
         )
      {
        *itr_m = ( *itr_m < 0 ? -1.0 : 1.0) * std::log( std::fabs( *itr_m));
      }

      if( m_Label.empty())
      {
        return;
      }

      linal::MatrixReference< char> id_mat( DATA->GetIds().GetMatrix());
      BCL_Assert
      (
        id_mat.GetNumberCols() >= m_Label.length(),
        "Specified label \"" + m_Label + "\" is too long for the given dataset (no fewer than "
          + util::Format()( id_mat.GetNumberCols()) + " characters allowed)"
      );

      std::string padded_label( m_Label);
      padded_label += std::string( id_mat.GetNumberCols() - m_Label.length(), ' ');
      for( size_t r( 0), end_r( id_mat.GetNumberRows()); r < end_r; ++r)
      {
        std::copy( padded_label.begin(), padded_label.end(), id_mat[ r]);
      }
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    util::ShPtr< descriptor::Dataset> DataSetLog::operator()( const descriptor::Dataset &DATA) const
    {
      util::ShPtr< descriptor::Dataset> changeable( DATA.HardCopy());
      this->operator()( changeable);
      return changeable;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetLog::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Multiplies the feature and result values of a dataset by a fixed value"
      );
      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );

      member_data.AddOptionalInitializer
      (
        "id label",
        "the id label to add to multiplied data points",
        io::Serialization::GetAgent( &m_Label)
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetLog::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_multiplied.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DataSetMultiplied::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetMultiplied())
    );

    //! @brief default constructor
    DataSetMultiplied::DataSetMultiplied() :
        m_FeatureMultiplier( 1.0),
        m_ResultMultiplier( 1.0),
        m_Label()
    {
    }

    //! @brief constructor from the rmsd
    //! @param RMSD the rmsd to use in the clustering algorithm
    //! @param RANGE the min and max # of clusters that would be acceptable
    //! @param MAX_STEPS_TO_REACH_CLUSTER_SIZE the maximum # of attempts to change the rmsd to get the # of clusters into the range
    //! @param AUTOSCALE true if the data need to be normalized before clustering
    DataSetMultiplied::DataSetMultiplied
    (
      const double &FEATURE_MULTIPLIER,
      const double &RESULT_MULTIPLIER,
      const std::string &ID_LABEL
    ) :
      m_FeatureMultiplier( FEATURE_MULTIPLIER),
      m_ResultMultiplier( RESULT_MULTIPLIER),
      m_Label( ID_LABEL)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DataSetMultiplied
    DataSetMultiplied *DataSetMultiplied::Clone() const
    {
      return new DataSetMultiplied( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetMultiplied::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetMultiplied>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetMultiplied::GetAlias() const
    {
      static const std::string s_Name( "Multiply");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void DataSetMultiplied::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void DataSetMultiplied::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetMultiplied::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetMultiplied::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetMultiplied::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetMultiplied::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetMultiplied::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      DataSetMultiplied::GenerateDataSet()
    {
      // get the base data set and use the clustering algorithm on it
      util::ShPtr< descriptor::Dataset> changeable( m_Retriever->GenerateDataSet());
      this->operator()( changeable);
      return changeable;
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    void DataSetMultiplied::operator ()( util::ShPtr< descriptor::Dataset> DATA) const
    {
      linal::MatrixReference< float> feature_matrix( DATA->GetFeatures().GetMatrix());
      linal::MatrixReference< float> result_matrix( DATA->GetResults().GetMatrix());
      feature_matrix *= m_FeatureMultiplier;
      result_matrix *= m_ResultMultiplier;

      if( m_Label.empty())
      {
        return;
      }

      linal::MatrixReference< char> id_mat( DATA->GetIds().GetMatrix());
      BCL_Assert
      (
        id_mat.GetNumberCols() >= m_Label.length(),
        "Specified label \"" + m_Label + "\" is too long for the given dataset (no fewer than "
          + util::Format()( id_mat.GetNumberCols()) + " characters allowed)"
      );

      std::string padded_label( m_Label);
      padded_label += std::string( id_mat.GetNumberCols() - m_Label.length(), ' ');
      for( size_t r( 0), end_r( id_mat.GetNumberRows()); r < end_r; ++r)
      {
        std::copy( padded_label.begin(), padded_label.end(), id_mat[ r]);
      }
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    util::ShPtr< descriptor::Dataset> DataSetMultiplied::operator()( const descriptor::Dataset &DATA) const
    {
      util::ShPtr< descriptor::Dataset> changeable( DATA.HardCopy());
      this->operator()( changeable);
      return changeable;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetMultiplied::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Multiplies the feature and result values of a dataset by a fixed value"
      );
      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );

      member_data.AddInitializer
      (
        "feature multiplier",
        "the value to multiply feature values by",
        io::Serialization::GetAgent( &m_FeatureMultiplier)
      );

      member_data.AddInitializer
      (
        "result multiplier",
        "the value to multiply result values by",
        io::Serialization::GetAgent( &m_ResultMultiplier)
      );

      member_data.AddOptionalInitializer
      (
        "id label",
        "the id label to add to multiplied data points",
        io::Serialization::GetAgent( &m_Label)
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetMultiplied::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_reduced_to_cluster_centers.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DataSetReducedToClusterCenters::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetReducedToClusterCenters())
    );

    //! @brief get the name associated with a particular selection
    //! @param SELECTION the selection type of interest
    //! @return the name associated with that selection type
    const std::string &DataSetReducedToClusterCenters::GetSelectionName( const Selection &SELECTION)
    {
      static const std::string s_names[] =
      {
        "First",
        "Center",
        "Max",
        "Min",
        GetStaticClassName< Selection>()
      };
      return s_names[ SELECTION];
    }

    //! @brief constructor from the rmsd
    //! @param RMSD the rmsd to use in the clustering algorithm
    //! @param RANGE the min and max # of clusters that would be acceptable
    //! @param MAX_STEPS_TO_REACH_CLUSTER_SIZE the maximum # of attempts to change the rmsd to get the # of clusters into the range
    //! @param AUTOSCALE true if the data need to be normalized before clustering
    DataSetReducedToClusterCenters::DataSetReducedToClusterCenters
    (
      const double &RMSD,
      const math::Range< size_t> &RANGE,
      const size_t &MAX_STEPS_TO_REACH_CLUSTER_SIZE,
      const bool &AUTOSCALE,
      const Selection &SELECTION
    ) :
      m_RMSD( RMSD),
      m_ClusterSizeRange( RANGE),
      m_MaxSteps( MAX_STEPS_TO_REACH_CLUSTER_SIZE),
      m_Autoscale( AUTOSCALE),
      m_Selection( SELECTION)
    {
      BCL_Assert( m_RMSD >= 0.0 && m_RMSD <= 1.0, "RMSD must be between 0.0 and 1.0");
    }

    //! @brief Clone function
    //! @return pointer to new DataSetReducedToClusterCenters
    DataSetReducedToClusterCenters *DataSetReducedToClusterCenters::Clone() const
    {
      return new DataSetReducedToClusterCenters( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetReducedToClusterCenters::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetReducedToClusterCenters>();
    }

    //! @brief return the rmsd parameter
    //! @return the rmsd parameter
    double DataSetReducedToClusterCenters::GetRMSDParameter() const
    {
      return m_RMSD;
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetReducedToClusterCenters::GetAlias() const
    {
      static const std::string s_Name( "Downsample");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void DataSetReducedToClusterCenters::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void DataSetReducedToClusterCenters::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetReducedToClusterCenters::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetReducedToClusterCenters::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToClusterCenters::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToClusterCenters::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToClusterCenters::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      DataSetReducedToClusterCenters::GenerateDataSet()
    {
      // get the base data set and use the clustering algorithm on it
      util::ShPtr< descriptor::Dataset> changeable( m_Retriever->GenerateDataSet());
      this->operator()( changeable);
      return changeable;
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    void DataSetReducedToClusterCenters::operator ()( util::ShPtr< descriptor::Dataset> DATA) const
    {
      // make references to the matrices
      const linal::MatrixConstReference< float> features( DATA->GetFeaturesReference());

      // cache some commonly needed numbers
      const size_t data_set_size( DATA->GetSize());

      // make objects to rescale the input/output, and rescaled data, if desired
      util::ShPtr< linal::Matrix< float> > sp_feature_matrix;

      // simple pointer to the feature matrix that will be used; this will be changed to the autoscaled matrix
      // if that matrix is selected
      util::SiPtr< const linal::MatrixConstInterface< float> > si_ptr_feature_matrix( features);

      if( m_Autoscale)
      {
        // make objects to rescale the input/output, and rescaled data, if desired
        RescaleFeatureDataSet rescale_features( DATA->GetFeaturesReference(), math::Range< float>( 0.0, 1.0));

        // clone the features
        sp_feature_matrix = util::ShPtr< linal::Matrix< float> >( new linal::Matrix< float>( features));
        si_ptr_feature_matrix = sp_feature_matrix;
        rescale_features.RescaleMatrix( *sp_feature_matrix);
      }

      // determine the actual min and max # of clusters, which can be no larger than the number of feature results
      const size_t min_number_clusters( std::min( data_set_size, m_ClusterSizeRange.GetMin()));
      const size_t max_number_clusters( std::min( data_set_size, m_ClusterSizeRange.GetMax()));

      // make an object to hold the representative feature indices
      storage::Vector< size_t> representatives;

      double min_rmsd( 0.0), max_rmsd( 2.0 * m_RMSD);

      // keep track of how many steps have been performed so far
      size_t number_steps( 0);

      // track cluster ids
      storage::Vector< size_t> cluster_ids;

      // determine the max rmsd by doubling max_rmsd until the number of clusters is below the maximum number
      while( number_steps < m_MaxSteps)
      {
        representatives = GetRepresentativeFeatures( *si_ptr_feature_matrix, max_rmsd, cluster_ids);
        if( representatives.GetSize() > max_number_clusters)
        {
          min_rmsd = max_rmsd;
          max_rmsd *= 2.0;
        }
        else
        {
          // the max rmsd is large enough, so stop
          break;
        }
        ++number_steps;
      }

      // now perform a binary search for the right rmsd
      for( ; number_steps < m_MaxSteps; ++number_steps)
      {
        const double rmsd_guess( ( min_rmsd + max_rmsd) / 2.0);
        representatives = GetRepresentativeFeatures( *si_ptr_feature_matrix, rmsd_guess, cluster_ids);

        if( representatives.GetSize() > max_number_clusters)
        {
          min_rmsd = rmsd_guess * ( 1.0 + std::numeric_limits< double>::epsilon());
        }
        else if( representatives.GetSize() < min_number_clusters)
        {
          max_rmsd = rmsd_guess / ( 1.0 + std::numeric_limits< double>::epsilon());
        }
        else
        {
          break;
        }
      }

      if( m_Selection == e_Center)
      {
        representatives = GetClusterCenters( cluster_ids, *si_ptr_feature_matrix);
      }
      else if( m_Selection == e_Max)
      {
        storage::Vector< linal::VectorConstReference< float> > cluster_maxes( representatives.GetSize());
        const linal::MatrixReference< float> ref_results( DATA->GetResultsReference());
        for( size_t i( 0), n_rows( ref_results.GetNumberRows()); i < n_rows; ++i)
        {
          // account for id offset of 1
          const size_t effective_id( cluster_ids( i) - 1);
          if
          (
            cluster_maxes( effective_id).GetSize() == size_t( 0)
            || cluster_maxes( effective_id) < ref_results.GetRow( i)
          )
          {
            cluster_maxes( effective_id) = ref_results.GetRow( i);
            representatives( effective_id) = i;
          }
        }
      }
      else if( m_Selection == e_Min)
      {
        storage::Vector< linal::VectorConstReference< float> > cluster_mins( representatives.GetSize());
        const linal::MatrixReference< float> ref_results( DATA->GetResultsReference());
        for( size_t i( 0), n_rows( ref_results.GetNumberRows()); i < n_rows; ++i)
        {
          const size_t effective_id( cluster_ids( i) - 1);
          if
          (
            cluster_mins( effective_id).GetSize() == size_t( 0)
            || ref_results.GetRow( i) < cluster_mins( effective_id)
          )
          {
            cluster_mins( effective_id) = ref_results.GetRow( i);
            representatives( effective_id) = i;
          }
        }
      }

      // select the desired features
      BCL_MessageStd( "Representatives: " + util::Format()( representatives));
      DATA->KeepRows( representatives);
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    util::ShPtr< descriptor::Dataset> DataSetReducedToClusterCenters::operator()( const descriptor::Dataset &DATA) const
    {
      util::ShPtr< descriptor::Dataset> changeable( DATA.HardCopy());
      this->operator()( changeable);
      return changeable;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief get a set of features that cover the same space as FEATURES at resolution RMSD
    //! @param FEATURES the set of features to consider
    //! @param RMSD the rmsd; vectors closer than this rmsd will be considered representative of each other
    //! @param CLUSTER_IDS vector that will store cluster ids (1-offset), used for refinement if m_ClusterIDs was set
    //! @return indices of features that cover the same space as FEATURES at resolution RMSD
    storage::Vector< size_t> DataSetReducedToClusterCenters::GetRepresentativeFeatures
    (
      const linal::MatrixConstInterface< float> &FEATURES,
      const double &RMSD,
      storage::Vector< size_t> &CLUSTER_IDS
    ) const
    {
      const size_t data_set_size( FEATURES.GetNumberRows());
      const size_t feature_size( FEATURES.GetNumberCols());

      // given the rmsd cutoff, we can get a minor speed improvement by computing the maximum square deviation
      // between the data set feature vectors, because then there is no need to take a square root or divide by the number
      // of features in the end
      const double mean_square_deviation( RMSD * RMSD);
      const double max_square_deviation( mean_square_deviation * feature_size);

      // keep track of how many points have not yet been clustered
      size_t unclustered_points( data_set_size);

      BCL_MessageDbg( "rmsd: " + util::Format()( max_square_deviation));

      // make a vector that holds 0 if a datum is already used in a cluster
      CLUSTER_IDS = storage::Vector< size_t>( data_set_size, size_t( 0));

      // make an object to hold the clusters that are found
      storage::Vector< size_t> cluster_centers;

      // allocate enough memory so that the vector never needs reallocation
      cluster_centers.AllocateMemory( data_set_size);
      for( size_t index( 0); index < data_set_size; ++index)
      {
        // ignore datums that are already in the cluster
        if( CLUSTER_IDS( index))
        {
          continue;
        }

        // add this data point to the set of clusters
        cluster_centers.PushBack( index);

        // if there are already too many clusters, stop
        if( cluster_centers.GetSize() > m_ClusterSizeRange.GetMax())
        {
          break;
        }
        CLUSTER_IDS( index) = cluster_centers.GetSize();
        --unclustered_points;

        // get a reference on the feature vector of the current data set vector, which defines the center of the cluster
        const float *cluster_vector_begin( FEATURES[ index]);

        // get the end of the current data set vector
        const float *cluster_vector_end( cluster_vector_begin + feature_size);

        // look at all other data that have not yet been placed in clusters, set their datum in cluster to 1 if
        // they are within the rmsd of this datum
        for( size_t index_b( index + 1); index_b < data_set_size; ++index_b)
        {
          // ignore datums that are already in the cluster
          if( CLUSTER_IDS( index_b))
          {
            continue;
          }

          // initialize a sum of the square deviation of the feature at index_b from the cluster vector
          double square_deviation( 0.0);

          // compute the square deviation, but stop if we get over the cutoff
          // since there will typically be many clusters, we usually will exceed the cutoff very quickly, so
          // short-circuiting is important here
          for
          (
            const float *itr_cluster_vec( cluster_vector_begin), *itr_current_vec( FEATURES[ index_b]);
            square_deviation < max_square_deviation && itr_cluster_vec != cluster_vector_end;
            ++itr_cluster_vec, ++itr_current_vec
          )
          {
            square_deviation += math::Sqr( *itr_cluster_vec - *itr_current_vec);
          }

          BCL_MessageDbg( " square_deviation between " + util::Format()( index) + " " + util::Format()( index_b) + " = " + util::Format()( square_deviation));

          // mark the datum at index_b as being already in a cluster so that we don't use it to form a new cluster
          if( square_deviation < max_square_deviation)
          {
            CLUSTER_IDS( index_b) = cluster_centers.GetSize();
            --unclustered_points;
          }
        }
        BCL_MessageVrb
        (
          "Point: " + util::Format()( index)
          + " # cluster centers: " + util::Format()( cluster_centers.GetSize())
          + " # unclustered points: " + util::Format()( unclustered_points)
        );

        // if there are no longer enough clusters to satisfy the min, then stop
        if( cluster_centers.GetSize() + unclustered_points < m_ClusterSizeRange.GetMin())
        {
          break;
        }
      }

      BCL_MessageStd( " RMSD: " + util::Format()( RMSD) + " #clusters: " + util::Format()( cluster_centers.GetSize()));

      return cluster_centers;
    }

    //! @brief refine the id set to be the center-most element of each cluster
    //! @param CLUSTER_IDS vector that will store cluster ids (1-offset), used for refinement if m_ClusterIDs was set
    //! @param FEATURES set of features to consider
    //! @return the updated feature ids
    storage::Vector< size_t> DataSetReducedToClusterCenters::GetClusterCenters
    (
      storage::Vector< size_t> &CLUSTER_IDS,
      const linal::MatrixConstInterface< float> &FEATURES
    ) const
    {
      storage::Vector< size_t> cluster_centers;
      const size_t data_set_size( FEATURES.GetNumberRows());
      const size_t feature_size( FEATURES.GetNumberCols());

      for( size_t current_cluster_id( 1);; ++current_cluster_id)
      {
        // get all the rows in this cluster
        storage::Vector< size_t> current_cluster_ids;
        storage::Vector< linal::VectorConstReference< float> > cluster_members;
        for( size_t row( 0); row < data_set_size; ++row)
        {
          if( CLUSTER_IDS( row) == current_cluster_id)
          {
            current_cluster_ids.PushBack( row);
            cluster_members.PushBack( linal::VectorConstReference< float>( feature_size, FEATURES[ row]));
          }
        }

        const size_t number_cluster_members( current_cluster_ids.GetSize());
        if( number_cluster_members == 0)
        {
          break;
        }
        // track sums of msd's for each row
        linal::Vector< float> msd_sum( number_cluster_members, float( 0.0));
        size_t min_rmsd_id( 0);
        for( size_t row_a( 0); row_a < number_cluster_members; ++row_a)
        {
          for( size_t row_b( row_a + 1); row_b < number_cluster_members; ++row_b)
          {
            // initialize a sum of the square deviation of the feature at index_b from the cluster vector
            const double rmsd( linal::Distance( cluster_members( row_a), cluster_members( row_b)));

            // add the squared deviation to the msd sum for each cluster point
            msd_sum( row_a) += rmsd;
            msd_sum( row_b) += rmsd;
          }

          // track the lowest-rmsd instance
          if( msd_sum( row_a) < msd_sum( min_rmsd_id))
          {
            min_rmsd_id = row_a;
          }
        }

        // get the actual feature id
        cluster_centers.PushBack( current_cluster_ids( min_rmsd_id));
      }

      return cluster_centers;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetReducedToClusterCenters::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Downsample the dataset while maintaining near maximal coverage of feature space"
      );
      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );

      member_data.AddInitializer
      (
        "rmsd",
        "features less than this root mean squared distance apart are initially eligible to be part of the same cluster",
        io::Serialization::GetAgentWithRange( &m_RMSD, 0.0, 1.0)
      );

      member_data.AddInitializer
      (
        "acceptable cluster size range",
        "range of # of desired points in the resulting data set.\n"
        "RMSD will be adjusted to try to get a number of representative features in this range",
        io::Serialization::GetAgent( &m_ClusterSizeRange)
      );

      member_data.AddInitializer
      (
        "max steps",
        "maximum number of times to change rmsd to try to downsample the data set to within the acceptable size of the cluster range",
        io::Serialization::GetAgent( &m_MaxSteps),
        "1"
      );

      member_data.AddInitializer
      (
        "autoscale",
        "whether to normalize / denormalize the data set before / after clustering",
        io::Serialization::GetAgent( &m_Autoscale),
        "0"
      );

      member_data.AddInitializer
      (
        "select",
        "Use First for the fastest selection of cluster-center-member (the first found in the dataset). "
        "Use Center to the center-most feature of each cluster (making the algorithm order independent and generally more precise)"
        "note that this will result in much slower performance if the number of clusters is small"
        "Use Max (Min) to choose the feature with the Max (or Min) result.  If multiple results, the min/max of each column is selected",
        io::Serialization::GetAgent( &m_Selection),
        "First"
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetReducedToClusterCenters::GetNominalSize() const
    {
      return std::min( m_Retriever->GetNominalSize(), m_ClusterSizeRange.GetMax());
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_reduced_to_k_means.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average.h"
#include "model/bcl_model_training_schedule.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> DataSetReducedToKMeans::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetReducedToKMeans())
    );

    //! @brief default constructor
    //! @param NUMBER_CLUSTERS the desired number of clusters
    //! @param MAX_RECLUSTER_ATTEMPTS the maximum number of times to try improving the clusters
    DataSetReducedToKMeans::DataSetReducedToKMeans
    (
      const size_t &NUMBER_CLUSTERS,
      const size_t &MAX_RECLUSTER_ATTEMPTS,
      const bool &AUTOSCALE
    ) :
      m_NumberClusters( NUMBER_CLUSTERS),
      m_MaxReclusterAttempts( MAX_RECLUSTER_ATTEMPTS),
      m_Autoscale( AUTOSCALE)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DataSetReducedToKMeans
    DataSetReducedToKMeans *DataSetReducedToKMeans::Clone() const
    {
      return new DataSetReducedToKMeans( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetReducedToKMeans::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetReducedToKMeans>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetReducedToKMeans::GetAlias() const
    {
      static const std::string s_alias( "KMeans");
      return s_alias;
    }

    //! @brief return the maximum number of clusters that will be returned
    //! @return the maximum number of clusters that will be returned
    //! the actual number of clusters will be no larger than the size of the data set
    size_t DataSetReducedToKMeans::GetNumberClusters() const
    {
      return m_NumberClusters;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void DataSetReducedToKMeans::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void DataSetReducedToKMeans::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetReducedToKMeans::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToKMeans::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToKMeans::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToKMeans::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetReducedToKMeans::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief compute the square of the distance between two vectors, so long as it is no greater than LIMIT
    //! @param VEC_A, VEC_B the two vectors
    //! @param LIMIT the maximum distance of interest
    //! @return the distance, so long as it is less than LIMIT, otherwise, a value larger than LIMIT
    float DataSetReducedToKMeans::LimitedSquareDistance
    (
      const linal::VectorConstInterface< float> &VEC_A,
      const linal::VectorConstInterface< float> &VEC_B,
      const float &LIMIT
    )
    {
      BCL_Assert( VEC_A.GetSize() == VEC_B.GetSize(), "Vectors must be of the same size to compute the distance");

      // get the beginning of each vector
      const float *vec_a_begin( VEC_A.Begin()), *vec_b_begin( VEC_B.Begin());

      // compute the distance, but stop if limit is reached
      float distance_squared( 0.0);
      for( size_t index( 0), size( VEC_A.GetSize()); distance_squared < LIMIT && index < size; ++index)
      {
        distance_squared += math::Sqr( vec_a_begin[ index] - vec_b_begin[ index]);
      }

      return distance_squared;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      DataSetReducedToKMeans::GenerateDataSet()
    {
      return operator ()( m_Retriever->GenerateDataSet());
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    util::ShPtr< descriptor::Dataset>
      DataSetReducedToKMeans::operator ()
      (
        const util::ShPtr< descriptor::Dataset> &DATA,
        const TrainingSchedule &BALANCING
      ) const
    {
      // make references to the feature and result matrices
      const linal::MatrixConstReference< float> features( DATA->GetFeaturesReference());
      const linal::MatrixConstReference< float> results( DATA->GetResultsReference());

      // cache some commonly needed numbers
      const size_t data_set_size( features.GetNumberRows());
      const size_t feature_size( features.GetNumberCols());
      const size_t result_size( results.GetNumberCols());

      // there are fewer features than m_NumberClusters, then just return a copy of the data set
      if( data_set_size <= m_NumberClusters)
      {
        return DATA;
      }

      TrainingSchedule balancer( BALANCING);
      // initialize the balancer w/ dummy balancing if no balancing was specified
      if( !balancer.GetSize())
      {
        balancer.Setup( *DATA->GetResultsPtr(), 0.0);
      }

      // make objects to rescale the input/output, and rescaled data, if desired
      util::ShPtr< linal::Matrix< float> > sp_feature_matrix;

      // make objects to rescale the input/output, and rescaled data, if desired
      RescaleFeatureDataSet rescale_feature_result;

      // make an object to hold si-ptrs to all the feature-results
      storage::Vector< storage::VectorND< 2, linal::VectorConstReference< float> > > feature_results( data_set_size);

      // perform the scaling functions
      if( m_Autoscale)
      {
        sp_feature_matrix = util::CloneToShPtr( DATA->GetFeaturesPtr()->GetRawMatrix());

        linal::Matrix< float> &features_rescaled( *sp_feature_matrix);

        rescale_feature_result = RescaleFeatureDataSet( *sp_feature_matrix, math::Range< float>( 0, 1));
        rescale_feature_result.RescaleMatrix( features_rescaled);

        for( size_t index( 0); index < data_set_size; ++index)
        {
          feature_results( index) =
            storage::VectorND< 2, linal::VectorConstReference< float> >
            (
              linal::VectorConstReference< float>( feature_size, features_rescaled[ index]),
              linal::VectorConstReference< float>( result_size, results[ index])
            );
        }
      }
      else
      {
        // just add the feature-results to the si-ptr vector
        for( size_t index( 0); index < data_set_size; ++index)
        {
          feature_results( index) =
            storage::VectorND< 2, linal::VectorConstReference< float> >
            (
              linal::VectorConstReference< float>( feature_size, features[ index]),
              linal::VectorConstReference< float>( result_size, results[ index])
            );
        }
      }

      // initialize the cluster index vector, which stores the cluster id of each feature
      storage::Vector< size_t> cluster_indices( data_set_size, util::GetUndefined< size_t>());

      // initialize the clusters
      storage::Vector< math::RunningAverage< linal::Vector< float> > > clusters
      (
        m_NumberClusters,
        math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( feature_size, 0.0))
      );

      // initialize the clusters with the first m_NumberClusters features
      for( size_t i( 0); i < m_NumberClusters; ++i)
      {
        clusters( i) += feature_results( i).First();

        // set the cluster indices up to the associated cluster, since we know that these are the closest values
        // this isn't necessary, but it speeds up the search for the closest node later on
        cluster_indices( i) = i;
      }

      // refine clusters for m_MaxReclusterAttempts or until they do not change, whichever comes first
      size_t clustering_attempt_number( 0);
      while
      (
        clustering_attempt_number < m_MaxReclusterAttempts
        && RefineClusters( feature_results, cluster_indices, clusters, balancer)
      )
      {
        ++clustering_attempt_number;
      }
      BCL_MessageVrb( "# reclusterings: " + util::Format()( clustering_attempt_number));

      // make an object to hold the clusters that are found
      // There is no sensical way to ID the clusters, so leave the ID columns empty
      util::ShPtr< descriptor::Dataset> sp_clusters;
      if
      (
        DATA->GetFeaturesPtr()->GetFeatureLabelSet().IsDefined()
        && DATA->GetResultsPtr()->GetFeatureLabelSet().IsDefined()
        && DATA->GetIdsPtr()->GetFeatureLabelSet().IsDefined()
      )
      {
        sp_clusters =
          util::ShPtr< descriptor::Dataset>
          (
            new descriptor::Dataset
            (
              m_NumberClusters,
              *DATA->GetFeaturesPtr()->GetFeatureLabelSet(),
              *DATA->GetResultsPtr()->GetFeatureLabelSet(),
              *DATA->GetIdsPtr()->GetFeatureLabelSet()
            )
          );
      }
      else
      {
        // initialize using just the basic sizes
        sp_clusters =
          util::ShPtr< descriptor::Dataset>
          (
            new descriptor::Dataset
            (
              m_NumberClusters,
              feature_size,
              result_size,
              DATA->GetIdSize()
            )
          );
      }
      linal::MatrixReference< float> clustered_features( sp_clusters->GetFeaturesReference());
      linal::MatrixReference< float> clustered_results( sp_clusters->GetResultsReference());

        // just get the results
      for( size_t itr( 0); itr < m_NumberClusters; ++itr)
      {
        clustered_features.ReplaceRow( itr, clusters( itr).GetAverage());
      }

      // descale if necessary
      if( m_Autoscale)
      {
        rescale_feature_result.DeScaleMatrix( clustered_features);
      }

      // reset the clusters so they can be used to average the results
      for( size_t itr( 0); itr < m_NumberClusters; ++itr)
      {
        clusters( itr) = math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( result_size, 0.0));
      }

      // now get the average results for each cluster
      for( size_t itr( 0); itr < data_set_size; ++itr)
      {
        clusters( cluster_indices( itr)) += feature_results( itr).Second();
      }

      // add the results to the clustered_features_results
      for( size_t i( 0); i < m_NumberClusters; ++i)
      {
        clustered_results.ReplaceRow( i, clusters( i).GetAverage());
      }

      // return a new data set initialized with the cluster centers
      return sp_clusters;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetReducedToKMeans::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Create a dataset using the k-means average of another dataset"
      );
      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );
      member_data.AddInitializer
      (
        "clusters",
        "number of clusters for k-means clustering",
        io::Serialization::GetAgentWithMin( &m_NumberClusters, size_t( 1))
      );

      member_data.AddInitializer
      (
        "max steps",
        "maximum number of times to refine the clusters before accepting the solution",
        io::Serialization::GetAgentWithMin( &m_MaxReclusterAttempts, size_t( 1)),
        "10"
      );

      member_data.AddInitializer
      (
        "autoscale",
        "whether to normalize / denormalize the data set before / after clustering",
        io::Serialization::GetAgent( &m_Autoscale),
        "0"
      );

      return member_data;
    }

    storage::Vector< size_t> DataSetReducedToKMeans::IdentifyClusterIndices( util::ShPtr< descriptor::Dataset> &REDUCED_MATRIX) const
    {
      // make references to the feature and result matrices
      const linal::MatrixConstReference< float> features( REDUCED_MATRIX->GetFeaturesReference());
      const linal::MatrixConstReference< float> results( REDUCED_MATRIX->GetResultsReference());

      // cache some commonly needed numbers
      const size_t data_set_size( features.GetNumberRows());
      const size_t feature_size( features.GetNumberCols());
      const size_t result_size( results.GetNumberCols());

      // initialize the cluster index vector, which stores the cluster id of each feature
      storage::Vector< size_t> cluster_indices( data_set_size, util::GetUndefined< size_t>());

      // initialize the clusters
      storage::Vector< math::RunningAverage< linal::Vector< float> > > clusters
      (
        m_NumberClusters,
        math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( feature_size, 0.0))
      );

      // make objects to rescale the input/output, and rescaled data, if desired
      util::ShPtr< linal::Matrix< float> > sp_feature_matrix;

      // make objects to rescale the input/output, and rescaled data, if desired
      RescaleFeatureDataSet rescale_feature_result;

      // make an object to hold si-ptrs to all the feature-results
      storage::Vector< storage::VectorND< 2, linal::VectorConstReference< float> > > feature_results( data_set_size);

      // perform the scaling functions
      if( m_Autoscale)
      {
        sp_feature_matrix = util::CloneToShPtr( REDUCED_MATRIX->GetFeaturesPtr()->GetRawMatrix());

        linal::Matrix< float> &features_rescaled( *sp_feature_matrix);

        rescale_feature_result = RescaleFeatureDataSet( *sp_feature_matrix, math::Range< float>( 0, 1));
        rescale_feature_result.RescaleMatrix( features_rescaled);

        for( size_t index( 0); index < data_set_size; ++index)
        {
          feature_results( index) =
            storage::VectorND< 2, linal::VectorConstReference< float> >
            (
              linal::VectorConstReference< float>( feature_size, features_rescaled[ index]),
              linal::VectorConstReference< float>( result_size, results[ index])
            );
        }
      }
      else
      {
        // just add the feature-results to the si-ptr vector
        for( size_t index( 0); index < data_set_size; ++index)
        {
          feature_results( index) =
            storage::VectorND< 2, linal::VectorConstReference< float> >
            (
              linal::VectorConstReference< float>( feature_size, features[ index]),
              linal::VectorConstReference< float>( result_size, results[ index])
            );
        }
      }

      // initialize the clusters with the first m_NumberClusters features
      for( size_t i( 0); i < m_NumberClusters; ++i)
      {
        clusters( i) += feature_results( i).First();

        // set the cluster indices up to the associated cluster, since we know that these are the closest values
        // this isn't necessary, but it speeds up the search for the closest node later on
        cluster_indices( i) = i;
      }

      // refine clusters for m_MaxReclusterAttempts or until they do not change, whichever comes first
      size_t clustering_attempt_number( 0);
      while
        (
            clustering_attempt_number < m_MaxReclusterAttempts
            && RefineClusters( feature_results, cluster_indices, clusters, TrainingSchedule())
        )
      {
        ++clustering_attempt_number;
      }
      return cluster_indices;
    }

    //! @brief refines the current set of clusters
    //! @param FEATURES the features to cluster
    //! @param FEATURE_CLUSTER_INDICES the index of the closest mean in the clusters last round, for each feature
    //! @param CLUSTERS the current vector of clusters, which will be updated
    //! @return true if the clusters changed
    bool DataSetReducedToKMeans::RefineClusters
    (
      const storage::Vector< storage::VectorND< 2, linal::VectorConstReference< float> > > &FEATURES,
      storage::Vector< size_t> &FEATURE_CLUSTER_INDICES,
      storage::Vector< math::RunningAverage< linal::Vector< float> > > &CLUSTERS,
      const TrainingSchedule &TRAINING_SCHEDULE
    ) const
    {
      size_t nr_features( TRAINING_SCHEDULE.GetSize());
      bool cluster_changed( false);

      // find the closest centroid for each feature
      for( size_t itr( 0); itr < nr_features; ++itr)
      {
        // get a reference to the current feature
        const linal::VectorConstInterface< float> &feature( FEATURES( TRAINING_SCHEDULE( itr)).First());

        // maintain the shortest distance found between this feature and any centroid, as well as the id of the centroid
        float best_distance( std::numeric_limits< float>::infinity());
        size_t best_cluster_id( std::numeric_limits< size_t>::max());

        // usually only a few features change clusters, so first get the distance to the cluster that this
        // feature was last associated with.  this way, our best_distance is likely to be the shortest, in which
        // case the limited square distance calculation in the for loop below can stop early because it will be
        // the other centroids will mostly be much farther away than the closest centroid last time
        if( FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr)) < m_NumberClusters)
        {
          // distance found
          best_distance =
            LimitedSquareDistance
            (
              feature,
              CLUSTERS( FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr))).GetAverage(),
              best_distance
            );
          best_cluster_id = FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr));
        }

        // look in the other clusters, to see if any of them are closer than the best-distance away
        for( size_t itr_cluster( 0); itr_cluster < m_NumberClusters; ++itr_cluster)
        {
          // ignore the cluster that was closest last round, since we already have its distance
          if( itr_cluster == FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr)))
          {
            continue;
          }

          // compute the distance to the cluster with id itr_cluster, limited by the best_distance seen so far
          const float distance( LimitedSquareDistance( feature, CLUSTERS( itr_cluster).GetAverage(), best_distance));

          // if the new distance is better than the previous best, update the best distance
          if( distance < best_distance)
          {
            best_distance = distance;
            best_cluster_id = itr_cluster;
          }
        }

        // check whether the best cluster id changed from the last round
        if( best_cluster_id != FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr)))
        {
          // yep, so update the cluster_changed bool and the closest cluster indice for this feature
          cluster_changed = true;
          FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( itr)) = best_cluster_id;
        }
      }

      // reset all the clusters, so that we can find the new centroids
      for( size_t i( 0); i < m_NumberClusters; ++i)
      {
        CLUSTERS( i).Reset();
      }

      // add the features to the nearest cluster's running average to compute the new clusters
      for( size_t i( 0); i < nr_features; ++i)
      {
        CLUSTERS( FEATURE_CLUSTER_INDICES( TRAINING_SCHEDULE( i))) += FEATURES( TRAINING_SCHEDULE( i)).First();
      }

      return cluster_changed;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetReducedToKMeans::GetNominalSize() const
    {
      return std::min( m_Retriever->GetNominalSize(), m_NumberClusters);
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_reduced_to_principal_components.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_principal_component_analysis.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> DataSetReducedToPrincipalComponents::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetReducedToPrincipalComponents())
    );

    //! @brief constructor from parameters
    //! @param FRACTION fraction of total sum to keep
    //! @param MAX_COMPONENTS_TO_KEEP maximum components to keep
    DataSetReducedToPrincipalComponents::DataSetReducedToPrincipalComponents
    (
      const double &FRACTION,
      const size_t &MAX_COMPONENTS_TO_KEEP
    ) :
      m_Fraction( FRACTION),
      m_MaxToKeep( MAX_COMPONENTS_TO_KEEP),
      m_NumberKept( 0)
    {
      BCL_Assert( m_Fraction >= 0.0 && m_Fraction <= 1.0, "Fraction must be between 0.0 and 1.0");
    }

    //! @brief Clone function
    //! @return pointer to new DataSetReducedToPrincipalComponents
    DataSetReducedToPrincipalComponents *DataSetReducedToPrincipalComponents::Clone() const
    {
      return new DataSetReducedToPrincipalComponents( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetReducedToPrincipalComponents::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetReducedToPrincipalComponents>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetReducedToPrincipalComponents::GetAlias() const
    {
      static const std::string s_Name( "PCA");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    void DataSetReducedToPrincipalComponents::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @param CODE the new code
    void DataSetReducedToPrincipalComponents::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetReducedToPrincipalComponents::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToPrincipalComponents::GetFeatureLabelsWithSizes() const
    {
      FeatureLabelSet labels;
      labels.PushBack( util::ObjectDataLabel( "PCA"), m_NumberKept);
      return labels;
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToPrincipalComponents::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetReducedToPrincipalComponents::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

    //! @brief set retriever
    //! @param RETRIEVER the retriever used to get the data set
    void DataSetReducedToPrincipalComponents::SetRetriever( const util::Implementation< RetrieveDataSetBase> &RETRIEVER)
    {
      m_Retriever = RETRIEVER;
    }

    //! @brief sets the filename of the rescaler, eigenvectors, and eigenvalues
    //! @param FILENAME the filename
    void DataSetReducedToPrincipalComponents::SetFilename( const std::string &FILENAME)
    {
      m_Filename = FILENAME;
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetReducedToPrincipalComponents::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of principal components
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    DataSetReducedToPrincipalComponents::GenerateDataSet()
    {
      // load in the info from the eigenvector matrix file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);
      RescaleFeatureDataSet rescaler;
      io::Serialize::Read( rescaler, input);
      storage::Pair< linal::Matrix< float>, linal::Vector< float> > eigenvectors_values;
      io::Serialize::Read( eigenvectors_values, input);
      io::File::CloseClearFStream( input);

      // get the base data set
      util::ShPtr< descriptor::Dataset> feature_result( m_Retriever->GenerateDataSet());

      // get a reference to the features matrix
      linal::MatrixReference< float> features( feature_result->GetFeaturesReference());
      rescaler.RescaleMatrix( features);

      // reduce input matrix
      linal::Matrix< float> new_matrix;
      linal::PrincipalComponentAnalysis< float>::ReduceInputMatrix
      (
        features,
        new_matrix,
        eigenvectors_values.First(),
        eigenvectors_values.Second(),
        m_Fraction,
        m_MaxToKeep
      );

      // copy the feature matrix, now that it is reduced
      util::ShPtr< FeatureDataSet< float> > features_sp( feature_result->GetFeaturesPtr());
      features_sp->GetRawMatrix() = new_matrix;
      features_sp->SetFeatureLabelSet( GetFeatureLabelsWithSizes());

      // return a new data set initialized with the feature results from the pca
      return feature_result;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetReducedToPrincipalComponents::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription( "Reduces dataset to principal component");

      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );

      member_data.AddInitializer
      (
        "fraction",
        "fraction of total eigenvalue sum to keep",
        io::Serialization::GetAgentWithRange( &m_Fraction, 0.0, 1.0)
      );

      member_data.AddInitializer
      (
        "components",
        "max number of components to allow in the output (independent of the fraction of total component)",
        io::Serialization::GetAgent( &m_MaxToKeep),
        "1000000"
      );

      member_data.AddInitializer
      (
        "filename",
        "file where rescaling, eigenvectors and values are stored",
        io::Serialization::GetAgentInputFilename( &m_Filename)
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetReducedToPrincipalComponents::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool DataSetReducedToPrincipalComponents::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      // load in the info from the eigenvector matrix file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);
      RescaleFeatureDataSet rescaler;
      io::Serialize::Read( rescaler, input);
      storage::Pair< linal::Matrix< float>, linal::Vector< float> > eigenvectors_values;
      io::Serialize::Read( eigenvectors_values, input);
      io::File::CloseClearFStream( input);
      linal::Matrix< float> features( 1, m_Retriever->GetFeatureLabelsWithSizes().GetSize(), 0.0);
      linal::Matrix< float> new_matrix;
      linal::PrincipalComponentAnalysis< float>::ReduceInputMatrix
      (
        features,
        new_matrix,
        eigenvectors_values.First(),
        eigenvectors_values.Second(),
        m_Fraction,
        m_MaxToKeep
      );
      m_NumberKept = new_matrix.GetNumberCols();
      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_data_set_score.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new DataSetScore
    DataSetScore *DataSetScore::Clone() const
    {
      return new DataSetScore( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DataSetScore::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief Get the features of the data set
    //! @return the features of the data set
    const FeatureLabelSet &DataSetScore::GetFeatures() const
    {
      return m_FeatureCodeLabel;
    }

    //! @brief Get the scores for the data set
    //! @return the scores for each feature of the data set
    const linal::Vector< float> &DataSetScore::GetScores() const
    {
      return m_Scores;
    }

    //! @brief Set the features of the data set
    //! @param FEATURES the new features
    void DataSetScore::SetFeatures( const FeatureLabelSet &FEATURES)
    {
      m_FeatureCodeLabel = FEATURES;
    }

    //! @brief Set the scores for each feature
    //! @param SCORES the new scores
    void DataSetScore::SetScores( const linal::Vector< float> &SCORES)
    {
      m_Scores = SCORES;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &DataSetScore::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_FeatureCodeLabel, ISTREAM);
      io::Serialize::Read( m_Scores, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &DataSetScore::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_FeatureCodeLabel, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Scores, OSTREAM, INDENT);
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_data_set_select_columns.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> DataSetSelectColumns::s_Instance
    (
      GetObjectInstances().AddInstance( new DataSetSelectColumns())
    );

    //! @brief default constructor
    //! @param FEATURE_SIZE the expected size of the features
    //! @param INDICES indices of the features to keep
    DataSetSelectColumns::DataSetSelectColumns
    (
      const size_t &FEATURE_SIZE,
      const storage::Vector< size_t> &INDICES
    ) :
      m_ColumnIndices( INDICES),
      m_FeatureSize( FEATURE_SIZE)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DataSetSelectColumns
    DataSetSelectColumns *DataSetSelectColumns::Clone() const
    {
      return new DataSetSelectColumns( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetSelectColumns::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetSelectColumns>();
    }

    //! @brief return the size of vectors expected by operator()
    //! @return the size of vectors expected by operator()
    size_t DataSetSelectColumns::GetInputFeatureSize() const
    {
      return m_FeatureSize;
    }

    //! @brief return the size of vectors returned by operator()
    //! @return the size of vectors returned by operator()
    size_t DataSetSelectColumns::GetOutputFeatureSize() const
    {
      return m_ColumnIndices.GetSize();
    }

    //! @brief return the indices of the input features returned by operator()
    //! @return the indices of the input features returned by operator()
    const storage::Vector< size_t> &DataSetSelectColumns::GetColumnIndices() const
    {
      return m_ColumnIndices;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief reduce a feature to have only the selected features
    //! @param FEATURES a row of a data set
    //! @param STORAGE a pointer to where begin storing the selected values
    void DataSetSelectColumns::operator ()
    (
      const linal::VectorConstInterface< float> &FEATURES,
      float *STORAGE
    ) const
    {
      // make sure the feature has the right size
      BCL_Assert
      (
        FEATURES.GetSize() == m_FeatureSize || !m_FeatureSize,
        "Feature given to " + GetClassIdentifier() + " with incorrect size: " + util::Format()( FEATURES.GetSize())
        + " expected size: " + util::Format()( m_FeatureSize)
      );

      // get an iterator to the beginning of the old feature
      const float *old_feature( FEATURES.Begin());

      // copy the features over in the desired order
      for
      (
        size_t new_feature_index( 0), new_feature_size( GetOutputFeatureSize());
        new_feature_index < new_feature_size;
        ++new_feature_index
      )
      {
        STORAGE[ new_feature_index] = old_feature[ m_ColumnIndices( new_feature_index)];
      }
    }

    //! @brief reduce a feature to have only the selected features
    //! @param FEATURES a row of a data set
    //! @param STORAGE a pointer to where begin storing the selected values
    void DataSetSelectColumns::operator ()
    (
      const linal::VectorConstInterface< char> &FEATURES,
      char *STORAGE
    ) const
    {
      // make sure the feature has the right size
      BCL_Assert
      (
        FEATURES.GetSize() == m_FeatureSize || !m_FeatureSize,
        "Feature given to " + GetClassIdentifier() + " with incorrect size: " + util::Format()( FEATURES.GetSize())
        + " expected size: " + util::Format()( m_FeatureSize)
      );

      // get an iterator to the beginning of the old feature
      const char *old_feature( FEATURES.Begin());

      // copy the features over in the desired order
      for
      (
        size_t new_feature_index( 0), new_feature_size( GetOutputFeatureSize());
        new_feature_index < new_feature_size;
        ++new_feature_index
      )
      {
        STORAGE[ new_feature_index] = old_feature[ m_ColumnIndices( new_feature_index)];
      }
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetSelectColumns::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Selects columns given indices"
      );

      member_data.AddInitializer
      (
        "",
        "indices of columns to select",
        io::Serialization::GetAgent( &m_ColumnIndices)
      );

      return member_data;
    } // GetParameters

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief derived classes can change the indices that will be retained
    //! @param INDICES indices of the features to keep
    void DataSetSelectColumns::SetColumnIndices( const storage::Vector< size_t> &INDICES)
    {
      m_ColumnIndices = INDICES;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_data_set_sqrt.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DataSetSqrt::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new DataSetSqrt())
    );

    //! @brief default constructor
    DataSetSqrt::DataSetSqrt() :
        m_Label()
    {
    }

    //! @brief constructor from the rmsd
    //! @param RMSD the rmsd to use in the clustering algorithm
    //! @param RANGE the min and max # of clusters that would be acceptable
    //! @param MAX_STEPS_TO_REACH_CLUSTER_SIZE the maximum # of attempts to change the rmsd to get the # of clusters into the range
    //! @param AUTOSCALE true if the data need to be normalized before clustering
    DataSetSqrt::DataSetSqrt
    (
      const std::string &ID_LABEL
    ) :
      m_Label( ID_LABEL)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DataSetSqrt
    DataSetSqrt *DataSetSqrt::Clone() const
    {
      return new DataSetSqrt( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &DataSetSqrt::GetClassIdentifier() const
    {
      return GetStaticClassName< DataSetSqrt>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &DataSetSqrt::GetAlias() const
    {
      static const std::string s_Name( "Sqrt");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void DataSetSqrt::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void DataSetSqrt::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void DataSetSqrt::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > DataSetSqrt::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetSqrt::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetSqrt::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet DataSetSqrt::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      DataSetSqrt::GenerateDataSet()
    {
      // get the base data set and use the clustering algorithm on it
      util::ShPtr< descriptor::Dataset> changeable( m_Retriever->GenerateDataSet());
      this->operator()( changeable);
      return changeable;
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    void DataSetSqrt::operator ()( util::ShPtr< descriptor::Dataset> DATA) const
    {
      linal::MatrixReference< float> feature_matrix( DATA->GetFeatures().GetMatrix());
      linal::MatrixReference< float> result_matrix( DATA->GetResults().GetMatrix());
      for( linal::MatrixReference< float>::iterator itr_m( feature_matrix.Begin()), itr_m_end( feature_matrix.End());
          itr_m != itr_m_end;
          ++itr_m
         )
      {
        *itr_m = ( *itr_m < 0 ? -1.0 : 1.0) * std::sqrt( std::fabs( *itr_m));
      }
      for( linal::MatrixReference< float>::iterator itr_m( result_matrix.Begin()), itr_m_end( result_matrix.End());
          itr_m != itr_m_end;
          ++itr_m
         )
      {
        *itr_m = ( *itr_m < 0 ? -1.0 : 1.0) * std::sqrt( std::fabs( *itr_m));
      }

      if( m_Label.empty())
      {
        return;
      }

      linal::MatrixReference< char> id_mat( DATA->GetIds().GetMatrix());
      BCL_Assert
      (
        id_mat.GetNumberCols() >= m_Label.length(),
        "Specified label \"" + m_Label + "\" is too long for the given dataset (no fewer than "
          + util::Format()( id_mat.GetNumberCols()) + " characters allowed)"
      );

      std::string padded_label( m_Label);
      padded_label += std::string( id_mat.GetNumberCols() - m_Label.length(), ' ');
      for( size_t r( 0), end_r( id_mat.GetNumberRows()); r < end_r; ++r)
      {
        std::copy( padded_label.begin(), padded_label.end(), id_mat[ r]);
      }
    }

    //! @brief reduce a data set
    //! @param DATA the data set
    //! @return the reduced data set
    util::ShPtr< descriptor::Dataset> DataSetSqrt::operator()( const descriptor::Dataset &DATA) const
    {
      util::ShPtr< descriptor::Dataset> changeable( DATA.HardCopy());
      this->operator()( changeable);
      return changeable;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DataSetSqrt::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Multiplies the feature and result values of a dataset by a fixed value"
      );
      member_data.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Retriever)
      );

      member_data.AddOptionalInitializer
      (
        "id label",
        "the id label to add to multiplied data points",
        io::Serialization::GetAgent( &m_Label)
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t DataSetSqrt::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_data_set_statistics.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new DataSetStatistics
    DataSetStatistics *DataSetStatistics::Clone() const
    {
      return new DataSetStatistics( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DataSetStatistics::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief compute the data set statistics from a dataset retriever
    //! @param DATASET some means of making a dataset retriever
    void DataSetStatistics::AddDataSet( util::Implementation< RetrieveDataSetBase> DATASET)
    {
      util::ShPtr< descriptor::Dataset> dataset( DATASET->GenerateDataSet());
      this->AddDataSet( *dataset);
    }

    //! @brief compute the data set statistics from a dataset
    //! @param DATASET a dataset
    void DataSetStatistics::AddDataSet( const descriptor::Dataset &DATASET)
    {
      const size_t n_features( DATASET.GetSize());
      const size_t n_feature_cols( DATASET.GetFeatureSize());
      const size_t n_result_cols( DATASET.GetResultSize());

      for( size_t feature( 0); feature < n_features; ++feature)
      {
        linal::VectorConstReference< float> feature_ref( n_feature_cols, DATASET.GetFeaturesReference()[ feature]);
        linal::VectorConstReference< float> result_ref( n_result_cols, DATASET.GetResultsReference()[ feature]);
        m_AveStdFeatures += feature_ref;
        m_MinMaxFeatures += feature_ref;
        m_AveStdResults += result_ref;
        m_MinMaxResults += result_ref;
      }
    }

    //! @brief compute the data set statistics as the difference between two other datasets
    //! @param DATASET_A, DATASET_B actual datasets
    void DataSetStatistics::AddDifference
    (
      const descriptor::Dataset &DATASET_A,
      const descriptor::Dataset &DATASET_B
    )
    {
      descriptor::Dataset difference( DATASET_A);
      linal::MatrixReference< float> features( difference.GetFeaturesReference());
      linal::MatrixReference< float> results( difference.GetResultsReference());
      features -= DATASET_B.GetFeaturesReference();
      results -= DATASET_B.GetResultsReference();
      AddDataSet( difference);
    }

    //! @brief compute the data set statistics as the difference between two other datasets
    //! @param DATASET_A, DATASET_B some means of making a dataset
    void DataSetStatistics::AddDifference
    (
      util::Implementation< RetrieveDataSetBase> DATASET_A,
      util::Implementation< RetrieveDataSetBase> DATASET_B
    )
    {
      util::ShPtr< descriptor::Dataset> dataset_a( DATASET_A->GenerateDataSet());
      util::ShPtr< descriptor::Dataset> dataset_b( DATASET_B->GenerateDataSet());
      linal::MatrixReference< float> features( dataset_a->GetFeaturesReference());
      linal::MatrixReference< float> results( dataset_a->GetResultsReference());
      features -= dataset_b->GetFeaturesReference();
      results -= dataset_b->GetResultsReference();
      AddDataSet( *dataset_a);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &DataSetStatistics::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_AveStdFeatures, ISTREAM);
      io::Serialize::Read( m_MinMaxFeatures, ISTREAM);
      io::Serialize::Read( m_AveStdResults, ISTREAM);
      io::Serialize::Read( m_MinMaxResults, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &DataSetStatistics::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_AveStdFeatures, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_MinMaxFeatures, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_AveStdResults, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_MinMaxResults, OSTREAM, INDENT);
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_decision_tree.h"

// includes from bcl - sorted alphabetically
#include "model/bcl_model_feature_data_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DecisionTree::s_Instance
    (
      GetObjectInstances().AddInstance( new DecisionTree())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor for a decision tree
    DecisionTree::DecisionTree() :
      m_ModalActivity(),
      m_Count( 0),
      m_SplitIndex( util::GetUndefined< size_t>()),
      m_SplitValue( util::GetUndefined< float>())
    {
    }

    //! @brief constructor with parameters
    //! @param ACTIVITY average activity of this node
    DecisionTree::DecisionTree( const linal::Vector< float> &ACTIVITY) :
      m_ModalActivity( ACTIVITY),
      m_Count( 1),
      m_SplitIndex( util::GetUndefined< size_t>()),
      m_SplitValue( util::GetUndefined< float>())
    {
    }

    //! @brief clone this and any underlying objects
    DecisionTree *DecisionTree::DeepClone() const
    {
      DecisionTree *deep_clone( Clone());

      if( !IsLeaf())
      {
        deep_clone->ConfigureBranch
        (
          m_SplitIndex,
          m_SplitValue,
          util::ShPtr< DecisionTree>( m_LeftChild->DeepClone()),
          util::ShPtr< DecisionTree>( m_RightChild->DeepClone())
        );
      }
      return deep_clone;
    }

    //! @brief remove the leaves of this decision tree if both leaves have ave activities on the same side of cutoff
    //!        or if either leaf is has < SIZE_CUTOFF members
    //! @param ACTIVITY_CUTOFF the cutoff to use; use undefined if only filtering by leaf size
    //! @param SIZE_CUTOFF the size cutoff to use, use 0 if only filtering by activity
    //! @note if both criteria are specified, then either criterion can trigger the pruning
    void DecisionTree::Prune( const double &ACTIVITY_CUTOFF, const size_t &SIZE_CUTOFF)
    {
      if( IsLeaf())
      {
        // leaves cannot prune themselves, so just return
        return;
      }

      // prune child nodes first before testing whether they are leaves
      m_LeftChild->Prune( ACTIVITY_CUTOFF, SIZE_CUTOFF);
      m_RightChild->Prune( ACTIVITY_CUTOFF, SIZE_CUTOFF);

      // if either child is not a leaf, there is nothing more to do
      if( !m_LeftChild->IsLeaf() || !m_RightChild->IsLeaf())
      {
        return;
      }

      // if either child is too small, prune
      if( m_LeftChild->m_Count < SIZE_CUTOFF || m_RightChild->m_Count < SIZE_CUTOFF)
      {
        // if either of the children have too few nodes, make this node a leaf
        MakeLeaf();
      }
      else if( util::IsDefined( ACTIVITY_CUTOFF))
      {
        const linal::Vector< float> &activities_left( m_LeftChild->GetActivity());
        const linal::Vector< float> &activities_right( m_RightChild->GetActivity());
        for( size_t i( 0), n_outputs( GetNumberOutputs()); i < n_outputs; ++i)
        {
          // check whether the features are on opposite sides of the cutoff
          if( bool( activities_left( i) <= ACTIVITY_CUTOFF) != bool( activities_right( i) <= ACTIVITY_CUTOFF))
          {
            return;
          }
        }

        // all activities were on the same side of the cutoff, so both child nodes are equivalent to this node
        // and can be removed
        MakeLeaf();
      }
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief set the average activity of this node of the decision tree
    //! @param AVERAGES averages of each result
    void DecisionTree::SetActivity( const storage::Vector< math::RunningAverage< float> > &AVERAGES)
    {
      // set modal activity to the correct size
      m_ModalActivity = linal::Vector< float>( AVERAGES.GetSize(), util::GetUndefined< float>());

      if( AVERAGES.IsEmpty())
      {
        // empty dataset, just set m_Count to 0 and return
        m_Count = 0;
      }
      else
      {
        // set m_Count
        m_Count = AVERAGES.FirstElement().GetWeight();
      }

      // for each result, set the modal activity to the value from AVERAGES_ABOVE / AVERAGES_BELOW if there were more (Above/Below) the cutoff
      for
      (
        size_t result_number( 0), number_results( AVERAGES.GetSize());
        result_number < number_results;
        ++result_number
      )
      {
        m_ModalActivity( result_number) = AVERAGES( result_number).GetAverage();
      }
    }

    //! @brief get the size of the tree rooted at this node
    //! @return the size of the tree rooted at this node
    size_t DecisionTree::GetTreeSize() const
    {
      return 1 + ( IsLeaf() ? 0 : m_LeftChild->GetTreeSize() + m_RightChild->GetTreeSize());
    }

    //! @brief configure this node to be a branch node with the given parameters
    //! @param SPLIT_INDEX specifies which index of feature to split based off of
    //! @param SPLIT_VALUE specifies which value of selected feature; values <= this value will be sent to left child, > will be sent right
    //! @param DECISION_NODE_PAIR shptr to pair with node and their decision function
    //! @param LEFT_CHILD, RIGHT_CHILD the children of this branch
    void DecisionTree::ConfigureBranch
    (
      const size_t &SPLIT_INDEX,
      const float &SPLIT_VALUE,
      const util::ShPtr< DecisionTree> &LEFT_CHILD,
      const util::ShPtr< DecisionTree> &RIGHT_CHILD
    )
    {
      m_SplitIndex = SPLIT_INDEX;
      m_SplitValue = SPLIT_VALUE;
      m_LeftChild = LEFT_CHILD;
      m_RightChild = RIGHT_CHILD;
      BCL_Assert
      (
        m_LeftChild.IsDefined() == m_RightChild.IsDefined(),
        "Children of a decision tree node must be either both defined or undefined"
      );

      BCL_Assert
      (
        m_LeftChild != m_RightChild,
        "Children of a decision tree node must be either both defined or undefined"
      );
    }

    //! @brief get a vector of decision trees, ordered by depth in the sub-tree
    //! @return the subtree below this node
    storage::Vector< util::ShPtrVector< DecisionTree> > DecisionTree::GetSubtreeLevels() const
    {
      storage::Vector< util::ShPtrVector< DecisionTree> > forest;
      if( !IsLeaf())
      {
        // get the left and right forests too
        storage::Vector< util::ShPtrVector< DecisionTree> > left_forest( m_LeftChild->GetSubtreeLevels());
        storage::Vector< util::ShPtrVector< DecisionTree> > right_forest( m_RightChild->GetSubtreeLevels());

        // allocate enough memory to hold the new nodes
        forest.Resize( std::max( left_forest.GetSize(), right_forest.GetSize()) + 1);

        // add this node's children to the forest
        forest( 0).PushBack( m_LeftChild);
        forest( 0).PushBack( m_RightChild);

        // concatenate the resulting forests
        size_t depth( 1);
        for
        (
          storage::Vector< util::ShPtrVector< DecisionTree> >::const_iterator
          itr_forest( left_forest.Begin()), itr_forest_end( left_forest.End());
          itr_forest != itr_forest_end;
          ++itr_forest, ++depth
        )
        {
          forest( depth).Append( *itr_forest);
        }
        depth = 1;
        for
        (
          storage::Vector< util::ShPtrVector< DecisionTree> >::const_iterator
          itr_forest( right_forest.Begin()), itr_forest_end( right_forest.End());
          itr_forest != itr_forest_end;
          ++itr_forest, ++depth
        )
        {
          forest( depth).Append( *itr_forest);
        }
      }
      return forest;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURES not rescaled features
    //! @return predcited result vector using a model
    FeatureDataSet< float> DecisionTree::PredictWithoutRescaling( const FeatureDataSetInterface< float> &FEATURES) const
    {
      linal::Matrix< float> results( FEATURES.GetNumberFeatures(), m_ModalActivity.GetSize());
      for
      (
        size_t feature_number( 0), number_features( FEATURES.GetNumberFeatures());
        feature_number < number_features;
        ++feature_number
      )
      {
        PredictFeature( FEATURES( feature_number), results[ feature_number]);
      }
      return FeatureDataSet< float>( results);
    }

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void DecisionTree::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      // no rescaling is ever performed by decision trees, so just remove any rescaling on the dataset
      FEATURE.DeScale();
    }

    //! @brief make a branch node into a leaf node
    void DecisionTree::MakeLeaf()
    {
      m_SplitIndex = util::GetUndefined< size_t>();
      m_SplitValue = util::GetUndefined< float>();
      m_LeftChild  = util::ShPtr< DecisionTree>();
      m_RightChild = util::ShPtr< DecisionTree>();
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURES normalized or rescaled features
    //! @return predcited results using a model
    FeatureDataSet< float> DecisionTree::operator()( const FeatureDataSetInterface< float> &FEATURES) const
    {
      return PredictWithoutRescaling( FEATURES);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &DecisionTree::Read( std::istream &ISTREAM)
    {
      // read in members
      io::Serialize::Read( m_SplitIndex, ISTREAM);
      io::Serialize::Read( m_SplitValue, ISTREAM);
      io::Serialize::Read( m_Count, ISTREAM);
      io::Serialize::Read( m_ModalActivity, ISTREAM);

      // if this was a branch node, read in the children
      if( util::IsDefined( m_SplitIndex))
      {
        io::Serialize::Read( m_LeftChild, ISTREAM);
        io::Serialize::Read( m_RightChild, ISTREAM);
      }
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT indentation
    //! @return outputstream which was written to
    std::ostream &DecisionTree::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_SplitIndex, OSTREAM, INDENT) << ' ';
      io::Serialize::Write( m_SplitValue, OSTREAM) << ' ';
      io::Serialize::Write( m_Count, OSTREAM) << '\n';

      io::Serialize::Write( m_ModalActivity, OSTREAM, INDENT);

      // write the children if this was a branch node
      if( util::IsDefined( m_SplitIndex))
      {
        OSTREAM << '\n';
        io::Serialize::Write( m_LeftChild,  OSTREAM, INDENT) << '\n';
        io::Serialize::Write( m_RightChild, OSTREAM, INDENT);
      }

      // end
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DecisionTree::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "see http://en.wikipedia.org/wiki/Decision_tree");
      return parameters;
    }

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @param RESULT where to store the result
    void DecisionTree::PredictFeature
    (
      const FeatureReference< float> &FEATURE,
      float *RESULT
    ) const
    {
      // for branch nodes, pass the decision on to the correct branch
      if( !IsLeaf())
      {
        if( FEATURE( m_SplitIndex) <= m_SplitValue)
        {
          m_LeftChild->PredictFeature( FEATURE, RESULT);
        }
        else
        {
          m_RightChild->PredictFeature( FEATURE, RESULT);
        }
      }
      else
      {
        // for leaf nodes, just copy the result
        std::copy( m_ModalActivity.Begin(), m_ModalActivity.End(), RESULT);
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_descriptor_selection_backward_elimination.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DescriptorSelectionBackwardElimination::s_Instance
    (
      util::Enumerated< DescriptorSelectionInterface>::AddInstance
      (
        new DescriptorSelectionBackwardElimination()
      )
    );

    //! @brief Clone function
    //! @return pointer to new DescriptorSelectionBackwardElimination
    DescriptorSelectionBackwardElimination *DescriptorSelectionBackwardElimination::Clone() const
    {
      return new DescriptorSelectionBackwardElimination();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DescriptorSelectionBackwardElimination::GetAlias() const
    {
      static const std::string s_Name( "FeatureBackwardElimination");
      return s_Name;
    }

    //! @brief assemble all descriptor combinations based on an initial and a total descriptor set as an object label
    //! @param INITIAL initial descriptor set as an object label
    //! @param TOTAL all available descriptor groups in descriptor selection process
    //! @return container with all possible descriptor combinations based on initial descriptor set
    const storage::Vector< util::ObjectDataLabel> DescriptorSelectionBackwardElimination::operator()
    (
      const util::ObjectDataLabel &INITIAL,
      const util::ObjectDataLabel &TOTAL
    ) const
    {
      // final vector of descriptor combination pairs
      storage::Vector< util::ObjectDataLabel> qsar_object_list;

      // position in qsar_object_list
      size_t position( 0);

      // iterate over all possible properties
      for
      (
        util::ObjectDataLabel::const_iterator itr_prop( INITIAL.Begin()), itr_prop_end( INITIAL.End());
        itr_prop != itr_prop_end;
        ++itr_prop, ++position
      )
      {
        // get the initial arguments
        storage::Vector< util::ObjectDataLabel> initial_labels( INITIAL.Begin(), INITIAL.End());
        initial_labels.RemoveElements( position, 1);

        // make new label containing all the original labels, along with the new one
        util::ObjectDataLabel new_label( INITIAL.GetName(), INITIAL.GetValue(), initial_labels);

        // insert current combination to final list
        qsar_object_list.PushBack( new_label);
      }

      // return list of descriptor combinations
      return qsar_object_list;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DescriptorSelectionBackwardElimination::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Sequential feature backward elimination - starts with all descriptor groups and removes one group from"
        "an initial successor group"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_descriptor_selection_exhaustive.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math.h"
#include "util/bcl_util_message.h"
// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DescriptorSelectionExhaustive::s_Instance
    (
      util::Enumerated< DescriptorSelectionInterface>::AddInstance
      (
        new DescriptorSelectionExhaustive()
      )
    );

    //! @brief Clone function
    //! @return pointer to new DescriptorSelectionExhaustive
    DescriptorSelectionExhaustive *DescriptorSelectionExhaustive::Clone() const
    {
      return new DescriptorSelectionExhaustive();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DescriptorSelectionExhaustive::GetAlias() const
    {
      static const std::string s_Name( "ExhaustiveSelection");
      return s_Name;
    }

    namespace
    {
      //! @brief update chosen with the next combination n-choose-k corresponds to N-Choose-chosen.getsize()
      bool NextCombination( storage::Vector< size_t> &CHOSEN, const size_t &N)
      {
        const size_t k( CHOSEN.GetSize());
        size_t mx_value( N - 1);
        for( size_t i( 0); i < k; ++i, --mx_value)
        {
          if( CHOSEN( i) < mx_value)
          {
            ++CHOSEN( i);
            for( size_t j( 0); j < i; ++j)
            {
              CHOSEN( j) = CHOSEN( i) + i - j;
            }
            return true;
            break;
          }
        }
        return false;
      }
    }

    //! @brief assemble all descriptor combinations based on an initial and a total descriptor set as an object label
    //! @param INITIAL initial descriptor set as an object label
    //! @param TOTAL all available descriptor groups in descriptor selection process
    //! @return container with all possible descriptor combinations based on initial descriptor set
    const storage::Vector< util::ObjectDataLabel> DescriptorSelectionExhaustive::operator()
    (
      const util::ObjectDataLabel &INITIAL,
      const util::ObjectDataLabel &TOTAL
    ) const
    {
      storage::Vector< util::ObjectDataLabel> labels;
      const size_t n( TOTAL.GetNumberArguments());
      for( size_t k( 1), max_k( std::min( m_MaxFeatures, TOTAL.GetNumberArguments() - 1)); k <= max_k; ++k)
      {
        storage::Vector< size_t> indices( k);
        for( size_t i( 0); i < k; ++i)
        {
          indices( i) = k - i - 1;
        }

        labels.AllocateMemory( labels.GetSize() + math::BinomialCoefficient( n, k));
        do
        {
          storage::Vector< util::ObjectDataLabel> sublabels;
          sublabels.AllocateMemory( k);
          for( size_t i( 0); i < k; ++i)
          {
            sublabels.PushBack( TOTAL.GetArgument( indices( i)));
          }
          labels.PushBack( util::ObjectDataLabel( "Combine", sublabels));
        } while( NextCombination( indices, n));
      }

      // return list of descriptor combinations
      return labels;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DescriptorSelectionExhaustive::GetSerializer() const
    {
      io::Serializer serial;
      serial.SetCommandLineIdentifier( "Exhaustive Forward feature selection");
      serial.AddInitializer( "max features", "max # of features to use", io::Serialization::GetAgent( &m_MaxFeatures), "2");
      return serial;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_descriptor_selection_feature_forward.h"

// includes from bcl - sorted alphabetically
#include "util/bcl_util_message.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DescriptorSelectionFeatureForward::s_Instance
    (
      util::Enumerated< DescriptorSelectionInterface>::AddInstance
      (
        new DescriptorSelectionFeatureForward()
      )
    );

    //! @brief Clone function
    //! @return pointer to new DescriptorSelectionFeatureForward
    DescriptorSelectionFeatureForward *DescriptorSelectionFeatureForward::Clone() const
    {
      return new DescriptorSelectionFeatureForward();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DescriptorSelectionFeatureForward::GetAlias() const
    {
      static const std::string s_Name( "FeatureForwardSelection");
      return s_Name;
    }

    //! @brief assemble all descriptor combinations based on an initial and a total descriptor set as an object label
    //! @param INITIAL initial descriptor set as an object label
    //! @param TOTAL all available descriptor groups in descriptor selection process
    //! @return container with all possible descriptor combinations based on initial descriptor set
    const storage::Vector< util::ObjectDataLabel> DescriptorSelectionFeatureForward::operator()
    (
      const util::ObjectDataLabel &INITIAL,
      const util::ObjectDataLabel &TOTAL
    ) const
    {
      // final vector of descriptor combination pairs
      storage::Vector< util::ObjectDataLabel> qsar_object_list;
      // set for extracting descriptor group already chosen
      storage::Set< util::ObjectDataLabel> initial_set;
      // descriptor groups available for descriptor selection based on initial set
      storage::Vector< util::ObjectDataLabel> entire_minus_initial;

      // iterate over all possible properties
      for
      (
        util::ObjectDataLabel::const_iterator
          itr_prop( INITIAL.Begin()),
          itr_prop_end( INITIAL.End());
        itr_prop != itr_prop_end;
        ++itr_prop
      )
      {
        initial_set.Insert( *itr_prop);
      }

      // iterate over all possible properties
      for
      (
        util::ObjectDataLabel::const_iterator
          itr_prop( TOTAL.Begin()),
          itr_prop_end( TOTAL.End());
        itr_prop != itr_prop_end;
        ++itr_prop
      )
      {
        // only if property is not in initial set it can stay in vector
        if( !initial_set.Contains( *itr_prop))
        {
          entire_minus_initial.PushBack( *itr_prop);
        }
      }

      BCL_MessageDbg
      (
        "entire_minus_initial list of QSAR code: \n" + util::Format()( entire_minus_initial)
      );

      // get the initial arguments
      storage::Vector< util::ObjectDataLabel> initial_labels( INITIAL.GetArguments());

      // iterate over all possible properties
      for
      (
        util::ObjectDataLabel::const_iterator
          itr_prop( entire_minus_initial.Begin()),
          itr_prop_end( entire_minus_initial.End());
        itr_prop != itr_prop_end;
        ++itr_prop
      )
      {
        // add the current label to the initial labels
        initial_labels.PushBack( *itr_prop);

        // make new label containing all the original labels, along with the new one
        util::ObjectDataLabel new_label( INITIAL.GetName(), INITIAL.GetValue(), initial_labels);

        // remove the most recently added label from the initial labels
        initial_labels.PopBack();

        // insert current combination to final list
        qsar_object_list.PushBack( new_label);
      }

      // return list of descriptor combinations
      return qsar_object_list;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DescriptorSelectionFeatureForward::GetSerializer() const
    {
      return
        io::Serializer().SetCommandLineIdentifier
        (
          "Sequential feature forward selection - starts with single descriptor groups and adds remaining groups to"
          "an initial successor group"
        );
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_binary_partition.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> DtreeBinaryPartition::s_Instance
    (
      GetObjectInstances().AddInstance( new DtreeBinaryPartition())
    );

    //! @brief Data as string
    //! @param DATA the data of interest
    //! @return the string for the data
    const std::string &DtreeBinaryPartition::GetDataName( const DtreeBinaryPartition::Data &DATA)
    {
      static const std::string s_names[ s_NumberData + 1] =
      {
        "SplitRating",
        "InitialNumIncorrect",
        "RatingTimesInitialNumIncorrect",
        "InitialIncorrectPlusFinalCorrect",
        GetStaticClassName< DtreeBinaryPartition::Data>()
      };
      return s_names[ size_t( DATA)];
    }

    //! @brief default constructor
    DtreeBinaryPartition::DtreeBinaryPartition() :
      m_FeatureIndex( util::GetUndefined< size_t>()),
      m_SplitValue( util::GetUndefined< float>()),
      m_SplitRating( -std::numeric_limits< float>::max()),
      m_Size( 0),
      m_InitialIncorrect( 0),
      m_FinalIncorrect( 0)
    {
    }

    //! @brief constructor from members
    DtreeBinaryPartition::DtreeBinaryPartition
    (
      const size_t &FEATURE_INDEX,
      const float  &SPLIT_VALUE,
      const float  &SPLIT_RATING,
      const size_t &SIZE,
      const size_t &NUM_INCORRECT
    ) :
      m_FeatureIndex( FEATURE_INDEX),
      m_SplitValue( SPLIT_VALUE),
      m_SplitRating( SPLIT_RATING),
      m_Size( SIZE),
      m_InitialIncorrect( NUM_INCORRECT),
      m_FinalIncorrect( 0)
    {
    }

    //! @brief Clone function
    //! @return pointer to new DtreeBinaryPartition
    DtreeBinaryPartition *DtreeBinaryPartition::Clone() const
    {
      return new DtreeBinaryPartition( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DtreeBinaryPartition::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the value specified by the enum
    //! @param DATA the data to retrieve from the partition
    //! @return the value specified by the enum
    float DtreeBinaryPartition::GetData( const DtreeBinaryPartition::Data &DATA) const
    {
      switch( DATA)
      {
        case e_SplitRating:
          return m_SplitRating;
        case e_InitialNumIncorrect:
          return m_InitialIncorrect;
        case e_RatingTimesInitialNumIncorrect:
          return m_InitialIncorrect;
        case e_InitialIncorrectPlusFinalCorrect:
          return 2 * m_InitialIncorrect - m_FinalIncorrect;
        default:
          BCL_Exit( "No value for data " + GetDataName( DATA), -1);
      }
      return 0.0;
    }

    //! @brief add incorrect estimate info to a binary partition
    //! @param STATE_COUNTS total state counts (from determine total state counts on the post-partition)
    void DtreeBinaryPartition::DetermineAccuracy( const storage::Vector< FeatureResultAndState> &DATA)
    {
      m_Size = DATA.GetSize();
      if( !util::IsDefined( m_FeatureIndex))
      {
        m_FinalIncorrect = m_InitialIncorrect = 0;
        return;
      }

      // initialize count of elements on the left and right
      linal::Vector< size_t> lhs_state_counts( DATA.FirstElement().GetResultState().GetSize(), size_t( 0));
      linal::Vector< size_t> rhs_state_counts( lhs_state_counts);

      // track the left and right hand sides
      size_t lhs_size( 0);

      //iterate over candidate set, which includes boundaries between possible split values,
      for
      (
        storage::Vector< FeatureResultAndState>::const_iterator itr( DATA.Begin()), itr_end( DATA.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this feature is on the lhs or rhs
        if( itr->GetFeature()( m_FeatureIndex) <= m_SplitValue)
        {
          lhs_state_counts += itr->GetResultState();
          ++lhs_size;
        }
        else
        {
          rhs_state_counts += itr->GetResultState();
        }
      }
      const size_t rhs_size( m_Size - lhs_size);

      m_FinalIncorrect
        = EstimateNumberIncorrect( lhs_size, lhs_state_counts) + EstimateNumberIncorrect( rhs_size, rhs_state_counts);
      m_InitialIncorrect = EstimateNumberIncorrect( m_Size, lhs_state_counts + rhs_state_counts);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &DtreeBinaryPartition::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_FeatureIndex, ISTREAM);
      io::Serialize::Read( m_SplitValue, ISTREAM);
      io::Serialize::Read( m_SplitRating, ISTREAM);
      io::Serialize::Read( m_InitialIncorrect, ISTREAM);
      io::Serialize::Read( m_FinalIncorrect, ISTREAM);
      io::Serialize::Read( m_Size, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &DtreeBinaryPartition::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_FeatureIndex, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SplitValue, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SplitRating, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_InitialIncorrect, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_FinalIncorrect, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Size, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief estimate # incorrect
    //! @param SIZE the total # of elements represented by the state counts vector
    //! @param STATE_COUNTS the count of features in each state
    size_t DtreeBinaryPartition::EstimateNumberIncorrect
    (
      const size_t &SIZE,
      const linal::Vector< size_t> &STATE_COUNTS
    )
    {
      size_t estimate( 0);
      // handle the common case where there is only 1 result.  In this case, the incorrect counts are exactly right
      if( STATE_COUNTS.GetSize() == size_t( 1))
      {
        estimate = std::min( STATE_COUNTS( 0), SIZE - STATE_COUNTS( 0));
      }
      else if( STATE_COUNTS.GetSize() > size_t( 0))
      {
        // estimate the # of incorrectly classified elements in each state
        // the upper bound is given by the sum of state counts in impure states
        // the lower bound is given by the maximum state count in an impure state
        size_t incorrect_count_upper_bound( 0);
        size_t incorrect_count_lower_bound( 0);
        for
        (
          const size_t *itr( STATE_COUNTS.Begin()), *itr_end( STATE_COUNTS.End());
          itr != itr_end;
          ++itr
        )
        {
          const size_t incorrect_this_state( std::min( *itr, SIZE - *itr));
          incorrect_count_upper_bound += incorrect_this_state;
          incorrect_count_lower_bound = std::max( incorrect_count_lower_bound, incorrect_this_state);
        }

        incorrect_count_upper_bound = std::min( incorrect_count_upper_bound, SIZE);
        estimate = ( incorrect_count_lower_bound + incorrect_count_upper_bound) / 2;
      }
      return estimate;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_data_partition_function_interface.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_comparisons.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief get all split ratings for this partition
    //! @param INPUTDATASET a dataset
    //! @param SHOW_STATUS whether to show current status
    //! @return a vector containing ratings for all columns
    linal::Vector< float> DtreeDataPartitionFunctionInterface::GetAllPartitionRatings
    (
      const storage::Vector< FeatureResultAndState> &INPUTDATASET,
      const bool &SHOW_STATUS
    ) const
    {
      // dataset information
      const size_t descriptor_count( INPUTDATASET.FirstElement().GetFeature().GetSize());

      linal::Vector< float> ratings( descriptor_count, -std::numeric_limits< float>::max());

      // cache # of items in each state on the right side of the partition (initially everything)
      linal::Vector< size_t> total_items_in_state( DetermineTotalStateCounts( INPUTDATASET));
      if( total_items_in_state.GetSize() == 0)
      {
        // pure node, return an undefined split
        return ratings;
      }

      const size_t status_bar_length( 20);

      for
      (
        size_t current_descriptor_index( 0);
        current_descriptor_index < descriptor_count;
        ++current_descriptor_index
      )
      {
        if( SHOW_STATUS)
        {
          // determine progress percent
          const size_t percent( float( current_descriptor_index) * 100.0 / float( descriptor_count));

          // determine number of stars in the status bar
          const size_t number_stars( percent * status_bar_length / 100);

          const std::string status
          (
            "["
            + std::string( number_stars, '*')
            + std::string( status_bar_length - number_stars, ' ')
            + "] "
            + util::Format()( percent) + "% "
            + util::Format()( current_descriptor_index) + "/" + util::Format()( descriptor_count)
            + " columns partitioned"
          );
          util::GetLogger().LogStatus( status);
        }
        DtreeBinaryPartition split_for_this_descriptor
        (
          DetermineSplitValueAndRating
          (
            ExtractAndSortDescriptorAtIndex( current_descriptor_index, INPUTDATASET),
            current_descriptor_index,
            total_items_in_state
          )
        );
        ratings( current_descriptor_index) = split_for_this_descriptor.GetSplitRating();
      }

      return ratings;
    }

    //! @brief extracts a column of descriptors at index INDEX, sorted by index
    //! @param INDEX descriptor index
    //! @param INPUTDATASET ShPtr to a dataset interface
    //! @return list of pairs of descriptor values and a FeatureReference to the results
    //! @note sorts descriptor values will be in ascending order
    storage::Vector< storage::Pair< float, FeatureReference< size_t> > >
      DtreeDataPartitionFunctionInterface::ExtractAndSortDescriptorAtIndex
    (
      const size_t INDEX,
      const storage::Vector< FeatureResultAndState> &INPUTDATASET
    )
    {
      // list of activities and expected results
      storage::Vector< storage::Pair< float, FeatureReference< size_t> > > descriptor_activity_pairs;

      // calculates the data set size once
      const size_t dataset_size( INPUTDATASET.GetSize());
      descriptor_activity_pairs.AllocateMemory( dataset_size);

      // Goes through the dataset and extracts the values for one descriptor and all associated activities into a
      // vector of pairs so that they can be sorted
      for( size_t dataset_counter( 0); dataset_counter < dataset_size; ++dataset_counter)
      {
        descriptor_activity_pairs.PushBack
        (
          storage::Pair< float, FeatureReference< size_t> >
          (
            INPUTDATASET( dataset_counter).GetFeature()( INDEX),
            INPUTDATASET( dataset_counter).GetResultState()
          )
        );
      }

      //! sort pairs by first value (Descriptor) in ascending order
      descriptor_activity_pairs.Sort
      (
        std::less< storage::Pair< float, FeatureReference< size_t> > >()
      );

      // return list of activities and expected results
      return descriptor_activity_pairs;
    } // ExtractAndSortDescriptorAtIndex

    //! @brief determines the best split value and rating
    //! @param SORTED_DESCRIPTORS sorted descriptor values for that feature
    //! @param FEATURE_INDEX index of the feature to split based off of
    //! @param TOTAL_STATE_COUNTS counts of values in each state
    //! @return the best split value and rating
    DtreeBinaryPartition
    DtreeDataPartitionFunctionInterface::DetermineSplitValueAndRating
    (
      const storage::Vector< storage::Pair< float, FeatureReference< size_t> > > &SORTED_DESCRIPTORS,
      const size_t &FEATURE_INDEX,
      const linal::Vector< size_t> &TOTAL_STATE_COUNTS
    ) const
    {
      // cache # of items in each state on the right side of the partition (initially everything)
      if( TOTAL_STATE_COUNTS.GetSize() == 0)
      {
        // only pure states were found, no need for a split
        return DtreeBinaryPartition();
      }

      // object to hold the best split
      DtreeBinaryPartition best_split;

      // get the rating of the complete partition
      const float initial_rating( RatePartition( TOTAL_STATE_COUNTS, SORTED_DESCRIPTORS.GetSize()));

      // iterator for considering candidate descriptor values
      storage::Vector< storage::Pair< float, FeatureReference< size_t> > >::const_iterator
        itr_left( SORTED_DESCRIPTORS.Begin()), itr_right( SORTED_DESCRIPTORS.Begin());

      // move itr_right to the right, so it points to the element on the right side of the partition
      ++itr_right;

      // initialize count of elements on the left and right
      // Initially, all elements are on the right of the partition
      // so initialize the right counts with the total counts
      linal::Vector< size_t> state_counts_left( itr_left->Second());
      linal::Vector< size_t> state_counts_right( TOTAL_STATE_COUNTS - itr_left->Second());

      // keep track of whether the loop just skipped over a segment of descriptors with identical values for which there
      // were multiple states, in which case we need to re-evaluate the partition at the end of the string
      // otherwise, there is no need to re-evaluate the partition until the state changes
      bool found_identical_descriptors_with_different_states( false);

      //iterate over candidate set, which includes boundaries between possible split values,
      // keeping track of the size of left and right hand sides
      for
      (
        size_t left_size( 1), right_size( SORTED_DESCRIPTORS.GetSize() - 1);
        right_size > 0;
        ++itr_left, ++itr_right, ++left_size, --right_size
      )
      {
        // check that the partition values differ
        if( itr_left->First() == itr_right->First())
        {
          // partition fences are the same, check whether the states differed so that we can update
          // found_identical_descriptors_with_different_states
          if( found_identical_descriptors_with_different_states || !( itr_left->Second() == itr_right->Second()))
          {
            found_identical_descriptors_with_different_states = true;
          }
        }
        // partition values differed
        // If the states were identical, or found_identical_descriptors_with_different_states was true, then we need to
        // evaluate this as a possible partition
        else if( found_identical_descriptors_with_different_states || !( itr_left->Second() == itr_right->Second()))
        {
          // reset found_identical_descriptors_with_different_states, since these feature values differed
          found_identical_descriptors_with_different_states = false;

          // calculate partition_rating
          const float partition_rating
          (
            RatePartition( state_counts_left, left_size)
            + RatePartition( state_counts_right, right_size)
            - initial_rating
          );

          // keep best information gain
          if( partition_rating > best_split.GetSplitRating())
          {
            BCL_MessageDbg
            (
              "partition_rating: " + util::Format()( partition_rating)
              + " states: L/R/T/LT/RT: " + util::Format()( left_size) + "/"
              + util::Format()( right_size) + "/"
              + util::Format()( SORTED_DESCRIPTORS.GetSize()) + "/"
              + util::Format()( state_counts_left( 0)) + "/"
              + util::Format()( state_counts_right( 0))
            );
            const float best_split_value( ( itr_left->First() + itr_right->First()) / 2.0);
            best_split = DtreeBinaryPartition( FEATURE_INDEX, best_split_value, partition_rating);
          }
        }

        // update state counts
        state_counts_left += itr_right->Second();
        state_counts_right -= itr_right->Second();
      }

      return best_split;
    }

    //! @brief determines the total state counts
    //! @param DATA dataset
    linal::Vector< size_t> DtreeDataPartitionFunctionInterface::DetermineTotalStateCounts
    (
      const storage::Vector< FeatureResultAndState> &DATA
    )
    {
      // handle the special case off an empty dataset
      if( DATA.IsEmpty())
      {
        return linal::Vector< size_t>();
      }

      // dataset information
      const size_t number_states( DATA.FirstElement().GetResultState().GetSize());
      const size_t total_dataset_elements( DATA.GetSize());

      // cache # of items in each state on the right side of the partition (initially everything)
      linal::Vector< size_t> total_items_in_state( number_states, size_t( 0));
      for( size_t data_counter( 0); data_counter < total_dataset_elements; ++data_counter)
      {
        // add the state vector to the total_items_in_state vector
        total_items_in_state += DATA( data_counter).GetResultState();
      }

      BCL_MessageVrb( "Total items in state: " + util::Format()( total_items_in_state));

      // determine whether this node is already pure, e.g. all items are in a single state
      const size_t number_used_states
      (
        total_items_in_state.GetSize()
        - std::count( total_items_in_state.Begin(), total_items_in_state.End(), size_t( 0))
        - std::count( total_items_in_state.Begin(), total_items_in_state.End(), total_dataset_elements)
      );

      // return if the dataset was already pure
      if( number_used_states == 0)
      {
        return linal::Vector< size_t>();
      }

      return total_items_in_state;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_gini_index_data_partition_function.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "util/bcl_util_message.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DtreeGiniIndexDataPartitionFunction::s_Instance
    (
      util::Enumerated< DtreeDataPartitionFunctionInterface>::AddInstance( new DtreeGiniIndexDataPartitionFunction())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    DtreeGiniIndexDataPartitionFunction *DtreeGiniIndexDataPartitionFunction::Clone() const
    {
      return new DtreeGiniIndexDataPartitionFunction( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DtreeGiniIndexDataPartitionFunction::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DtreeGiniIndexDataPartitionFunction::GetAlias() const
    {
      static const std::string s_Name( "Gini");
      return s_Name;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief operator determines the best partition of the data
    //! @param DATA vector of feature result references to consider
    //! @return function returns how to partition to the data
    DtreeBinaryPartition DtreeGiniIndexDataPartitionFunction::operator()
    (
      const storage::Vector< FeatureResultAndState> &DATA
    ) const
    {
      // dataset information
      const size_t descriptor_count( DATA.FirstElement().GetFeature().GetSize());

      // cache # of items in each state on the right side of the partition (initially everything)
      linal::Vector< size_t> total_items_in_state( DetermineTotalStateCounts( DATA));
      if( total_items_in_state.GetSize() == 0)
      {
        // pure node, return an undefined split
        return DtreeBinaryPartition();
      }

      // object to hold the best split
      DtreeBinaryPartition best_split;

      for
      (
        size_t current_descriptor_index( 0);
        current_descriptor_index < descriptor_count;
        ++current_descriptor_index
      )
      {
        DtreeBinaryPartition split_for_this_descriptor
        (
          DetermineSplitValueAndRating
          (
            ExtractAndSortDescriptorAtIndex( current_descriptor_index, DATA),
            current_descriptor_index,
            total_items_in_state
          )
        );

        BCL_MessageVrb
        (
          "split_for_this_descriptor index,value,info gain: "
          + util::Format()( split_for_this_descriptor.GetFeatureIndex()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitValue()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitRating())
        );
        if
        (
          util::IsDefined( split_for_this_descriptor.GetFeatureIndex()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitValue()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitRating()) &&
          split_for_this_descriptor.GetSplitRating() > best_split.GetSplitRating()
        )
        {
          best_split = split_for_this_descriptor;
        }
      }

      // determine accuracy, which may be used to select which node to expand next
      best_split.DetermineAccuracy( DATA);
      return best_split;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DtreeGiniIndexDataPartitionFunction::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses gini index to decide which feature index to make the next decision based on see http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity"
      );
      return parameters;
    }

    //! @brief calculates information gain from counts of states in each segment
    //! @param STATE_COUNTS counts of each state (e.g. true, false, or 0,1,2, etc.) on one segment of the partition
    //! @param SIZE total number of items in the partition
    //! @return the information gain metric (unnormalized)
    float DtreeGiniIndexDataPartitionFunction::RatePartition
    (
      const linal::Vector< size_t> &STATE_COUNTS,
      const size_t &SIZE
    ) const
    {
      // compute the gini index
      float sum_gini( 0.0);
      for( const size_t *itr( STATE_COUNTS.Begin()), *itr_end( STATE_COUNTS.End()); itr != itr_end; ++itr)
      {
        sum_gini += math::Sqr( float( *itr) / float( SIZE));
      }
      return sum_gini * SIZE;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_information_gain_data_partition_function.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "util/bcl_util_message.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DtreeInformationGainDataPartitionFunction::s_Instance
    (
      util::Enumerated< DtreeDataPartitionFunctionInterface>::AddInstance( new DtreeInformationGainDataPartitionFunction())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    DtreeInformationGainDataPartitionFunction *DtreeInformationGainDataPartitionFunction::Clone() const
    {
      return new DtreeInformationGainDataPartitionFunction( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DtreeInformationGainDataPartitionFunction::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DtreeInformationGainDataPartitionFunction::GetAlias() const
    {
      static const std::string s_Name( "InformationGain");
      return s_Name;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief operator determines the best partition of the data
    //! @param DATA vector of feature result references to consider
    //! @return function returns how to partition to the data
    DtreeBinaryPartition DtreeInformationGainDataPartitionFunction::operator()
    (
      const storage::Vector< FeatureResultAndState> &DATA
    ) const
    {
      // dataset information
      const size_t descriptor_count( DATA.FirstElement().GetFeature().GetSize());

      // cache # of items in each state on the right side of the partition (initially everything)
      linal::Vector< size_t> total_items_in_state( DetermineTotalStateCounts( DATA));
      if( total_items_in_state.GetSize() == 0)
      {
        // pure node, return an undefined split
        return DtreeBinaryPartition();
      }

      // object to hold the best split
      DtreeBinaryPartition best_split;

      for
      (
        size_t current_descriptor_index( 0);
        current_descriptor_index < descriptor_count;
        ++current_descriptor_index
      )
      {
        DtreeBinaryPartition split_for_this_descriptor
        (
          DetermineSplitValueAndRating
          (
            ExtractAndSortDescriptorAtIndex( current_descriptor_index, DATA),
            current_descriptor_index,
            total_items_in_state
          )
        );

        BCL_MessageVrb
        (
          "split_for_this_descriptor index,value,info gain: "
          + util::Format()( split_for_this_descriptor.GetFeatureIndex()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitValue()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitRating())
        );
        if
        (
          util::IsDefined( split_for_this_descriptor.GetFeatureIndex()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitValue()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitRating()) &&
          split_for_this_descriptor.GetSplitRating() > best_split.GetSplitRating()
        )
        {
          best_split = split_for_this_descriptor;
        }
      }

      // determine accuracy, which may be used to select which node to expand next
      best_split.DetermineAccuracy( DATA);
      return best_split;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DtreeInformationGainDataPartitionFunction::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses raw information gain to decide which feature index to make the next decision based on see http://en.wikipedia.org/wiki/Information_gain"
      );
      return parameters;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief calculates information gain from counts of states in each segment
    //! @param STATE_COUNTS counts of each state (e.g. true, false, or 0,1,2, etc.) on one segment of the partition
    //! @param SIZE total number of items in the partition
    //! @return the information gain metric (unnormalized)
    float DtreeInformationGainDataPartitionFunction::RatePartition
    (
      const linal::Vector< size_t> &STATE_COUNTS,
      const size_t &SIZE
    ) const
    {
      // initialize a sum for the information gain of each state
      float information_gain( 0.0);

      // get the total number of elements in that state
      const float size_as_float( SIZE);

      for
      (
        auto itr( STATE_COUNTS.Begin()), itr_end( STATE_COUNTS.End());
        itr != itr_end;
        ++itr
      )
      {
        // only calculate information gain for states that were present in this partition
        if( *itr != 0 && *itr != SIZE) // information gain for presence of state
        {
          // information gain for each state (S) is
          // count( items in S) * log( probability of item in S in this partition)
          information_gain += *itr * std::log( *itr / size_as_float);

          // information gain for absence of state
          const float count_not_in_state( SIZE - *itr);
          information_gain += count_not_in_state * std::log( count_not_in_state / size_as_float);
        }
      }

      // return the information gain for this partition
      return information_gain;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_roc_data_partition_function.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DtreeRocDataPartitionFunction::s_Instance
    (
      util::Enumerated< DtreeDataPartitionFunctionInterface>::AddInstance( new DtreeRocDataPartitionFunction())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    //! m_MinimumAuROC = 0.5, m_MaximumAuROC = 0.95 according to (Hossain 2006)
    DtreeRocDataPartitionFunction::DtreeRocDataPartitionFunction() :
      m_MinimumAuROC( 0.5),
      m_MaximumAuROC( 0.95)
    {
    }

    //! @brief constructor taking all parameters
    //! @param MAX maximum of allowed ROC AUC
    //! @param MIN minimum of allowed ROC AUC
    DtreeRocDataPartitionFunction::DtreeRocDataPartitionFunction( float MIN, float MAX)
    {
      BCL_Assert( MIN < MAX, "MIN must be less than MAX");
      m_MinimumAuROC = MIN;
      m_MaximumAuROC = MAX;
    }

    //! copy constructor
    DtreeRocDataPartitionFunction *DtreeRocDataPartitionFunction::Clone() const
    {
      return new DtreeRocDataPartitionFunction( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DtreeRocDataPartitionFunction::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DtreeRocDataPartitionFunction::GetAlias() const
    {
      static const std::string s_Name( "ROC");
      return s_Name;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief operator determines the best partition of the data
    //! @param DATA vector of feature result references to consider
    //! @return function returns how to partition to the data
    DtreeBinaryPartition DtreeRocDataPartitionFunction::operator()
    (
      const storage::Vector< FeatureResultAndState> &DATA
    ) const
    {
      // get the total counts of each state
      const linal::Vector< size_t> total_state_counts( DetermineTotalStateCounts( DATA));

      // make sure this node wasn't already pure, if it was, just return the undefined split
      if( total_state_counts.GetSize() == 0)
      {
        return DtreeBinaryPartition();
      }

      // holds the best descriptor and AuROC as a pair and initializes to zero
      DtreeBinaryPartition best_split;
      float best_auroc( 0.0);

      // number of descriptors
      const size_t total_descriptors( DATA.FirstElement().GetFeature().GetSize());

      // number of results
      const size_t number_results( DATA.FirstElement().GetResultState().GetSize());

      // list of descriptor value and result bool for roc curve
      storage::Vector< storage::Pair< float, FeatureReference< size_t> > > descriptor_activity_pairs;

      // loop over all descriptors
      for( size_t current_index( 0); current_index < total_descriptors; ++current_index)
      {
        // list of descriptor value and result bool for roc curve
        descriptor_activity_pairs = ExtractAndSortDescriptorAtIndex( current_index, DATA);

        // compute minimum, maximum, and average roc curve values
        double min_roc( 1.0), max_roc( 0.0);
        math::RunningAverage< double> ave_roc;

        for( size_t result_number( 0); result_number < number_results; ++result_number)
        {
          storage::List< storage::Pair< double, bool> > descriptor_activity_pairs_double;
          // casting to double from float because that's what roc curve wants... will be changed in the future XXX
          for( size_t feature_id( 0), number_features( DATA.GetSize()); feature_id < number_features; ++feature_id)
          {
            descriptor_activity_pairs_double.PushBack();
            // store feature value
            descriptor_activity_pairs_double.LastElement().First() = descriptor_activity_pairs( feature_id).First();
            // store result as bool
            descriptor_activity_pairs_double.LastElement().Second()
              = descriptor_activity_pairs( feature_id).Second()( result_number);
          }

          // calculate AuROC for the current descriptor list and compare it to the saved max AuROC
          math::ROCCurve temp_roc_curve( descriptor_activity_pairs_double);
          // calculate AUC
          float temp_Au_roc_curve( temp_roc_curve.Integral());

          // fpr undefined roc curve values, continue
          if( !util::IsDefined( temp_Au_roc_curve))
          {
            continue;
          }

          // take max of distance from diagonal in ROC curve
          temp_Au_roc_curve = std::max( temp_Au_roc_curve, float( 1) - temp_Au_roc_curve);

          if( temp_Au_roc_curve < min_roc)
          {
            min_roc = temp_Au_roc_curve;
          }
          if( temp_Au_roc_curve > max_roc)
          {
            max_roc = temp_Au_roc_curve;
          }
          ave_roc += temp_Au_roc_curve;
        }
        // save a new descriptor with the new max AuROC
        if( ave_roc.GetAverage() > best_auroc && max_roc <= m_MaximumAuROC && min_roc >= m_MinimumAuROC)
        {
          best_split =
            DetermineSplitValueAndRating
            (
              descriptor_activity_pairs,
              current_index,
              total_state_counts
            );
          best_auroc = ave_roc.GetAverage();
        }
      }

      // return best descriptor auc
      BCL_MessageDbg( "best split is :" + util::Format()( best_split));

      // determine accuracy, which may be used to select which node to expand next
      best_split.DetermineAccuracy( DATA);

      // return the best split
      return best_split;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief rates by gini-index (denormalized
    //! @param STATE_COUNTS counts of each state (e.g. true, false, or 0,1,2, etc.) on one segment of the partition
    //! @param SIZE total number of items in the partition
    //! @return the # of correct classifications on this side
    float DtreeRocDataPartitionFunction::RatePartition
    (
      const linal::Vector< size_t> &STATE_COUNTS,
      const size_t &SIZE
    ) const
    {
      // compute the gini index
      float sum_gini( 0.0);
      for( const size_t *itr( STATE_COUNTS.Begin()), *itr_end( STATE_COUNTS.End()); itr != itr_end; ++itr)
      {
        sum_gini += math::Sqr( float( *itr) / float( SIZE));
      }
      return sum_gini * SIZE;

      // return the maximum value for this partition (the max # of correct classifications)
      // this was the method chosen in the hussan paper, unfortunately it results in all the actives being split into
      // separate nodes, resulting in a tree roughly 2x the size of the # of actives
      // It is ONLY appropriate for categorical data, it utterly fails when numerical data is used
      //return math::Statistics::MaximumValue( STATE_COUNTS.Begin(), STATE_COUNTS.End());
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DtreeRocDataPartitionFunction::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses information gain based on the receiver operator curve to decide which feature index to make the next decision with"
      );

      parameters.AddInitializer
      (
        "min area under ROC",
        "minimum area under the ROC curve before termination of iterations",
        io::Serialization::GetAgent( &m_MinimumAuROC),
        "0.5"
      );

      parameters.AddInitializer
      (
        "max area under ROC",
        "maximum area under the ROC curve (AuROC) before termination of iterations, based on Hossain paper to avoid overfitting",
        io::Serialization::GetAgent( &m_MaximumAuROC),
        "0.95"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_dtree_sequence_data_partition_function.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "util/bcl_util_message.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> DtreeSequenceDataPartitionFunction::s_Instance
    (
      util::Enumerated< DtreeDataPartitionFunctionInterface>::AddInstance( new DtreeSequenceDataPartitionFunction())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    DtreeSequenceDataPartitionFunction *DtreeSequenceDataPartitionFunction::Clone() const
    {
      return new DtreeSequenceDataPartitionFunction( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &DtreeSequenceDataPartitionFunction::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &DtreeSequenceDataPartitionFunction::GetAlias() const
    {
      static const std::string s_Name( "Sequence");
      return s_Name;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief operator determines the best partition of the data
    //! @param DATA vector of feature result references to consider
    //! @return function returns how to partition to the data
    DtreeBinaryPartition DtreeSequenceDataPartitionFunction::operator()
    (
      const storage::Vector< FeatureResultAndState> &DATA
    ) const
    {
      // dataset information
      const size_t descriptor_count( DATA.FirstElement().GetFeature().GetSize());

      // cache # of items in each state on the right side of the partition (initially everything)
      linal::Vector< size_t> total_items_in_state( DetermineTotalStateCounts( DATA));
      if( total_items_in_state.GetSize() == 0)
      {
        // pure node, return an undefined split
        return DtreeBinaryPartition();
      }

      // object to hold the best split
      DtreeBinaryPartition best_split;

      for
      (
        size_t current_descriptor_index( 0);
        current_descriptor_index < descriptor_count;
        ++current_descriptor_index
      )
      {
        DtreeBinaryPartition split_for_this_descriptor
        (
          DetermineSplitValueAndRating
          (
            ExtractAndSortDescriptorAtIndex( current_descriptor_index, DATA),
            current_descriptor_index,
            total_items_in_state
          )
        );

        BCL_MessageVrb
        (
          "split_for_this_descriptor index,value,info gain: "
          + util::Format()( split_for_this_descriptor.GetFeatureIndex()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitValue()) + ","
          + util::Format()( split_for_this_descriptor.GetSplitRating())
        );
        if
        (
          util::IsDefined( split_for_this_descriptor.GetFeatureIndex()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitValue()) &&
          util::IsDefined( split_for_this_descriptor.GetSplitRating()) &&
          split_for_this_descriptor.GetSplitRating() > best_split.GetSplitRating()
        )
        {
          best_split = split_for_this_descriptor;
        }
      }

      // determine accuracy, which may be used to select which node to expand next
      best_split.DetermineAccuracy( DATA);
      return best_split;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer DtreeSequenceDataPartitionFunction::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Allows only splits that leave at least one of the two resulting node pure. "
        "The resulting model will be a decision sequence rather than a tree"
      );
      return parameters;
    }

    //! @brief calculates information gain from counts of states in each segment
    //! @param STATE_COUNTS counts of each state (e.g. true, false, or 0,1,2, etc.) on one segment of the partition
    //! @param SIZE total number of items in the partition
    //! @return the information gain metric (unnormalized)
    float DtreeSequenceDataPartitionFunction::RatePartition
    (
      const linal::Vector< size_t> &STATE_COUNTS,
      const size_t &SIZE
    ) const
    {
      // get the total number of elements in that state
      float pure_class_counts( 0.0);

      for
      (
        auto itr( STATE_COUNTS.Begin()), itr_end( STATE_COUNTS.End());
        itr != itr_end;
        ++itr
      )
      {
        // only calculate information gain for states that were present in this partition
        if( *itr == 0 || *itr == SIZE) // information gain for presence of state
        {
          pure_class_counts += SIZE;
        }
      }
      return pure_class_counts;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_feature_data_set.hpp"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////
  // data //
  //////////

    // instantiate s_Instance
    template< typename t_DataType>
    const util::SiPtr< const util::ObjectInterface> FeatureDataSet< t_DataType>::s_Instance
    (
      GetObjectInstances().AddInstance( new FeatureDataSet< t_DataType>())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    template class BCL_API FeatureDataSet< float>;
    template class BCL_API FeatureDataSet< char>;

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_feature_label_set.h"

// includes from bcl - sorted alphabetically
#include "biol/bcl_biol_aa_base.h"
#include "chemistry/bcl_chemistry_atom_conformational_interface.h"
#include "descriptor/bcl_descriptor_base.h"
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_statistics.h"
#include "storage/bcl_storage_list.h"
#include "util/bcl_util_implementation.h"
#include "util/bcl_util_string_functions.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> FeatureLabelSet::s_Instance
    (
      GetObjectInstances().AddInstance( new FeatureLabelSet())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    FeatureLabelSet::FeatureLabelSet() :
      m_OrderedProperties(),
      m_Size( 0),
      m_OuterName( "Combine")
    {
    }

    //! @brief constructor from outer name
    //! @param NAME name for the set, usually a descriptor that takes a list of other descriptors
    FeatureLabelSet::FeatureLabelSet
    (
      const std::string &NAME,
      const util::Implementation< util::ImplementationInterface> &IMPL
    ) :
      m_OrderedProperties(),
      m_Size( 0),
      m_OuterName( NAME),
      m_ImplementationInterface
      (
        IMPL.IsDefined()
        ? util::Implementation< util::ImplementationInterface>( IMPL->Empty())
        : IMPL
      )
    {
      if( NAME.find( '!') != std::string::npos && !m_ImplementationInterface.IsDefined())
      {
        storage::Vector< std::string> split( util::SplitString( NAME, "!"));
        if( split.GetSize() > 1 && !split( 0).empty())
        {
          std::stringstream err_stream;
          const std::string impl_interface( split( 0));
          split.RemoveElements( 0, 1);
          if( m_ImplementationInterface.TryRead( util::ObjectDataLabel( impl_interface), err_stream))
          {
            m_OuterName = util::Join( "!", split);
          }
        }
        else
        {
          m_OuterName = split( 0);
        }
      }
    }

    //! @brief constructor from members
    //! @param NAME name for the set, usually a descriptor that takes a list of other descriptors
    //! @param PROPERTIES properties of the feature
    //! @param PROPERTY_SIZES sizes of those properties
    FeatureLabelSet::FeatureLabelSet
    (
      const std::string &NAME,
      const storage::Vector< util::ObjectDataLabel> &PROPERTIES,
      const storage::Vector< size_t> &PROPERTY_SIZES,
      const util::Implementation< util::ImplementationInterface> &IMPL
    ) :
      m_OrderedProperties(),
      m_Size( 0),
      m_OuterName( NAME),
      m_ImplementationInterface
      (
        IMPL.IsDefined()
        ? util::Implementation< util::ImplementationInterface>( IMPL->Empty())
        : IMPL
      )
    {
      BCL_Assert
      (
        PROPERTIES.GetSize() == PROPERTY_SIZES.GetSize(),
        "Must have a size for each property, but received "
        + util::Format()( PROPERTIES.GetSize()) + " properties and " + util::Format()( PROPERTY_SIZES.GetSize())
        + " sizes: " + util::Format()( PROPERTIES) + " " + util::Format()( PROPERTY_SIZES)
      );
      for
      (
        size_t property_id( 0), number_properties( PROPERTIES.GetSize());
        property_id < number_properties;
        ++property_id
      )
      {
        PushBack( PROPERTIES( property_id), PROPERTY_SIZES( property_id));
      }
      if( NAME.find( '!') != std::string::npos && !m_ImplementationInterface.IsDefined())
      {
        storage::Vector< std::string> split( util::SplitString( NAME, "!"));
        if( split.GetSize() > 1 && !split( 0).empty())
        {
          std::stringstream err_stream;
          const std::string impl_interface( split( 0));
          split.RemoveElements( 0, 1);
          if( m_ImplementationInterface.TryRead( util::ObjectDataLabel( impl_interface), err_stream))
          {
            m_OuterName = util::Join( "!", split);
          }
        }
        else
        {
          m_OuterName = split( 0);
        }
      }
    }

    //! @brief virtual copy constructor
    FeatureLabelSet *FeatureLabelSet::Clone() const
    {
      return new FeatureLabelSet( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &FeatureLabelSet::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief return the alias / name used when constructing this object
    //! @returnthe alias / name used when constructing this object
    const std::string &FeatureLabelSet::GetAlias() const
    {
      return m_OuterName;
    }

    //! @brief return the data label
    //! @return data label as string
    std::string FeatureLabelSet::GetString() const
    {
      return GetLabel().ToString();
    }

    //! @brief return a label with the descriptors in this
    //! @return a label with the descriptors in this
    util::ObjectDataLabel FeatureLabelSet::GetLabel() const
    {
      if( m_OuterName.empty() && m_OrderedProperties.GetSize() == size_t( 1))
      {
        // return the only descriptors label
        return m_OrderedProperties.FirstElement();
      }
      return util::ObjectDataLabel( m_OuterName, m_OrderedProperties);
    }

    //! @brief Get a vector containing all member data from this property as separate data labels
    const storage::Vector< util::ObjectDataLabel> &FeatureLabelSet::GetMemberLabels() const
    {
      return m_OrderedProperties;
    }

    //! @brief get the sizes of each property
    //! @return sizes of each property, in the same order as GetMemberLabels
    const storage::Vector< size_t> &FeatureLabelSet::GetPropertySizes() const
    {
      return m_PropertiesSizes;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief push back a property and its expected size
    //! @param PROPERTY_LABEL label describing the property
    //! @param PROPERTY_SIZE size of the label
    void FeatureLabelSet::PushBack
    (
      const util::ObjectDataLabel &PROPERTY_LABEL,
      const size_t &PROPERTY_SIZE
    )
    {
      // remove the split label set, if it is present
      if( m_SplitFeatures.IsDefined())
      {
        m_SplitFeatures = util::OwnPtr< FeatureLabelSet>();
      }

      // get the range for the last element in ordered properties, if applicable

      // add the property to the vector
      m_OrderedProperties.PushBack( PROPERTY_LABEL);

      // create a math::Range object to hold the range of this property
      math::Range< size_t> range
      (
        math::RangeBorders::e_LeftClosed, m_Size, m_Size + PROPERTY_SIZE, math::RangeBorders::e_RightClosed
      );

      // add the size to the total size
      m_Size += PROPERTY_SIZE;

      // add the size to the property size vector
      m_PropertiesSizes.PushBack( PROPERTY_SIZE);

      // add the property to the map
      m_PropertiesToRanges[ PROPERTY_LABEL] = range;

      BCL_Assert
      (
        m_OrderedProperties.GetSize() == m_PropertiesToRanges.GetSize(),
        "Duplicated object data labels are not allowed in feature label set. Duplicate label was: "
        + PROPERTY_LABEL.ToString()
      );
    }

    //! @param LABEL label of the desired property
    //! @return segment(s) of feature vector used by the given property
    storage::Vector< size_t> FeatureLabelSet::GetPropertyIndices( const util::ObjectDataLabel &LABEL) const
    {
      // look for the label in the map
      storage::Map< util::ObjectDataLabel, math::Range< size_t> >::const_iterator itr( m_PropertiesToRanges.Find( LABEL));

      // if the property is not found, consider whether it is a subproperty
      if( itr == m_PropertiesToRanges.End())
      {
        // property did not exist directly; try to retrieve it assuming it is a subproperty
        return LABEL == GetLabel() ? storage::CreateIndexVector( m_Size) : GetSubPropertyIndices( LABEL);
      }

      // return indices in the selected range
      return storage::CreateIndexVector( itr->second.GetMin(), itr->second.GetMax());
    }

    //! @brief create a feature label subset
    //! @param SUB_FEATURES indices of the complete feature to keep
    //! @return feature label set with the given sub features
    //! @note: if a descriptor has multiple return values and must be split, it is assumed that there is a
    //! meta-descriptor named Partial, that takes another descriptor and a parameter called indices, which contains
    //! the indices of the feature labels to keep.
    FeatureLabelSet FeatureLabelSet::CreateSubFeatureLabelSet( const storage::Vector< size_t> &SUB_FEATURES) const
    {
      FeatureLabelSet desired_features( m_OuterName, m_ImplementationInterface);

      // determine which features are needed
      storage::Vector< size_t> index_in_kept_features( m_Size, util::GetUndefined< size_t>());
      size_t counter( 0);
      for
      (
        storage::Vector< size_t>::const_iterator
          itr( SUB_FEATURES.Begin()), itr_end( SUB_FEATURES.End());
        itr != itr_end;
        ++itr, ++counter
      )
      {
        index_in_kept_features( *itr) = counter;
      }

      storage::Vector< util::ObjectDataLabel> new_labels( m_PropertiesToRanges.GetSize());
      storage::Vector< size_t> new_sizes( m_PropertiesToRanges.GetSize(), size_t( 0));

      storage::Map< util::ObjectDataLabel, size_t> label_to_original_index;
      counter = 0;
      for
      (
        util::ObjectDataLabel::const_iterator
          itr( m_OrderedProperties.Begin()), itr_end( m_OrderedProperties.End());
        itr != itr_end;
        ++itr, ++counter
      )
      {
        label_to_original_index[ *itr] = counter;
        // always add zero-sized properties
        if( m_PropertiesSizes( counter) == size_t( 0))
        {
          desired_features.PushBack( *itr, size_t( 0));
        }
      }

      // iterate over all feature
      for
      (
        storage::Map< util::ObjectDataLabel, math::Range< size_t> >::const_iterator
          itr( m_PropertiesToRanges.Begin()), itr_end( m_PropertiesToRanges.End());
        itr != itr_end;
        ++itr
      )
      {
        // get the size of this descriptor
        const size_t feature_size( itr->second.GetWidth());
        const size_t feature_start( itr->second.GetMin());

        // get the index of this feature in the original feature label set
        const size_t original_index( label_to_original_index[ itr->first]);

        storage::List< size_t> sub_features;
        for( size_t index( feature_start), index_end( itr->second.GetMax()); index < index_end; ++index)
        {
          if( util::IsDefined( index_in_kept_features( index)))
          {
            sub_features.PushBack( index - feature_start);
          }
        }

        // if part of this descriptor is to be kept, add it to the lists
        if( sub_features.GetSize())
        {
          new_sizes( original_index) = sub_features.GetSize();
          if( sub_features.GetSize() == feature_size)
          {
            // add the entire property to the desired features set
            new_labels( original_index) = CollapsePartials( itr->first);
          }
          else
          {
            // create the sub-feature label
            util::ObjectDataLabel subfeature
            (
              "Partial",
              storage::Vector< util::ObjectDataLabel>::Create
              (
                itr->first,
                util::ObjectDataLabel
                (
                  "indices",
                  io::Serialization::GetAgent( &sub_features)->GetLabel()
                )
              )
            );
            new_labels( original_index) = CollapsePartials( subfeature);
          }
        }
      }

      for( size_t label_index( 0), labels_size( new_sizes.GetSize()); label_index < labels_size; ++label_index)
      {
        if( new_sizes( label_index))
        {
          desired_features.PushBack( new_labels( label_index), new_sizes( label_index));
        }
      }

      return desired_features;
    }

    //! @brief create a feature label subset
    //! @param SKIP_ZERO_LENGTH_DESCRIPTORS true to ignore zero-length descriptors during the split. In this case, all
    //!        descriptors in the returned labels will have size == 1
    //! @return feature label set with each multi-column descriptor split using the Partial descriptor (vector index)
    //! @note: if a descriptor has multiple return values and must be split, it is assumed that there is a
    //! meta-descriptor named Partial, that takes another descriptor and a parameter called indices, which contains
    //! the indices of the feature labels to keep.
    FeatureLabelSet FeatureLabelSet::SplitFeatureLabelSet( const bool &SKIP_ZERO_LENGTH_DESCRIPTORS) const
    {
      const size_t number_zero_length_descriptors
      (
        SKIP_ZERO_LENGTH_DESCRIPTORS
        ? 0
        : std::count( m_PropertiesSizes.Begin(), m_PropertiesSizes.End(), size_t( 0))
      );
      storage::Vector< util::ObjectDataLabel> labels;
      labels.AllocateMemory( m_Size + number_zero_length_descriptors);

      storage::Vector< size_t> label_sizes( m_Size + number_zero_length_descriptors, size_t( 1));

      storage::Vector< size_t> sub_feature( 1, 0);
      util::OwnPtr< io::SerializationInterface> indices_container_label_sp
      (
        io::Serialization::GetAgent( &sub_feature)
      );

      storage::Vector< size_t>::const_iterator itr_sizes( m_PropertiesSizes.Begin());
      storage::Vector< size_t>::iterator itr_split_sizes( label_sizes.Begin());
      for
      (
        util::ObjectDataLabel::const_iterator
          itr( m_OrderedProperties.Begin()), itr_end( m_OrderedProperties.End());
        itr != itr_end;
        ++itr, ++itr_sizes, ++itr_split_sizes
      )
      {
        const size_t size( *itr_sizes);

        // handle single-value properties trivially
        if( size <= size_t( 1))
        {
          if( size == size_t( 0))
          {
            if( SKIP_ZERO_LENGTH_DESCRIPTORS)
            {
              --itr_split_sizes;
              continue;
            }
            else
            {
              *itr_split_sizes = 0;
            }
          }
          labels.PushBack( CollapsePartials( *itr));
          continue;
        }

        for( size_t id( 0); id < size; ++id)
        {
          sub_feature( 0) = id;
          // create the sub-feature label
          util::ObjectDataLabel subfeature
          (
            "Partial",
            storage::Vector< util::ObjectDataLabel>::Create
            (
              *itr,
              util::ObjectDataLabel( "indices", indices_container_label_sp->GetLabel())
            )
          );

          labels.PushBack( CollapsePartials( subfeature));
        }
      }

      return FeatureLabelSet( m_OuterName, labels, label_sizes, m_ImplementationInterface);
    }

    //! @brief get common overlap of features in this and given FeatureLabelSets
    FeatureLabelSet FeatureLabelSet::GetCommonFeatures( const FeatureLabelSet &COMPARE) const
    {
      std::set< util::ObjectDataLabel> labels_this
      (
        m_OrderedProperties.Begin(),
        m_OrderedProperties.End()
      );

      std::set< util::ObjectDataLabel> labels_that
      (
        COMPARE.m_OrderedProperties.Begin(),
        COMPARE.m_OrderedProperties.End()
      );

      std::list< util::ObjectDataLabel> common_labels_list;

      std::set_intersection
      (
        labels_this.begin(),
        labels_this.end(),
        labels_that.begin(),
        labels_that.end(),
        std::inserter( common_labels_list, common_labels_list.begin())
      );

      storage::Vector< util::ObjectDataLabel> common_labels( common_labels_list.begin(), common_labels_list.end());
      storage::Vector< size_t> common_sizes;
      for
      (
        util::ObjectDataLabel::const_iterator
          itr( common_labels.Begin()), itr_end( common_labels.End());
        itr != itr_end;
        ++itr
      )
      {
        common_sizes.PushBack( m_PropertiesToRanges.Find( *itr)->second.GetWidth());
      }
      return FeatureLabelSet( m_OuterName, common_labels, common_sizes, m_ImplementationInterface);
    }

    //! @brief merge two object data labels, respecting split descriptors
    //! @param LABEL_A, LABEL_B the object data labels of interest
    //! @return the merged labels
    util::ObjectDataLabel FeatureLabelSet::MergeConsideringPartials
    (
      const util::ObjectDataLabel &LABEL_A,
      const util::ObjectDataLabel &LABEL_B
    )
    {
      // for each descriptor that is a partial (e.g. column of another descriptor):
      // add the entry into the map, along with the corresponding descriptor indices
      // for each descriptor that is not a partial,
      // add the entry into the map without any indices
      storage::Map< util::ObjectDataLabel, storage::Set< size_t> > descriptor_indices;
      storage::Set< util::ObjectDataLabel> complete_descriptors;
      for
      (
        storage::Vector< util::ObjectDataLabel>::const_iterator itr_a( LABEL_A.Begin()), itr_a_end( LABEL_A.End());
        itr_a != itr_a_end;
        ++itr_a
      )
      {
        // attempt to decompose the descriptor into a partial
        storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> > decomposed( DecomposePartial( *itr_a));
        if( decomposed.First().IsEmpty())
        {
          // complete descriptor, add it to the set
          complete_descriptors.Insert( *itr_a);
          // if the descriptor was in the map, remove it
          descriptor_indices.Erase( *itr_a);
        }
        else if( !complete_descriptors.Contains( decomposed.First()))
        {
          const storage::Vector< size_t> &descriptor_ids( decomposed.Second());
          // partial descriptor, add the indices to the map
          descriptor_indices[ decomposed.First()].InsertElements( descriptor_ids.Begin(), descriptor_ids.End());
        }
      }

      for
      (
        storage::Vector< util::ObjectDataLabel>::const_iterator itr_b( LABEL_B.Begin()), itr_b_end( LABEL_B.End());
        itr_b != itr_b_end;
        ++itr_b
      )
      {
        // attempt to decompose the descriptor into a partial
        storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> > decomposed( DecomposePartial( *itr_b));
        if( decomposed.First().IsEmpty())
        {
          // complete descriptor, add it to the set
          complete_descriptors.Insert( *itr_b);
          // if the descriptor was in the map, remove it
          descriptor_indices.Erase( *itr_b);
        }
        else if( !complete_descriptors.Contains( decomposed.First()))
        {
          const storage::Vector< size_t> &descriptor_ids( decomposed.Second());
          // partial descriptor, add the indices to the map
          descriptor_indices[ decomposed.First()].InsertElements( descriptor_ids.Begin(), descriptor_ids.End());
        }
      }

      // get all the complete descriptors
      storage::Vector< util::ObjectDataLabel> descriptors( complete_descriptors.Begin(), complete_descriptors.End());

      storage::Vector< size_t> sub_feature( 1, 0);
      util::OwnPtr< io::SerializationInterface> indices_container_label_sp
      (
        io::Serialization::GetAgent( &sub_feature)
      );

      // add all other descriptors from the map
      for
      (
        storage::Map< util::ObjectDataLabel, storage::Set< size_t> >::const_iterator
          itr( descriptor_indices.Begin()), itr_end( descriptor_indices.End());
        itr != itr_end;
        ++itr
      )
      {
        // copy the indices
        sub_feature = storage::Vector< size_t>( itr->second.Begin(), itr->second.End());

        // compose the partial descriptor
        util::ObjectDataLabel subfeature
        (
          "Partial",
          storage::Vector< util::ObjectDataLabel>::Create
          (
            itr->first,
            util::ObjectDataLabel( "indices", indices_container_label_sp->GetLabel())
          )
        );
        descriptors.PushBack( CollapsePartials( subfeature));
      }
      return util::ObjectDataLabel( LABEL_A.GetName(), LABEL_A.GetValue(), descriptors);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &FeatureLabelSet::Read( std::istream &ISTREAM)
    {
      // load the label
      util::ObjectDataLabel label( ISTREAM);

      // load the sizes of each property
      storage::Vector< size_t> property_sizes;
      ISTREAM >> property_sizes;

      m_ImplementationInterface.Reset();
      if( label.GetValue().find( '!') == std::string::npos)
      {
        // recreate this object with the properties and their sizes
        *this = FeatureLabelSet( label.GetValue(), label.GetArguments(), property_sizes);
      }
      else
      {
        storage::Vector< std::string> split( util::SplitString( label.GetValue(), "!"));
        std::stringstream err_stream;
        const std::string impl_interface( split( 0));
        split.RemoveElements( 0, 1);
        if( m_ImplementationInterface.TryRead( util::ObjectDataLabel( impl_interface), err_stream))
        {
          // valid object type
          *this =
            FeatureLabelSet
            (
              util::Join( "!", split),
              label.GetArguments(),
              property_sizes,
              m_ImplementationInterface
            );
        }
        else
        {
          // invalid object type
          BCL_MessageStd
          (
            "Warning; descriptor namespace with label: " + impl_interface
            + " could not be read; selection of descriptor subsets will not work if any descriptors had implicit / "
            "default values given. error output: " + err_stream.str()
          );
          *this = FeatureLabelSet( label.GetValue(), label.GetArguments(), property_sizes);
        }
      }

      // remove the split label set, if it is present
      if( m_SplitFeatures.IsDefined())
      {
        m_SplitFeatures = util::OwnPtr< FeatureLabelSet>();
      }

      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &FeatureLabelSet::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      OSTREAM <<
        util::ObjectDataLabel
        (
          ( m_ImplementationInterface.IsDefined() ? m_ImplementationInterface->GetAlias() : "") + "!" + m_OuterName,
          m_OrderedProperties
        ).ToString( 120, INDENT, 1) << '\n';
      OSTREAM << m_PropertiesSizes;

      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief helper function to decompose a partial into the innermost object data label and the vector of indices
    //! @param LABEL the partial label of interest
    //! @return a pair containing the main descriptors object data label and the indices for the partial
    storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >
      FeatureLabelSet::DecomposePartial( const util::ObjectDataLabel &LABEL)
    {
      static const std::string s_partial( "Partial"), s_indices( "indices");

      // indices property, once found
      util::ObjectDataLabel::const_iterator itr_indices_property( LABEL.End());

      // storage for ptr to main property, once found
      util::ObjectDataLabel::const_iterator itr_main_property( LABEL.End());

      // first, test whether the property is a subproperty; which must have 2 arguments
      if( LABEL.GetValue() == s_partial && LABEL.GetNumberArguments() == size_t( 2))
      {
        itr_indices_property = LABEL.FindValue( s_indices, false);
        itr_main_property = LABEL.Begin();
        if( itr_indices_property == LABEL.Begin())
        {
          ++itr_main_property;
        }
      }

      // label was not a partial
      if( itr_indices_property == LABEL.End())
      {
        return storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >();
      }

      storage::Vector< size_t> indices_main_property;
      util::OwnPtr< io::SerializationInterface> indices_container_label_sp
      (
        io::Serialization::GetAgent( &indices_main_property)
      );

      std::stringstream err_stream;
      BCL_Assert
      (
        indices_container_label_sp->TryRead( *itr_indices_property, err_stream),
        "Non-numeric value in sub-property indices: " + err_stream.str()
      );
      return
        storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >( *itr_main_property, indices_main_property);
    }

    namespace
    {
      //! @brief helper function to determine synonyms for a given object data label by trying to initialize a descriptor
      //!        from the given label
      //! @param LABEL the label of interest
      //! @param SYNONYMS_CONTAINER container in which to store synonyms that are different from LABEL
      template< typename t_ObjectType, typename t_ReturnType>
      void TryInterpretingAsDescriptor
      (
        const util::ObjectDataLabel &LABEL,
        storage::Set< util::ObjectDataLabel> &SYNONYMS_CONTAINER
      )
      {
        std::ostringstream err_stream;
        util::Implementation< descriptor::Base< t_ObjectType, t_ReturnType> > impl( LABEL, err_stream);
        if( impl.IsDefined())
        {
          SYNONYMS_CONTAINER.Insert( LABEL);
        }
      }
    }

    //! @brief get a vector with all known synonyms for a given label, created from trying to update the data label
    //!        with all template instances of descriptor::Base
    //! @param LABEL the label to retrieve all synonyms for. These are primarily necessary when
    storage::Set< util::ObjectDataLabel> FeatureLabelSet::GetSynonyms( const util::ObjectDataLabel &LABEL)
    {
      storage::Set< util::ObjectDataLabel> synonyms;
      TryInterpretingAsDescriptor< chemistry::AtomConformationalInterface, char>( LABEL, synonyms);
      TryInterpretingAsDescriptor< chemistry::AtomConformationalInterface, float>( LABEL, synonyms);
      TryInterpretingAsDescriptor< biol::AABase, char>( LABEL, synonyms);
      TryInterpretingAsDescriptor< biol::AABase, float>( LABEL, synonyms);
      TryInterpretingAsDescriptor< char, char>( LABEL, synonyms);
      TryInterpretingAsDescriptor< char, float>( LABEL, synonyms);
      synonyms.Erase( LABEL);
      return synonyms;
    }

    //! @brief clean a label that might have multiple levels of partials
    //! @param LABEL the old label to consider
    util::ObjectDataLabel FeatureLabelSet::CollapsePartials( const util::ObjectDataLabel &LABEL)
    {
      static const std::string s_partial( "Partial"), s_indices( "indices");
      if( LABEL.GetValue() != s_partial || LABEL.GetNumberArguments() != size_t( 2))
      {
        return LABEL;
      }

      // decompose the partial into partial and indices
      storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >
        decomposed_partial_outer( DecomposePartial( LABEL));

      // determine whether there was a partial
      if( decomposed_partial_outer.First().IsEmpty() || decomposed_partial_outer.Second().IsEmpty())
      {
        return LABEL;
      }

      // find the sub-partials indices
      storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >
        decomposed_partial_inner( DecomposePartial( decomposed_partial_outer.First()));

      // no sub-label to extract
      if( decomposed_partial_inner.First().IsEmpty() || decomposed_partial_inner.Second().IsEmpty())
      {
        return LABEL;
      }

      // now perform the mapping
      storage::Vector< size_t> &partial_indices_outer( decomposed_partial_outer.Second());
      const storage::Vector< size_t> &partial_indices_inner( decomposed_partial_inner.Second());
      for
      (
        storage::Vector< size_t>::iterator
          itr_outer( partial_indices_outer.Begin()), itr_outer_end( partial_indices_outer.End());
        itr_outer != itr_outer_end;
        ++itr_outer
      )
      {
        BCL_Assert( *itr_outer < partial_indices_inner.GetSize(), "Invalid partial!");
        *itr_outer = partial_indices_inner( *itr_outer);
      }

      // create the serialization handler for the partials' indices
      util::OwnPtr< io::SerializationInterface> partial_indices_outer_handler
      (
        io::Serialization::GetAgent( &partial_indices_outer)
      );

      // create the new label with the remapped indices
      storage::Vector< util::ObjectDataLabel> new_arguments( 2);
      new_arguments( 0) = decomposed_partial_inner.First();
      new_arguments( 1) = partial_indices_outer_handler->GetLabel();
      new_arguments( 1).SetValue( s_indices);
      util::ObjectDataLabel new_label( s_partial, new_arguments);
      if( new_arguments( 0).GetValue() == s_partial)
      {
        return CollapsePartials( new_label);
      }
      return new_label;
    }

    //! @brief retrieve the indices of a sub-property
    //! @param LABEL property of the form Partial(x,indices(y1,y2,...,yn))
    //! @return y1,y2,...yn of the property x, if it could be located
    storage::Vector< size_t> FeatureLabelSet::GetSubPropertyIndices( const util::ObjectDataLabel &LABEL) const
    {
      // decompose the partial into partial and indices
      storage::Pair< util::ObjectDataLabel, storage::Vector< size_t> >
        decomposed_partial( DecomposePartial( LABEL));

      if( decomposed_partial.First().IsEmpty())
      {
        if( !m_ImplementationInterface.IsDefined())
        {
          BCL_Exit( LABEL.ToString() + " was not found in feature label set " + GetString(), -1);
        }
        util::Implementation< util::ImplementationInterface> impl_copy( m_ImplementationInterface.HardCopy());
        std::stringstream err_stream;
        BCL_Assert
        (
          impl_copy->TryRead( LABEL, err_stream),
          LABEL.ToString() + " was not found in feature label set " + GetString()
          + " and could not be standardized by " + impl_copy.GetString() + " because: "
          + err_stream.str()
        );
        util::ObjectDataLabel new_label( impl_copy->GetLabel());
        if( new_label != LABEL)
        {
          return GetPropertyIndices( new_label);
        }
      }

      // look for the label in the map
      storage::Map< util::ObjectDataLabel, math::Range< size_t> >::const_iterator itr_map
      (
        m_PropertiesToRanges.Find( decomposed_partial.First())
      );
      storage::Vector< size_t> indices_main_property;
      if( itr_map != m_PropertiesToRanges.End())
      {
        indices_main_property = GetPropertyIndices( decomposed_partial.First());

        // reorder to whatever order the indices were given in
        indices_main_property.Reorder( decomposed_partial.Second());
      }
      else
      {
        // this guarantees that any sub-features are located

        // create a feature label set for the requested feature, fully split
        FeatureLabelSet feature_for_label
        (
          "",
          storage::Vector< util::ObjectDataLabel>( size_t( 1), LABEL),
          storage::Vector< size_t>( size_t( 1), decomposed_partial.Second().GetSize()),
          m_ImplementationInterface
        );
        if( feature_for_label.GetSize() > size_t( 1))
        {
          feature_for_label = feature_for_label.SplitFeatureLabelSet();
        }

        if
        (
          math::Statistics::MaximumValue( m_PropertiesSizes.Begin(), m_PropertiesSizes.End()) > size_t( 1)
          || math::Statistics::MaximumValue( feature_for_label.GetPropertySizes().Begin(), feature_for_label.GetPropertySizes().End()) > size_t( 1)
        )
        {
          // create a split feature label set
          if( !m_SplitFeatures.IsDefined())
          {
            m_SplitFeatures = util::OwnPtr< FeatureLabelSet>( SplitFeatureLabelSet().Clone());
          }

          // get the common features
          for
          (
            storage::Vector< util::ObjectDataLabel>::const_iterator
              itr_features_label( feature_for_label.GetMemberLabels().Begin()),
              itr_features_label_end( feature_for_label.GetMemberLabels().End());
            itr_features_label != itr_features_label_end;
            ++itr_features_label
          )
          {
            indices_main_property.Append( m_SplitFeatures->GetPropertyIndices( *itr_features_label));
          }
        }
        else
        {
          BCL_Exit( "Could not locate property: " + LABEL.ToString() + " within " + GetString(), -1);
        }
      }

      return indices_main_property;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_feature_result_and_state.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> FeatureResultAndState::s_Instance
    (
      GetObjectInstances().AddInstance( new FeatureResultAndState())
    );

    //! @brief default constructor
    FeatureResultAndState::FeatureResultAndState() :
      m_Feature(),
      m_Result(),
      m_ResultState()
    {
    }

    //! @brief constructor from members
    FeatureResultAndState::FeatureResultAndState
    (
      const FeatureReference< float>  &FEATURE,
      const FeatureReference< float>  &RESULT,
      const FeatureReference< size_t> &RESULT_STATE
    ) :
      m_Feature( FEATURE),
      m_Result( RESULT),
      m_ResultState( RESULT_STATE)
    {
    }

    //! @brief Clone function
    //! @return pointer to new FeatureResultAndState
    FeatureResultAndState *FeatureResultAndState::Clone() const
    {
      return new FeatureResultAndState( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &FeatureResultAndState::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the features
    //! @return the features
    const FeatureReference< float> &FeatureResultAndState::GetFeature() const
    {
      return m_Feature;
    }

    //! @brief get the result
    //! @return the result
    const FeatureReference< float> &FeatureResultAndState::GetResult() const
    {
      return m_Result;
    }

    //! @brief get the result state
    //! @return the result state
    const FeatureReference< size_t> &FeatureResultAndState::GetResultState() const
    {
      return m_ResultState;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &FeatureResultAndState::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_Feature, ISTREAM);
      io::Serialize::Read( m_Result, ISTREAM);
      io::Serialize::Read( m_ResultState, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &FeatureResultAndState::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_Feature, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Result, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ResultState, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_feature_label_set.h"
#include "model/bcl_model_has_labels_base.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void HasLabelsBase::SetFeatures( const FeatureLabelSet &CODE)
    {
      m_FeatureCodeObject = CODE;
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void HasLabelsBase::SetResults( const FeatureLabelSet &CODE)
    {
      m_ResultCodeObject = CODE;
    }

    //! @brief Set the code / label for the ids (3rd part) of the data set
    //! @param CODE the new code
    void HasLabelsBase::SetIds( const FeatureLabelSet &CODE)
    {
      m_IdCodeObject = CODE;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @return the code / label for the feature (1st part) of the data set
    const FeatureLabelSet &HasLabelsBase::GetFeatureCode() const
    {
      return m_FeatureCodeObject;
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    const FeatureLabelSet &HasLabelsBase::GetResultCode() const
    {
      return m_ResultCodeObject;
    }

    //! @brief Get the code / label for the ids (3rd part) of the data set
    //! @return the code / label for the ids (3rd part) of the data set
    const FeatureLabelSet &HasLabelsBase::GetIdCode() const
    {
      return m_IdCodeObject;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_interface_retrieve_from_file.h"

// includes from bcl - sorted alphabetically
#include "command/bcl_command_flag_interface.h"
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "model/bcl_model.h"
#include "sched/bcl_sched_mutex.h"
#include "util/bcl_util_sh_ptr_list.h"
#include "util/bcl_util_stopwatch.h"
#include "util/bcl_util_time.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! @brief ExtensionType as string
    //! @param INIT_TYPE the ExtensionType
    //! @return the string for the INIT_TYPE
    const std::string &InterfaceRetrieveFromFile::GetExtensionTypeDescriptor( const ExtensionType &TYPE)
    {
      static const std::string s_extensions[] =
      {
        "model",
        "descriptor",
        "result",
        "info",
        GetStaticClassName< ExtensionType>()
      };
      return s_extensions[ size_t( TYPE)];
    }

    //! format to convert Key to string
    const util::Format InterfaceRetrieveFromFile::s_KeyToString
    (
      util::Format().W( 6).R().Fill( '0')
    );

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> InterfaceRetrieveFromFile::s_Instance
    (
      util::Enumerated< RetrieveInterface>::AddInstance
      (
        new InterfaceRetrieveFromFile()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    InterfaceRetrieveFromFile::InterfaceRetrieveFromFile() :
      m_DirectoryName(),
      m_FilePrefix( ""),
      m_SelectBestModels( false)
    {
    }

    //! @brief construct from directory name and file prefix
    //! @param DIRECTORY_NAME name of directory, e.g. /home/user/models/
    //! @param FILE_PREFIX prefix to all files for this model
    InterfaceRetrieveFromFile::InterfaceRetrieveFromFile
    (
      const std::string &DIRECTORY_NAME,
      const std::string &FILE_PREFIX
    ) :
      m_DirectoryName( DIRECTORY_NAME),
      m_Directory( m_DirectoryName),
      m_FilePrefix( FILE_PREFIX),
      m_SelectBestModels( false)
    {
      ReadInitializerSuccessHook( util::ObjectDataLabel(), util::GetLogger());
    }

    //! @brief Clone function
    //! @return pointer to new InterfaceRetrieveFromFile
    InterfaceRetrieveFromFile *InterfaceRetrieveFromFile::Clone() const
    {
      return new InterfaceRetrieveFromFile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &InterfaceRetrieveFromFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &InterfaceRetrieveFromFile::GetAlias() const
    {
      static const std::string s_Name( "File");
      return s_Name;
    }

    //! @brief number of objects in source
    //! @return number of objects with that source prefix
    size_t InterfaceRetrieveFromFile::GetSize() const
    {
      return GetAllKeys().GetSize();
    }

    //! @brief get all keys for given range
    //! @param RANGE range of key values
    //! @return all keys of given prefix
    storage::Vector< std::string> InterfaceRetrieveFromFile::GetKeys( const math::Range< size_t> &RANGE) const
    {
      // keys
      storage::Vector< std::string> keys( GetAllKeys());
      storage::Vector< std::string> valid_keys;

      for
      (
        storage::Vector< std::string>::const_iterator itr( keys.Begin()), itr_end( keys.End());
        itr != itr_end;
        ++itr
      )
      {
        const size_t key_id( util::ConvertStringToNumericalValue< size_t>( *itr));

        // check for valid id
        if( RANGE.IsWithin( key_id))
        {
          valid_keys.PushBack( *itr);
        }
      }

      // return valid keys
      return valid_keys;
    }

    //! @brief get all keys for given source
    //! @return all keys of given prefix
    storage::Vector< std::string> InterfaceRetrieveFromFile::GetAllKeys() const
    {
      if( !m_Key.empty())
      {
        // singular key, just return it
        return storage::Vector< std::string>( size_t( 1), m_Key);
      }

      // cache
      static sched::Mutex s_mutex;
      static storage::Map< util::ObjectDataLabel, storage::Vector< std::string> > s_keys;

      s_mutex.Lock();
      storage::Vector< std::string> &keys( s_keys[ GetLabel()]);

      if( !keys.IsEmpty())
      {
        s_mutex.Unlock();
        return keys;
      }

      // directory content
      storage::List< io::DirectoryEntry> dir_entries;
      for
      (
        io::StreamBufferClasses::const_iterator
          itr_compression( io::GetStreamBufferClasses().Begin()),
          itr_compression_end( io::GetStreamBufferClasses().End());
        itr_compression != itr_compression_end;
        ++itr_compression
      )
      {
        dir_entries.Append
        (
          m_Directory.ListEntries
          (
            io::Directory::e_File,
            m_FilePrefix,
            ( **itr_compression)->AddExtension( "." + GetExtensionTypeDescriptor( e_Model))
          )
        );
      }

      // iterate over all entries
      for( storage::List< io::DirectoryEntry>::const_iterator itr( dir_entries.Begin()), itr_end( dir_entries.End()); itr != itr_end; ++itr)
      {
        // filename without extension
        std::string key( io::File::RemoveLastExtension( io::File::RemoveCompressionExtension( itr->GetName())));

        // extract the key
        key.erase( 0, std::string( m_FilePrefix).length());

        // key needs to be valid
        if( IsValidKey( key))
        {
          keys.PushBack( key);
        }
      }
      keys.Sort( std::less< std::string>());

      if( m_SelectBestModels)
      {
        keys = FindBestModelForEachIndependentSet( keys);
      }
      s_mutex.Unlock();

      return keys;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief get model
    //! @param KEY key identifier for specific smallmolecule in given source like "000001"
    //! @return shptr to smallmolecule of interest, undefined if there is no such smallmolecule
    RetrieveInterface::t_ModelPtr InterfaceRetrieveFromFile::Retrieve( const std::string &KEY) const
    {
      // model
      util::ShPtr< Interface> sp_interface;

      const std::string key( s_KeyToString( KEY));

      // check the key
      if( !IsValidKey( key))
      {
        BCL_MessageCrt( KEY + " is not a valid model key should be 6 integers, e.g. 123456");
        return RetrieveInterface::t_ModelPtr();
      }

      const std::string basename
      (
        m_AbsolutePath + "/" + m_FilePrefix + key + "." + GetExtensionTypeDescriptor( e_Model)
      );

      // if storing models ever becomes a memory issue, move this into another class and add a reset function
      // As of this writing, memory usage for models is typically trivial compared with dataset storage
      static sched::Mutex s_mutex;
      static storage::Map< std::string, RetrieveInterface::t_ModelPtr> s_results;

      s_mutex.Lock();
      RetrieveInterface::t_ModelPtr &interface( s_results[ basename]);
      if( interface.IsDefined())
      {
        s_mutex.Unlock();
        return t_ModelPtr( interface.operator ->(), false);
      }
      // filename and check if exists
      const std::string filename( Filename( key, e_Model));

      // check if file with that name exists
      if( !io::DirectoryEntry( filename).DoesExist())
      {
        BCL_MessageCrt( "could not find model " + filename);
        s_mutex.Unlock();
        return RetrieveInterface::t_ModelPtr();
      }

      BCL_MessageVrb( "Reading model with key = " + key + " at " + filename);
      // retrieve stored model
      io::IFStream input;

      static util::Stopwatch s_timer( "Reading models", util::Time( 0, 500), util::Message::e_Verbose, true, false);
      s_timer.Start();
      io::File::MustOpenIFStream( input, filename);
      io::Serialize::Read( sp_interface, input);
      io::File::CloseClearFStream( input);
      s_timer.Stop();

      // return retrieved small molecule
      interface = RetrieveInterface::t_ModelPtr( sp_interface->Clone());
      s_mutex.Unlock();
      return t_ModelPtr( interface.operator ->(), false);
    }

    //! @brief get result descriptor for the storage
    //! @return ObjectDataLabel for the results in this storage
    util::ObjectDataLabel InterfaceRetrieveFromFile::RetrieveResultDescriptor() const
    {
      static sched::Mutex s_mutex;
      static storage::Map< std::string, util::ObjectDataLabel> s_results;

      s_mutex.Lock();
      util::ObjectDataLabel &result( s_results[ m_AbsolutePath + "/" + m_FilePrefix]);
      if( !result.IsEmpty())
      {
        s_mutex.Unlock();
        return result;
      }
      result = this->RetrieveResultDescriptor( Filename( "", e_Result));
      s_mutex.Unlock();
      return result;
    }

    //! @brief get result descriptor located in the given filename
    //! @param FILENAME filename in which the results descriptor is located
    //! @return ObjectDataLabel for the results in this storage
    util::ObjectDataLabel InterfaceRetrieveFromFile::RetrieveResultDescriptor
    (
      const std::string &FILENAME
    )
    {
      // check if file with that name exists
      if( !io::DirectoryEntry( FILENAME).DoesExist())
      {
        return util::ObjectDataLabel();
      }

      // create factory and retrieve protein model
      io::IFStream input;

      // check that the writer's lock file isn't currently locked
      const io::DirectoryEntry lock_file( FILENAME + ".lock");
      while( lock_file.DoesExist())
      {
        // sleep a millisecond and check again
        util::Time::Delay( util::Time( 0, 1));
      }

      // clean up the path name for purposes of creating a semaphore
      io::File::MustOpenIFStream( input, FILENAME);
      util::ObjectDataLabel descriptors( input);
      io::File::CloseClearFStream( input);

      // return retrieved descriptor set
      return descriptors;
    }

    //! @brief get descriptor set of model by key
    //! @param KEY key identifier for specific model/descriptor set in given source
    //! @return shptr to descriptorset of interest
    util::ObjectDataLabel
    InterfaceRetrieveFromFile::RetrieveDescriptorSet( const std::string &KEY) const
    {
      // check the key
      if( !IsValidKey( KEY))
      {
        return util::ObjectDataLabel();
      }

      // filename and check if exists
      std::string filename( Filename( KEY, e_Descriptor));

      // check if file with that name exists
      if( !io::DirectoryEntry( filename).DoesExist())
      {
        // Often the descriptors are identical for all models in a given directory. In this case, allow there to be a
        // default descriptors file in the folder to avoid duplication
        filename = Filename( "", e_Descriptor);
        if( !io::DirectoryEntry( filename).DoesExist())
        {
          return util::ObjectDataLabel();
        }
      }

      // create factory and retrieve protein model
      io::IFStream input;
      io::File::MustOpenIFStream( input, filename);
      util::ObjectDataLabel descriptors( input);
      io::File::CloseClearFStream( input);

      // return retrieved descriptor set
      return descriptors;
    }

    //! @brief get cv info of model by key
    //! @param KEY key identifier for specific model/descriptor set in given source
    //! @return cross validation information for the model
    CrossValidationInfo InterfaceRetrieveFromFile::RetrieveCVInfo( const std::string &KEY) const
    {
      // check the key
      if( !IsValidKey( KEY))
      {
        return CrossValidationInfo();
      }

      // filename and check if exists
      const std::string filename( Filename( KEY, e_Info));

      // check if file with that name exists
      if( !io::DirectoryEntry( filename).DoesExist())
      {
        return CrossValidationInfo();
      }

      // return retrieved cv info
      return CrossValidationInfo( filename);
    }

    //! @brief get ensemble of descriptors associated each associated with a model
    //! @return shptr to list of descriptors of interest
    storage::List< util::ObjectDataLabel> InterfaceRetrieveFromFile::RetrieveEnsembleDescriptors() const
    {
      // get all keys
      storage::Vector< std::string> keys( GetAllKeys());

      // list for retrieved descriptors
      storage::List< util::ObjectDataLabel> descriptors;

      for
      (
        storage::Vector< std::string>::const_iterator itr_keys( keys.Begin()), itr_keys_end( keys.End());
        itr_keys != itr_keys_end;
        ++itr_keys
      )
      {
        // insert descriptor set according to key
        descriptors.PushBack( RetrieveDescriptorSet( *itr_keys));
      }

      // return list with descriptors
      return descriptors;
    }

    //! @brief get ensemble of descriptors associated each associated with a model
    //! @return shptr to list of descriptors of interest
    storage::List< CrossValidationInfo> InterfaceRetrieveFromFile::RetrieveEnsembleCVInfo() const
    {
      // get all keys
      storage::Vector< std::string> keys( GetAllKeys());

      // list for retrieved descriptors
      storage::List< CrossValidationInfo> cv_infos;

      for
      (
        storage::Vector< std::string>::const_iterator itr_keys( keys.Begin()), itr_keys_end( keys.End());
        itr_keys != itr_keys_end;
        ++itr_keys
      )
      {
        // insert descriptor set according to key
        cv_infos.PushBack( RetrieveCVInfo( *itr_keys));
      }

      // return list with descriptors
      return cv_infos;
    }

    //! @brief get ensemble of objects
    //! @return shptr list of smallmolececules from given source
    RetrieveInterface::t_Container InterfaceRetrieveFromFile::RetrieveEnsemble() const
    {
      // acquire all keys for that source and return ensemble
      return RetrieveEnsemble( GetAllKeys());
    }

    //! @brief get ensemble of objects for given keys
    //! @param KEYS vector of identifiers for specific objects in given source like "000001", "000002"
    //! @return shptr list of objects from given source
    RetrieveInterface::t_Container InterfaceRetrieveFromFile::RetrieveEnsemble
    (
      const storage::Vector< std::string> &KEYS
    ) const
    {

      RetrieveInterface::t_Container ensemble;

      // iterate over all keys
      for( storage::Vector< std::string>::const_iterator itr( KEYS.Begin()), itr_end( KEYS.End()); itr != itr_end; ++itr)
      {
        RetrieveInterface::t_ModelPtr current( Retrieve( *itr));
        if( current.IsDefined())
        {
          ensemble.PushBack( current);
        }
      }
      // end
      return ensemble;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief construct complete filename from source and key
    //! @brief KEY the key for that protein
    //! @brief filename of form {initializer}/m_FilePrefix{KEY}.s_ModelFileExtension
    std::string InterfaceRetrieveFromFile::Filename( const std::string &KEY, const ExtensionType &EXTENSION_TYPE) const
    {
      const std::string basename
      (
        m_FilePrefix + KEY + io::File::GetExtensionDelimiter() + GetExtensionTypeDescriptor( EXTENSION_TYPE)
      );
      util::ShPtr< io::Directory> directory( m_Directory.Clone());
      // test compressed extensions
      for
      (
        io::StreamBufferClasses::const_iterator
          itr( io::GetStreamBufferClasses().Begin()), itr_end( io::GetStreamBufferClasses().End());
        itr != itr_end;
        ++itr
      )
      {
        const io::DirectoryEntry compressed_file( directory, ( **itr)->AddExtension( basename));
        if( compressed_file.DoesExist())
        {
          return compressed_file.GetFullName();
        }
      }
      // file does not exist with any compression type; return filename sans extension
      return m_Directory.AppendFilename( basename);
    }

    //! @brief helper function to select the best models for each unique independent set
    //! @param KEYS the set of keys to begin with
    //! @return a set of keys; one key for each unique independent set
    storage::Vector< std::string> InterfaceRetrieveFromFile::FindBestModelForEachIndependentSet
    (
      const storage::Vector< std::string> &KEYS
    ) const
    {
      if( KEYS.GetSize() < size_t( 2))
      {
        return KEYS;
      }

      // list for retrieved descriptors
      storage::Map< util::ObjectDataLabel, storage::Pair< float, std::string> > independent_set_to_best_result_and_key;

      // get the 1st cross-validation info
      const CrossValidationInfo first_cross_validation_info( RetrieveCVInfo( KEYS.FirstElement()));

      // determine the improvement type
      const opti::ImprovementTypeEnum improvement_type( first_cross_validation_info.GetImprovementType());

      // insert this value into the map
      independent_set_to_best_result_and_key[ first_cross_validation_info.GetIndependentDatasetRetriever()] =
        storage::Pair< float, std::string>( first_cross_validation_info.GetResult(), KEYS.FirstElement());

      for
      (
        storage::Vector< std::string>::const_iterator itr_keys( KEYS.Begin() + 1), itr_keys_end( KEYS.End());
        itr_keys != itr_keys_end;
        ++itr_keys
      )
      {
        const CrossValidationInfo cross_validation_info( RetrieveCVInfo( *itr_keys));
        BCL_Assert
        (
          improvement_type == cross_validation_info.GetImprovementType(),
          "Models in a given storage should all have the same improvement type but did not in " + this->GetString()
        );
        // find the current independent set in the map
        storage::Map< util::ObjectDataLabel, storage::Pair< float, std::string> >::iterator
          itr_independent
          (
            independent_set_to_best_result_and_key.Find( cross_validation_info.GetIndependentDatasetRetriever())
          );

        // if the independent set is not current in the map, insert this result and continue
        if( itr_independent == independent_set_to_best_result_and_key.End())
        {
          independent_set_to_best_result_and_key[ cross_validation_info.GetIndependentDatasetRetriever()] =
            storage::Pair< float, std::string>( cross_validation_info.GetResult(), *itr_keys);
          continue;
        }

        // independent set is already in the map.  Check whether this model performed better on it
        const float prior_result( itr_independent->second.First());
        if( opti::DoesImprove( cross_validation_info.GetResult(), prior_result, improvement_type))
        {
          // update with this key, since the corresponding model was better
          itr_independent->second.First() = cross_validation_info.GetResult();
          itr_independent->second.Second() = *itr_keys;
        }
      }

      // copy the best key for each cross validation into a vector and return it
      storage::Vector< std::string> best_keys;
      best_keys.AllocateMemory( independent_set_to_best_result_and_key.GetSize());
      for
      (
        storage::Map< util::ObjectDataLabel, storage::Pair< float, std::string> >::const_iterator
          itr( independent_set_to_best_result_and_key.Begin()), itr_end( independent_set_to_best_result_and_key.End());
        itr != itr_end;
        ++itr
      )
      {
        best_keys.PushBack( itr->second.Second());
      }
      best_keys.Sort( std::less< std::string>());
      return best_keys;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer InterfaceRetrieveFromFile::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Storage of trained model on the file system"
      );
      parameters.AddInitializer
      (
        "directory",
        "directory to stored models (for particular session)",
        io::Serialization::GetAgent( &m_DirectoryName)
      );
      parameters.AddInitializer
      (
        "prefix",
        "file prefix to model name eg. MYPREFIX000000." + GetExtensionTypeDescriptor( e_Model),
        io::Serialization::GetAgent( &m_FilePrefix),
        ""
      );
      parameters.AddInitializer
      (
        "pick best",
        "If set, only the best model for each unique independent set will be retrieved. "
        "The \"best\" model is the one with the best final objective function value on its independent set",
        io::Serialization::GetAgent( &m_SelectBestModels),
        "False"
      );
      parameters.AddOptionalInitializer
      (
        "key",
        "Single model key to retrieve; if omitted, all models from the directory with the given prefix are used",
        io::Serialization::GetAgent( &m_Key)
      );
      return parameters;
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool InterfaceRetrieveFromFile::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      // create directory
      m_Directory = io::Directory( m_DirectoryName);
      if( !m_Directory.DoesExist() && !io::File::IsAbsolutePath( m_DirectoryName) && Model::GetModelPathFlag()->GetFlag())
      {
        m_Directory = io::Directory( Model::AddModelPath( m_DirectoryName));
      }
      m_AbsolutePath = io::File::MakeAbsolutePath( m_Directory.GetPath());
      m_Directory = io::Directory( m_AbsolutePath);

      // check whether storage directory exists
      if( !m_Directory.DoesExist())
      {
        ERROR_STREAM << "storage directory does not exist: " << m_Directory.GetPath();
        return false;
      }

      if( !m_Key.empty())
      {
        m_Key = s_KeyToString( m_Key);
        if( !io::DirectoryEntry( Filename( m_Key, e_Model)).DoesExist())
        {
          ERROR_STREAM
            << "given key did not exist in the filename at: "
            << Filename( m_Key, e_Model);
          return false;
        }
      }

      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_interface_store_in_file.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_interface_retrieve_from_file.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! format to convert Key to string
    const util::Format InterfaceStoreInFile::s_KeyToString
    (
      util::Format().W( 6).R().Fill( '0')
    );

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> InterfaceStoreInFile::s_Instance
    (
      util::Enumerated< StoreInterface>::AddInstance( new InterfaceStoreInFile())
    );

    //! @brief ExtensionType as string
    //! @param INIT_TYPE the ExtensionType
    //! @return the string for the INIT_TYPE
    const std::string &InterfaceStoreInFile::GetExtensionTypeDescriptor( const ExtensionType &TYPE)
    {
      static const std::string s_extensions[] =
      {
        "model",
        "descriptor",
        "result",
        "info",
        "independent",
        GetStaticClassName< ExtensionType>()
      };
      return s_extensions[ size_t( TYPE)];
    }

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    InterfaceStoreInFile::InterfaceStoreInFile() :
      m_DirectoryName(),
      m_WriteDescriptors( true),
      m_WriteExperimentalPredicted( false),
      m_FilePrefix( ""),
      m_ModelKey( util::GetUndefinedSize_t()),
      m_HaveOpenLockFile( false),
      m_Compression( io::GetStreamBufferClasses().e_Uncompressed)
    {
      if( io::GetStreamBufferClasses().HaveEnumWithName( "BZ2"))
      {
        m_Compression = io::StreamBufferClass( "BZ2");
      }
      else if( io::GetStreamBufferClasses().HaveEnumWithName( "GZ"))
      {
        m_Compression = io::StreamBufferClass( "GZ");
      }
    }

    //! @brief default constructor
    InterfaceStoreInFile::InterfaceStoreInFile( const std::string &DIRECTORY_NAME) :
      m_DirectoryName(),
      m_WriteDescriptors( true),
      m_WriteExperimentalPredicted( false),
      m_FilePrefix( ""),
      m_ModelKey( util::GetUndefinedSize_t()),
      m_HaveOpenLockFile( false),
      m_Compression( io::GetStreamBufferClasses().e_Uncompressed)
    {
      m_Directory = io::Directory( DIRECTORY_NAME);

      // check if directory exists or could be created
      if( !m_Directory.DoesExist())
      {
        m_Directory.Make();
      }

      if( io::GetStreamBufferClasses().HaveEnumWithName( "BZ2"))
      {
        m_Compression = io::StreamBufferClass( "BZ2");
      }
      else if( io::GetStreamBufferClasses().HaveEnumWithName( "GZ"))
      {
        m_Compression = io::StreamBufferClass( "GZ");
      }
    }

    //! @brief destructor
    InterfaceStoreInFile::~InterfaceStoreInFile()
    {
      CleanUp();
    }

    //! @brief Clone function
    //! @return pointer to new InterfaceStoreInFile
    InterfaceStoreInFile *InterfaceStoreInFile::Clone() const
    {
      return new InterfaceStoreInFile( *this);
    }

    //! @brief cleanup function, to remove lock file if it exists
    void InterfaceStoreInFile::CleanUp()
    {
      // if the lock file is still open at exit
      // (this can happen due to the user terminating the program precisely when the result file is written out)
      // then remove the lock file
      if( m_HaveOpenLockFile)
      {
        io::DirectoryEntry lock_file( Filename( "", e_Result) + ".lock");
        if( lock_file.DoesExist())
        {
          lock_file.Remove();
        }
      }
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &InterfaceStoreInFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &InterfaceStoreInFile::GetAlias() const
    {
      static const std::string s_Name( "File");
      return s_Name;
    }

    //! @brief number of objects in source
    //! @return number of objects with that source prefix
    size_t InterfaceStoreInFile::GetSize() const
    {
      return GetAllKeys().GetSize();
    }

    //! @brief get all keys for given range
    //! @param RANGE range of key values
    //! @return all keys of given prefix
    storage::Vector< std::string> InterfaceStoreInFile::GetKeys( const math::Range< size_t> &RANGE) const
    {
      // keys
      storage::Vector< std::string> keys( GetAllKeys());
      storage::Vector< std::string> valid_keys;

      for
      (
        storage::Vector< std::string>::const_iterator itr( keys.Begin()), itr_end( keys.End());
        itr != itr_end;
        ++itr
      )
      {
        const size_t key_id( util::ConvertStringToNumericalValue< size_t>( *itr));

        // check for valid id
        if( RANGE.IsWithin( key_id))
        {
          valid_keys.PushBack( *itr);
        }
      }

      // return valid keys
      return valid_keys;
    }

    //! @brief get all keys for given source
    //! @return all keys of given prefix
    storage::Vector< std::string> InterfaceStoreInFile::GetAllKeys() const
    {
      // keys
      storage::Vector< std::string> keys;

      // directory content
      storage::List< io::DirectoryEntry> dir_entries;
      for
      (
        io::StreamBufferClasses::const_iterator
          itr_compression( io::GetStreamBufferClasses().Begin()),
          itr_compression_end( io::GetStreamBufferClasses().End());
        itr_compression != itr_compression_end;
        ++itr_compression
      )
      {
        dir_entries.Append
        (
          m_Directory.ListEntries
          (
            io::Directory::e_File,
            m_FilePrefix,
            ( **itr_compression)->AddExtension( "." + GetExtensionTypeDescriptor( e_Model))
          )
        );
      }

      // iterate over all entries
      for( storage::List< io::DirectoryEntry>::const_iterator itr( dir_entries.Begin()), itr_end( dir_entries.End()); itr != itr_end; ++itr)
      {
        // filename without extension
        std::string key( io::File::RemoveLastExtension( io::File::RemoveCompressionExtension( itr->GetName())));

        // extract the key
        key.erase( 0, std::string( m_FilePrefix).length());

        // key needs to be valid
        if( IsValidKey( key))
        {
          keys.PushBack( key);
        }
      }

      return keys;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief try to store the result descriptor
    //! should assert if the labels are already stored and differ from those given
    //! @param RESULT_DESCRIPTOR result object data label for all models in this storage
    void InterfaceStoreInFile::StoreResultDescriptor( const util::ObjectDataLabel &RESULT_DESCRIPTOR)
    {
      // filename and check if exists
      const std::string filename( Filename( "", e_Result));
      io::DirectoryEntry file( filename);
      const std::string lock_filename( filename + ".lock");
      io::DirectoryEntry lock_file( lock_filename);

      io::OFStream output;

      // check if the lock file already exists, if so, wait for it to vanish
      if( file.DoesExist() || lock_file.DoesExist())
      {
        // ensure that the already-stored descriptors match with those given
        const util::ObjectDataLabel retrieved_result
        (
          InterfaceRetrieveFromFile::RetrieveResultDescriptor( filename)
        );
        BCL_Assert
        (
          retrieved_result == RESULT_DESCRIPTOR,
          "Tried to store result descriptor\n" + RESULT_DESCRIPTOR.ToString()
          + " but models in this storage already have a different result descriptor:\n"
          + retrieved_result.ToString()
        );
        return;
      }

      // create the lock file
      BCL_Assert( io::File::TryOpenOFStream( output, lock_filename), "Could not create lock file!");
      m_HaveOpenLockFile = true;
      io::File::CloseClearFStream( output);

      // create the descriptor file
      io::File::MustOpenOFStream( output, filename);
      output << RESULT_DESCRIPTOR.ToString();
      io::File::CloseClearFStream( output);

      // remove the lock file
      lock_file.Remove();
      m_HaveOpenLockFile = false;
    }

    //! @brief store model with additional information in filesystem
    //! @param MODEL model::Interface that will be stored
    //! @param RESULT result value that was evaluated by objective function with current model
    //! @param DESCRIPTORS descriptors used to create dataset the current model was trained on
    //! @param METHOD_NAME name of iterate label that defines the used machine learning algorithm
    //! @param OBJ_FUNCTION objective function used in calculating the result value
    //! @param CV_INFO cross validation info
    //! @return key of stored model
    std::string InterfaceStoreInFile::Store
    (
      const util::ShPtr< Interface> &MODEL,
      const float RESULT,
      const util::ObjectDataLabel &DESCRIPTORS,
      const util::ObjectDataLabel &METHOD_NAME,
      const util::ObjectDataLabel &OBJ_FUNCTION,
      const CrossValidationInfo &CV_INFO
    )
    {
      // initialize current keys
      size_t current_key( 0);

      // get all keys
      storage::Vector< std::string> keys( GetAllKeys());

      // if there are keys existent for that source
      if( util::IsDefined( m_ModelKey))
      {
        current_key = m_ModelKey;
      }
      else if( !keys.IsEmpty())
      {
        // kind largest key
        storage::Vector< std::string>::const_iterator itr( std::max_element( keys.Begin(), keys.End()));

        // convert string to number
        current_key = util::ConvertStringToNumericalValue< size_t>( *itr);
      }

      std::string key;
      while( true)
      {
        key = s_KeyToString( current_key);
        io::DirectoryEntry filename( Filename( key, e_Model));

        // check if a file with the name exists
        const bool file_exists( filename.DoesExist());

        // in the mean time, somebody might have written to the directory
        if( file_exists)
        {
          // advance to next key
          ++current_key;
          continue;
        }

        Store( MODEL, RESULT, DESCRIPTORS, METHOD_NAME, OBJ_FUNCTION, CV_INFO, util::Format()( current_key));

        break;
      }

      // increment key
      return key;
    }

    //! @brief store model with additional information in filesystem
    //! @param MODEL model::Interface that will be stored
    //! @param RESULT result value that was evaluated by objective function with current model
    //! @param DESCRIPTORS descriptors used to create dataset the current model was trained on
    //! @param KEY prefered key for model to store
    //! @param METHOD_NAME name of iterate label that defines the used machine learning algorithm
    //! @param OBJ_FUNCTION objective function used in calculating the result value
    //! @param CV_INFO cross validation info
    //! @return key of stored model
    std::string InterfaceStoreInFile::Store
    (
      const util::ShPtr< Interface> &MODEL,
      const float RESULT,
      const util::ObjectDataLabel &DESCRIPTORS,
      const util::ObjectDataLabel &METHOD_NAME,
      const util::ObjectDataLabel &OBJ_FUNCTION,
      const CrossValidationInfo &CV_INFO,
      const std::string &KEY
    )
    {
      // convert key into right format
      const std::string key( s_KeyToString( KEY));

      // check that key is valid
      if( !IsValidKey( key))
      {
        BCL_MessageStd( "Model file with invalid key: " + key + " . Omitting storage!");
        return std::string();
      }

      WriteToFile( MODEL, key, e_Model);
      WriteToFile( CV_INFO, key, e_Info);

      if( m_WriteDescriptors)
      {
        WriteToFile( DESCRIPTORS, key, e_Descriptor);
      }

      // end
      return key;
    }

    //! @brief store model with additional information in filesystem
    //! @param MODEL model::Interface that will be stored
    //! @param DESCRIPTORS descriptors used to create dataset the current model was trained on
    //! @return key of stored model
    std::string InterfaceStoreInFile::Store
    (
      const util::ShPtr< Interface> &MODEL,
      const util::ObjectDataLabel &DESCRIPTORS
    )
    {
      // convert key into right format
      storage::Vector< std::string> keys( GetAllKeys());
      const std::string key
      (
        s_KeyToString
        (
          keys.GetSize() > 0
          ? util::ConvertStringToNumericalValue< size_t>( keys.LastElement()) + 1
          : size_t( 0)
        )
      );

      // check that key is valid
      if( !IsValidKey( key))
      {
        BCL_MessageStd( "Model file with invalid key: " + key + " . Omitting storage!");
        return std::string();
      }

      WriteToFile( MODEL, key, e_Model);

      if( m_WriteDescriptors)
      {
        WriteToFile( DESCRIPTORS, key, e_Descriptor);
      }

      // end
      return key;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief construct complete filename from source and key
    //! @brief KEY the key for that protein
    //! @brief filename of form {initializer}/m_FilePrefix{KEY}.s_ModelFileExtension
    std::string InterfaceStoreInFile::Filename( const std::string &KEY, const ExtensionType &EXTENSION_TYPE) const
    {
      return m_Directory.AppendFilename( ( *m_Compression)->AddExtension( m_FilePrefix + KEY + io::File::GetExtensionDelimiter() + GetExtensionTypeDescriptor( EXTENSION_TYPE)));
    }

    //! @brief write util::ObjectInterface from source and key
    //! @param OBJECT current object that should be written in file
    //! @param KEY current key
    //! @param EXTENSION_TYPE extention type of file the object will be written into
    void InterfaceStoreInFile::WriteToFile( const util::ObjectInterface &OBJECT, const std::string &KEY, const ExtensionType &EXTENSION_TYPE) const
    {
      // Filename(  and check if exists
      io::DirectoryEntry filename( Filename( KEY, EXTENSION_TYPE));

      if( filename.DoesExist())
      {
        BCL_MessageStd( "file with key: " + KEY + " already exists. Omitting storage!");
        return;
      }

      // initialize stream
      io::OFStream write;
      io::File::MustOpenOFStream( write, filename.GetFullName());
      // write result
      io::Serialize::Write( OBJECT, write);
      io::File::CloseClearFStream( write);
    }

    //! @brief write util::ObjectDataLabel from source and key
    //! @param OBJECT current object that should be written in file
    //! @param KEY current key
    //! @param EXTENSION_TYPE extention type of file the object will be written into
    void InterfaceStoreInFile::WriteToFile( const util::ObjectDataLabel &LABEL, const std::string &KEY, const ExtensionType &EXTENSION_TYPE) const
    {
      // Filename(  and check if exists
      io::DirectoryEntry filename( Filename( KEY, EXTENSION_TYPE));

      if( filename.DoesExist())
      {
        BCL_MessageStd( "file with key: " + KEY + " already exists. Omitting storage!");
        return;
      }

      // initialize stream
      io::OFStream write;
      io::File::MustOpenOFStream( write, filename.GetFullName());
      write << LABEL.ToString();
      io::File::CloseClearFStream( write);
    }

    //! @brief check if key is valid string
    //! @param KEY the key to be checked
    //! @return true if the key is valid
    bool InterfaceStoreInFile::IsValidKey( const std::string &KEY)
    {
      // check that the key is of size 6 and that it is an unsigned integer
      return KEY.length() == 6 && util::LengthOfUnsignedIntegerType( KEY) == 6;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer InterfaceStoreInFile::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Storage of trained model on the file system"
      );
      parameters.AddInitializer
      (
        "directory",
        "directory to stored models (for particular session)",
        io::Serialization::GetAgent( &m_DirectoryName)
      );
      parameters.AddInitializer
      (
        "prefix",
        "file prefix to model name eg. MYPREFIX000000." + GetExtensionTypeDescriptor( e_Model),
        io::Serialization::GetAgent( &m_FilePrefix),
        ""
      );
      parameters.AddInitializer
      (
        "write_descriptors",
        "write out descriptors to file",
        io::Serialization::GetAgent( &m_WriteDescriptors),
        "1"
      );
      parameters.AddOptionalInitializer
      (
        "key",
        "model key if not given it will be automatically set! in case of key duplicates the model will be overwritten!",
        io::Serialization::GetAgent( &m_ModelKey)
      );

      return parameters;
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool InterfaceStoreInFile::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      m_Directory = io::Directory( m_DirectoryName);

      if( io::GetStreamBufferClasses().HaveEnumWithName( "BZ2"))
      {
        m_Compression = io::StreamBufferClass( "BZ2");
      }
      else if( io::GetStreamBufferClasses().HaveEnumWithName( "GZ"))
      {
        m_Compression = io::StreamBufferClass( "GZ");
      }
      else
      {
        m_Compression = io::GetStreamBufferClasses().e_Uncompressed;
      }
      // check if directory exists or could be created
      if( !m_Directory.DoesExist())
      {
        return m_Directory.Make();
      }
      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_kappa_nearest_neighbor.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average.h"
#include "storage/bcl_storage_triplet.h"
// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range
    const math::Range< float> KappaNearestNeighbor::s_DefaultInputRange( 0, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> KappaNearestNeighbor::s_Instance
    (
      GetObjectInstances().AddInstance( new KappaNearestNeighbor())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    KappaNearestNeighbor::KappaNearestNeighbor() :
      m_TrainingData(),
      m_Kappa( 1)
    {
    }

    //! @brief constructor from training data, query data, and rescale functions
    //! @param TRAINING_DATA data to train the NeuralNetwork on
    //! @param KAPPA kappa value for number of nearest neighbors to consider
    KappaNearestNeighbor::KappaNearestNeighbor
    (
      const util::ShPtr< descriptor::Dataset> &TRAINING_DATA,
      const size_t KAPPA
    ) :
      m_TrainingData( TRAINING_DATA),
      m_Kappa( KAPPA)
    {
    }

    //! @brief copy constructor
    KappaNearestNeighbor *KappaNearestNeighbor::Clone() const
    {
      return new KappaNearestNeighbor( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &KappaNearestNeighbor::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &KappaNearestNeighbor::GetAlias() const
    {
      static const std::string s_Name( "KappaNearestNeighbor");
      return s_Name;
    }

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void KappaNearestNeighbor::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_TrainingData->GetFeaturesPtr()->GetScaling())
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_TrainingData->GetFeaturesPtr()->GetScaling());
      }
    }

    //! @brief performs the kappa nearest neighbor algorithm
    //! @param QUERY_VECTOR math::Vector< float> of features which to the neighbors will be found
    //! @param KAPPA number of neigbors
    //! @param TRAINING_FEATURES that from the kappa nearest neighbors are selected
    //! @return StorageSet of kappa nearest neighbors and the corresponding distance from QUERY_VECTOR
    storage::Vector< storage::Pair< float, size_t> > KappaNearestNeighbor::FindWithoutRescaling
    (
      const FeatureReference< float> &QUERY_VECTOR,
      const size_t KAPPA,
      const FeatureDataSet< float> &TRAINING_FEATURES
    )
    {
      BCL_Assert( TRAINING_FEATURES.GetNumberFeatures() > 0, "kNN requires training data!");

      // storing size of data set
      const size_t training_size( TRAINING_FEATURES.GetNumberFeatures());

      BCL_Message( util::Message::e_Debug, "training data size " + util::Format()( training_size));

      // make a set with keys being the kappa lowest distances and place in the training data from which they came
      // If we only used the distance as the key in the map, then if two training data had the same distance from they
      // query vector, the second one would overwrite the value of the first.  With the indice as the second value,
      // we will keep both values in the set
      storage::Set< storage::Pair< float, size_t> >
        kappa_nearest_neighbor_distance;

      // keep track of the kappa-th smallest distance seen so far
      float cutoff_distance( std::numeric_limits< float>::infinity());

      // store the end of the query vector
      const float *itr_query_end( QUERY_VECTOR.End());

      // loop over reference data
      for( size_t itr( 0); itr < training_size; ++itr)
      {
        float distance( 0.0);
        // store norm of difference vector
        for
        (
          const float *itr_data( TRAINING_FEATURES( itr).Begin()), *itr_query( QUERY_VECTOR.Begin());
          distance < cutoff_distance && itr_query != itr_query_end;
          ++itr_data, ++itr_query
        )
        {
          distance += math::Sqr( *itr_data - *itr_query);
        }

        // if the new distance is >= to the cutoff distance, then the output value won't be considered, so continue
        if( distance >= cutoff_distance)
        {
          continue;
        }

        distance = math::Sqrt( distance);

        // if the map has fewer distances than KAPPA, go ahead and add the distance to the map
        kappa_nearest_neighbor_distance.Insert( storage::Pair< float, size_t>( distance, itr));

        // if the map is now larger than kappa, remove the last element
        if( kappa_nearest_neighbor_distance.GetSize() > KAPPA)
        {
          storage::Set< storage::Pair< float, size_t> >::iterator
            itr_end( kappa_nearest_neighbor_distance.End()), new_itr_end;

          // go to the last element
          new_itr_end = --itr_end;

          // new_itr_end needs to go one forward
          --new_itr_end;

          // remove the last element
          kappa_nearest_neighbor_distance.RemoveElement( itr_end);

          // store the new cutoff distance, which is the distance of the last element in the map
          cutoff_distance = math::Sqr( new_itr_end->First());
        }
      }
      //copy into vector
      return storage::Vector< storage::Pair< float, size_t> >( kappa_nearest_neighbor_distance.Begin(), kappa_nearest_neighbor_distance.End());
    }

    //! @brief performs the kappa nearest neighbor algorithm
    //! @param QUERY_VECTORS math::Vector< float> of features
    //!        and known output of query point
    //! @param RESULT_STORAGE pointer to vector or matrix where result should be stored; must be preallocated
    storage::Vector< storage::Triplet< float, size_t, size_t> > KappaNearestNeighbor::FindWithoutRescaling
    (
      const FeatureDataSet< float> &QUERY_VECTORS,
      const size_t KAPPA,
      const FeatureDataSet< float> &TRAINING_FEATURES
    )
    {
      BCL_Assert( TRAINING_FEATURES.GetNumberFeatures() > 0, "kNN requires training data!");

      // storing size of data set
      const size_t training_size( TRAINING_FEATURES.GetNumberFeatures());

      BCL_Message( util::Message::e_Debug, "training data size " + util::Format()( training_size));

      // make a set with keys being the kappa lowest distances and place in the training data from which they came
      // If we only used the distance as the key in the map, then if two training data had the same distance from they
      // query vector, the second one would overwrite the value of the first.  With the indice as the second value,
      // we will keep both values in the set
      storage::Set< storage::Triplet< float, size_t, size_t> >
        kappa_nearest_neighbor_distance;

      // keep track of the kappa-th smallest distance seen so far
      float cutoff_distance( std::numeric_limits< float>::infinity());
      size_t current_size( 0);
      for( size_t query_nr( 0), query_sz( QUERY_VECTORS.GetNumberFeatures()); query_nr < query_sz; ++query_nr)
      {
        // store the end of the query vector
        const float *itr_query_end( QUERY_VECTORS( query_nr).End());

        // loop over reference data
        for( size_t itr( 0); itr < training_size; ++itr)
        {
          float distance( 0.0);
          // store norm of difference vector
          for
          (
            const float *itr_data( TRAINING_FEATURES[ itr]), *itr_query( QUERY_VECTORS[ query_nr]);
            distance < cutoff_distance && itr_query != itr_query_end;
            ++itr_data, ++itr_query
          )
          {
            distance += math::Sqr( *itr_data - *itr_query);
          }

          // if the new distance is >= to the cutoff distance, then the output value won't be considered, so continue
          if( distance >= cutoff_distance)
          {
            continue;
          }

          distance = math::Sqrt( distance);

          // if the map has fewer distances than KAPPA, go ahead and add the distance to the map
          kappa_nearest_neighbor_distance.Insert( storage::Triplet< float, size_t, size_t>( distance, query_nr, itr));

          // if the map is now larger than kappa, remove the last element
          if( current_size + 1 > KAPPA)
          {
            storage::Set< storage::Triplet< float, size_t, size_t> >::iterator
              itr_end( kappa_nearest_neighbor_distance.End()), new_itr_end;

            // go to the last element
            new_itr_end = --itr_end;

            // new_itr_end needs to go one forward
            --new_itr_end;

            // remove the last element
            kappa_nearest_neighbor_distance.RemoveElement( itr_end);

            // store the new cutoff distance, which is the distance of the last element in the map
            cutoff_distance = math::Sqr( new_itr_end->First());
          }
          else
          {
            ++current_size;
          }
        }
      }
      //copy into vector
      return storage::Vector< storage::Triplet< float, size_t, size_t> >( kappa_nearest_neighbor_distance.Begin(), kappa_nearest_neighbor_distance.End());
    }

  //////////////
  // operator //
  //////////////

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> KappaNearestNeighbor::PredictWithoutRescaling( const FeatureDataSetInterface< float> &FEATURE) const
    {
      const size_t num_features( FEATURE.GetNumberFeatures());
      linal::Matrix< float> predicted_results( num_features, m_TrainingData->GetResultSize());
      for( size_t feature_number( 0); feature_number < num_features; ++feature_number)
      {
        EuclideanDistance( FEATURE( feature_number), predicted_results[ feature_number]);
      }

      return FeatureDataSet< float>( predicted_results);
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> KappaNearestNeighbor::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_TrainingData->GetFeaturesPtr()->GetScaling())
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_TrainingData->GetFeaturesPtr()->GetScaling());
        return PredictWithoutRescaling( feature).DeScale();
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read KappaNearestNeighbor from std::istream
    std::istream &KappaNearestNeighbor::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_TrainingData,  ISTREAM);
      io::Serialize::Read( m_Kappa,         ISTREAM);

      // end
      return ISTREAM;
    }

    //! write KappaNearestNeighbor into std::ostream
    std::ostream &KappaNearestNeighbor::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_TrainingData,  OSTREAM) << '\n';
      io::Serialize::Write( m_Kappa,         OSTREAM);

      // end
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief performs the kappa nearest neighbor algorithm
    //! @param QUERY_VECTOR linal::Vector< float> of features
    //!        and known output of query point
    //! @param RESULT_STORAGE pointer to vector or matrix where result should be stored; must be preallocated
    void KappaNearestNeighbor::EuclideanDistance
    (
      const FeatureReference< float> &QUERY_VECTOR,
      float *RESULT_STORAGE
    ) const
    {
      BCL_Assert( m_TrainingData->GetSize() > 0, "kNN requires training data!");

      // storing size of data set
      const size_t training_size( m_TrainingData->GetSize());
      const FeatureDataSetInterface< float> &training_features( *m_TrainingData->GetFeaturesPtr());

      BCL_MessageDbg( "training data size " + util::Format()( training_size));

      // make a set with keys being the kappa lowest distances and place in the training data from which they came
      // If we only used the distance as the key in the map, then if two training data had the same distance from they
      // query vector, the second one would overwrite the value of the first.  With the indice as the second value,
      // we will keep both values in the set
      storage::Set< storage::Pair< float, size_t> >
        kappa_nearest_neighbor_distance;

      // keep track of the kappa-th smallest distance seen so far
      float cutoff_distance( std::numeric_limits< float>::infinity());

      // store the end of the query vector
      const float *itr_query_end( QUERY_VECTOR.End());

      // loop over reference data
      for( size_t itr( 0); itr < training_size; ++itr)
      {
        float distance( 0.0);
        // store norm of difference vector
        for
        (
          const float *itr_data( training_features( itr).Begin()), *itr_query( QUERY_VECTOR.Begin());
          distance < cutoff_distance && itr_query != itr_query_end;
          ++itr_data, ++itr_query
        )
        {
          distance += math::Sqr( *itr_data - *itr_query);
        }

        // if the new distance is >= to the cutoff distance, then the output value won't be considered, so continue
        if( distance >= cutoff_distance)
        {
          continue;
        }

        distance = math::Sqrt( distance);

        // if the map has fewer distances than m_Kappa, go ahead and add the distance to the map
        kappa_nearest_neighbor_distance.Insert( storage::Pair< float, size_t>( distance, itr));

        // if the map is now larger than kappa, remove the last element
        if( kappa_nearest_neighbor_distance.GetSize() > m_Kappa)
        {
          storage::Set< storage::Pair< float, size_t> >::iterator
            itr_end( kappa_nearest_neighbor_distance.End()), new_itr_end;

          // go to the last element
          new_itr_end = --itr_end;

          // new_itr_end needs to go one forward
          --new_itr_end;

          // remove the last element
          kappa_nearest_neighbor_distance.RemoveElement( itr_end);

          // store the new cutoff distance, which is the distance of the last element in the map
          cutoff_distance = math::Sqr( new_itr_end->First());
        }
      }

      // setup activities sum
      const FeatureDataSetInterface< float> &training_results( *m_TrainingData->GetResultsPtr());
      math::RunningAverage< linal::Vector< float> > weighted_ave_activity( training_results( 0));

      // check for distances of zero; if zero distances are present, average them and return
      // to avoid nearly - zero distances that result in overflow, consider any distance < 1e-16 to be equivalent
      storage::Set< storage::Pair< float, size_t> >::const_iterator
        itr_act( kappa_nearest_neighbor_distance.Begin()),
        itr_act_end( kappa_nearest_neighbor_distance.End());

      if( itr_act->First() < float( 1.0e-16))
      {
        for( ; itr_act != itr_act_end && itr_act->First() < float( 1.0e-16); ++itr_act)
        {
          weighted_ave_activity += training_results( itr_act->Second());
        }
      }
      else
      {
        // no distances of essentially zero, so add up the kappa nearest neighbors, weighted by distance
        for( ; itr_act != itr_act_end; ++itr_act)
        {
          // weight each output by distance ^ -1
          const float weight( 1.0 / itr_act->First());

          // add the weighted activity to the weighted activities sum
          weighted_ave_activity.AddWeightedObservation( training_results( itr_act->Second()), weight);
        }
      }

      std::copy( weighted_ave_activity.GetAverage().Begin(), weighted_ave_activity.GetAverage().End(), RESULT_STORAGE);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer KappaNearestNeighbor::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "see http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
      );

      parameters.AddInitializer
      (
        "kappa",
        "number of nearest neighbors to use for computing average",
        io::Serialization::GetAgentWithMin( &m_Kappa, size_t( 1)),
        "3"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_kohonen_network_applicability_domain.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_tertiary_function_job_with_data.h"
#include "storage/bcl_storage_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> KohonenNetworkApplicabilityDomain::s_Instance
    (
      GetObjectInstances().AddInstance( new KohonenNetworkApplicabilityDomain())
    );

    //! @brief constructor from parameters
    //! @param NETWORK the network to copy
    //! @param SHARE_DISTANCE_METRIC Whether to share the distance metric across all nodes
    //! @param MODEL_RETRIEVER retriever for the models
    KohonenNetworkApplicabilityDomain::KohonenNetworkApplicabilityDomain
    (
      const KohonenNetworkAverage &NETWORK,
      const bool &SHARE_DISTANCE_METRIC,
      const util::Implementation< RetrieveInterface> &MODEL_RETRIEVER
    ) :
      KohonenNetworkAverage( NETWORK),
      m_NodesSharedDistanceMetric( SHARE_DISTANCE_METRIC),
      m_ModelRetriever( MODEL_RETRIEVER)
    {
      ReadInitializerSuccessHook( util::ObjectDataLabel(), util::GetLogger());
    }

    //! @brief setup all internal splines
    //! @param NORMALIZED_DATA all the training data, already normalized
    //! @param PREVIOUS_WINNERS previous rounds node winners, will be updated
    void KohonenNetworkApplicabilityDomain::SetupSplines
    (
      const FeatureDataSetInterface< float> &NORMALIZED_DATA,
      const storage::Vector< size_t> &PREVIOUS_WINNERS,
      const bool &SHARE_DISTANCE_METRIC
    )
    {
      m_NodesSharedDistanceMetric = SHARE_DISTANCE_METRIC;
      linal::MatrixConstReference< float> matrix( NORMALIZED_DATA.GetMatrix());

      if( !m_NodesSharedDistanceMetric)
      {
        // different distance metric for each node of the map
        const size_t n_nodes( KohonenNetworkAverage::m_Codebook.GetSize());

        // create a vector of references, one per node in the kohonen map, with all the closest points to that node
        storage::Vector< storage::Vector< linal::VectorConstReference< float> > > closest_neighbor_map( n_nodes);

        for( size_t i( 0), n_features( NORMALIZED_DATA.GetNumberFeatures()); i < n_features; ++i)
        {
          closest_neighbor_map( PREVIOUS_WINNERS( i)).PushBack( matrix.GetRow( i));
        }
        m_Splines.Resize( n_nodes);
        for( size_t node_id( 0); node_id < n_nodes; ++node_id)
        {
          m_Splines( node_id).Resize( 1);
          m_Splines( node_id)( 0) =
           MapData
           (
             closest_neighbor_map( node_id),
             KohonenNetworkAverage::m_Codebook( node_id).GetFeatureVector()
           );
        }
      }
      else
      {
        m_Splines.Resize( 1);
        m_Splines( 0).Resize( 1);

        const size_t number_points( NORMALIZED_DATA.GetNumberFeatures());
        const storage::Vector< KohonenNode> &nodes( KohonenNetworkAverage::m_Codebook);

        // compute all distances
        linal::Vector< double> distances( number_points);
        for( size_t point_id( 0); point_id < number_points; ++point_id)
        {
          distances( point_id)
            = linal::Distance( matrix.GetRow( point_id), nodes( PREVIOUS_WINNERS( point_id)).GetFeatureVector());
        }

        // sort the vector
        std::sort( distances.Begin(), distances.End());

        // extract some representative points from the series 0, 1/100, 1/8, 1/4, 3/8, 1/2, 3/4, 7/8, 15/16, 31/32, 63/64, 127/128, 1
        // interpolate between closest points. These points are chosen with a heavy tail because distances from kohonen
        // nodes have a high slope towards the end of this distribution
        int n_interpolation_points( 0);
        for( size_t x( number_points); x; x >>= 1)
        {
          ++n_interpolation_points;
        }
        n_interpolation_points = std::max( n_interpolation_points, 2) + 5;
        linal::Vector< double> cumul_dist_function( n_interpolation_points, double( 0.0));
        linal::Vector< double> rep_distances( n_interpolation_points, double( 0.0));
        for( int i( 7), denominator( 4); i < n_interpolation_points; ++i, denominator <<= 1)
        {
          cumul_dist_function( i) = double( denominator - 1) / double( denominator);
        }
        cumul_dist_function( 0) = 0.0;
        cumul_dist_function( 1) = 0.01;
        cumul_dist_function( 2) = 0.125;
        cumul_dist_function( 3) = 0.25;
        cumul_dist_function( 4) = 0.375;
        cumul_dist_function( 5) = 0.5;
        cumul_dist_function( 6) = 0.625;

        cumul_dist_function( n_interpolation_points - 1) = 1.0;
        if( !distances.IsEmpty())
        {
          rep_distances( n_interpolation_points - 1) = distances.Last();
        }
        else
        {
          rep_distances( n_interpolation_points - 1) = 0.01;
        }
        for( int i( 1); i < n_interpolation_points - 1; ++i)
        {
          const size_t lower_bound( cumul_dist_function( i) * number_points);
          const double divided_lo( double( lower_bound) / double( number_points));
          rep_distances( i) = distances( lower_bound);
          if( divided_lo == lower_bound || lower_bound + 1 == number_points)
          {
            continue;
          }
          const size_t upper_bound( lower_bound + 1);
          const double dy( 1.0 / double( number_points));
          const double dx( distances( upper_bound) - distances( lower_bound));
          // interpolation based on nearest points
          rep_distances( i) += ( cumul_dist_function( i) - divided_lo) * dx / dy;
        }

        // train the spline
        m_Splines( 0)( 0).Train( rep_distances, cumul_dist_function, 0, 0);
      }
    } // Calibrate

    //! @brief setup all internal splines
    //! @param SPLINES splines, one for each node, or just one (for whole network)
    void KohonenNetworkApplicabilityDomain::SetupSplines
    (
      const storage::Vector< storage::Vector< math::CubicSplineDamped> > &SPLINES
    )
    {
      m_Splines = SPLINES;
      m_NodesSharedDistanceMetric = m_Splines.GetSize() == size_t( 1);
    }

    //! @brief set the model that this object should call to obtain the un-transformed prediction
    //! @param MODELS the models to call to compute the underlying prediction
    void KohonenNetworkApplicabilityDomain::SetupModels
    (
      const util::Implementation< RetrieveInterface> &MODEL_RETRIEVER
    )
    {
      m_ModelRetriever = MODEL_RETRIEVER;
    }

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURES rescaled features
    //! @return predicted result vector using a model
    FeatureDataSet< float> KohonenNetworkApplicabilityDomain::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURES
    ) const
    {
      if( m_ModelRetriever.IsDefined() && m_Models.IsEmpty())
      {
        m_Models = m_ModelRetriever->RetrieveEnsemble();
      }
      storage::Vector< size_t> winning_nodes;
      storage::Vector< sched::Mutex> muteces;
      GetWinningNodeIndices
      (
        FEATURES,
        math::Range< size_t>( 0, FEATURES.GetNumberFeatures()),
        storage::Vector< size_t>(),
        winning_nodes,
        muteces
      );

      const linal::MatrixConstReference< float> features_mat( FEATURES.GetMatrix());

      // create a results matrix
      FeatureDataSet< float> results_fds
      (
        FEATURES.GetNumberFeatures(),
        GetNumberOutputs(),
        float( 0.0)
      );
      linal::Matrix< float> &results( results_fds.GetRawMatrix());

      if( !m_ModelRetriever.IsDefined())
      {
        const bool distance_based_single_result
        (
          GetNumberOutputs() > size_t( 1) && m_Splines( 0).GetSize() == size_t( 1)
        );
        for
        (
          size_t result_n( 0), n_results( distance_based_single_result ? size_t( 1) : GetNumberOutputs());
          result_n < n_results;
          ++result_n
        )
        {
          for
          (
            size_t feature_id( 0), number_features( FEATURES.GetNumberFeatures());
            feature_id < number_features;
            ++feature_id
          )
          {
            // get the feature from the dataset
            const linal::VectorConstReference< float> feature( features_mat.GetRow( feature_id));

            if( !feature.IsDefined())
            {
              if( distance_based_single_result)
              {
                // all results will be equal
                results.GetRow( feature_id) = 0.0;
              }
              else
              {
                results( feature_id, result_n) = 0.0;
              }
              continue;
            }

            // get the corresponding node in the kohonen map
            const linal::VectorConstInterface< float> &node( m_Codebook( winning_nodes( feature_id)).GetFeatureVector());

            // get the corresponding spline
            const math::CubicSplineDamped &spline
            (
              m_Splines( m_NodesSharedDistanceMetric ? 0 : winning_nodes( feature_id))( result_n)
            );

            // distance between kohonen map node and feature
            const float distance( linal::Distance( node, feature));

            // get spline value. Handle fencepost cases too
            const float spline_fract
            (
              distance > spline.GetXValues().Last()
              ? distance / spline.GetXValues().Last()
              : float( std::max( spline( distance), double( 0.0)))
            );

            if( distance_based_single_result)
            {
              // all results will be equal
              results.GetRow( feature_id) = spline_fract;
            }
            else
            {
              results( feature_id, result_n) = spline_fract;
            }
          }
        }
      }
      else
      {
        storage::Vector< linal::Matrix< float> > all_results( m_Models.GetSize());
        // iterate through models, average results
        // create a results matrix
        math::RunningAverageSD< linal::Matrix< float> > ave_results, ave_local_ppv;
        FeatureDataSet< float> fds_descaled( FEATURES);
        fds_descaled.DeScale();
        for( size_t model_n( 0), n_models( m_Models.GetSize()); model_n < n_models; ++model_n)
        {
          linal::Matrix< float> output( ( *m_Models( model_n))( fds_descaled).GetMatrix());
          ave_results += output;
          fds_descaled.DeScale();
          for( size_t result_n( 0), n_results( GetNumberOutputs()); result_n < n_results; ++result_n)
          {
            for
            (
              size_t feature_id( 0), number_features( FEATURES.GetNumberFeatures());
              feature_id < number_features;
              ++feature_id
            )
            {

              // get the corresponding spline
              const math::CubicSplineDamped &spline
              (
                m_Splines( m_NodesSharedDistanceMetric ? 0 : winning_nodes( feature_id))( result_n)
              );

              // distance between kohonen map node and feature
              const float distance( output( feature_id, result_n));

              // get spline value. Handle fencepost cases too
              const float spline_fract
              (
                distance > spline.GetXValues().Last()
                ? spline.GetYValues().Last()
                : distance < spline.GetXValues().First()
                  ? spline.GetYValues().First()
                  : spline( distance)
              );
              output( feature_id, result_n) = spline_fract;
            }
          }
          all_results( model_n) = output;
          ave_local_ppv += output;
        }
        const linal::Matrix< float> &ave_result( ave_results.GetAverage());
        for( size_t result_n( 0), n_results( GetNumberOutputs()); result_n < n_results; ++result_n)
        {
          for
          (
            size_t feature_id( 0), number_features( FEATURES.GetNumberFeatures());
            feature_id < number_features;
            ++feature_id
          )
          {

            // get the corresponding spline
            const math::CubicSplineDamped &spline
            (
              m_Splines( m_NodesSharedDistanceMetric ? 0 : winning_nodes( feature_id))( result_n)
            );

            // distance between kohonen map node and feature
            const float distance( ave_result( feature_id, result_n));

            // get spline value. Handle fencepost cases too
            const float spline_fract
            (
              distance > spline.GetXValues().Last()
              ? spline.GetYValues().Last()
              : distance < spline.GetXValues().First()
                ? spline.GetYValues().First()
                : spline( distance)
            );

            results( feature_id, result_n) = spline_fract;
          }
        }
      }

      return FeatureDataSet< float>( results);
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> KohonenNetworkApplicabilityDomain::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_RescaleInput);
        return PredictWithoutRescaling( feature).DeScale();
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &KohonenNetworkApplicabilityDomain::Read( std::istream &ISTREAM)
    {
      KohonenNetworkAverage::Read( ISTREAM);
      io::Serialize::Read( m_Splines, ISTREAM);
      io::Serialize::Read( m_NodesSharedDistanceMetric, ISTREAM);
      io::Serialize::Read( m_ModelRetriever, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &KohonenNetworkApplicabilityDomain::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      KohonenNetworkAverage::Write( OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Splines, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NodesSharedDistanceMetric, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ModelRetriever, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief Trains the cubic spline based on distances to NODE
    //! @param MAPPED_POINTS all points mapped to NODE
    math::CubicSplineDamped KohonenNetworkApplicabilityDomain::MapData
    (
      const storage::Vector< linal::VectorConstReference< float> > &MAPPED_POINTS,
      const linal::VectorConstInterface< float> &NODE
    )
    {
      const size_t number_points( MAPPED_POINTS.GetSize());
      // compute all distances
      linal::Vector< double> distances( number_points);
      for( size_t point_id( 0); point_id < number_points; ++point_id)
      {
        distances( point_id) = linal::Distance( MAPPED_POINTS( point_id), NODE);
      }

      // sort the vector
      std::sort( distances.Begin(), distances.End());

      // extract some representative points from the series 0, 1/2, 3/4, 7/8, 15/16, 31/32, 63/64, 127/128, 1
      // interpolate between closest points. These points are chosen with a heavy tail because distances from kohonen
      // nodes have a high slope towards the end of this distribution
      int n_interpolation_points( 0);
      for( size_t x( number_points); x; x >>= 1)
      {
        ++n_interpolation_points;
      }
      n_interpolation_points = std::max( n_interpolation_points, 2) + 5;

      linal::Vector< double> cumul_dist_function( n_interpolation_points, double( 0.0));
      linal::Vector< double> rep_distances( n_interpolation_points, double( 0.0));

      cumul_dist_function( 0) = 0.0;
      cumul_dist_function( 1) = 0.01;
      cumul_dist_function( 2) = 0.125;
      cumul_dist_function( 3) = 0.25;
      cumul_dist_function( 4) = 0.375;
      cumul_dist_function( 5) = 0.5;
      cumul_dist_function( 6) = 0.625;
      for( int i( 7), denominator( 4); i < n_interpolation_points; ++i, denominator <<= 1)
      {
        cumul_dist_function( i) = double( denominator - 1) / double( denominator);
      }
      cumul_dist_function( n_interpolation_points - 1) = 1.0;
      if( !distances.IsEmpty())
      {
        rep_distances( n_interpolation_points - 1) = distances.Last();
      }
      else
      {
        rep_distances( n_interpolation_points - 1) = 0.01;
      }
      for( int i( 1); i < n_interpolation_points - 1; ++i)
      {
        const size_t lower_bound( cumul_dist_function( i) * number_points);
        const double divided_lo( double( lower_bound) / double( number_points));
        rep_distances( i) = distances( lower_bound);
        if( divided_lo == lower_bound || lower_bound + 1 == number_points)
        {
          continue;
        }
        const size_t upper_bound( lower_bound + 1);
        const double dy( 1.0 / double( number_points));
        const double dx( distances( upper_bound) - distances( lower_bound));
        // interpolation based on nearest points
        rep_distances( i) += ( cumul_dist_function( i) - divided_lo) * dx / dy;
      }

      // train the spline
      math::CubicSplineDamped spline;
      spline.Train
      (
        rep_distances,
        cumul_dist_function,
        0,
        0
      );
      return spline;
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERROR_STREAM the stream to write errors to
    bool KohonenNetworkApplicabilityDomain::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERROR_STREAM
    )
    {
      if( m_ModelRetriever.IsDefined())
      {
        m_Models = m_ModelRetriever->RetrieveEnsemble();
      }
      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_kohonen_network_average.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "sched/bcl_sched_mutex.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_tertiary_function_job_with_data.h"
#include "storage/bcl_storage_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> KohonenNetworkAverage::s_Instance
    (
      GetObjectInstances().AddInstance( new KohonenNetworkAverage())
    );

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURES rescaled features
    //! @return predicted result vector using a model
    FeatureDataSet< float> KohonenNetworkAverage::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURES
    ) const
    {
      storage::Vector< size_t> winning_nodes;
      storage::Vector< sched::Mutex> muteces;

      const size_t number_features( FEATURES.GetNumberFeatures());

      GetWinningNodeIndices
      (
        FEATURES,
        math::Range< size_t>( 0, FEATURES.GetNumberFeatures()),
        storage::Vector< size_t>(),
        winning_nodes,
        muteces
      );

      // create a results matrix
      linal::Matrix< float> results( FEATURES.GetNumberFeatures(), m_Codebook( 0).GetResultVector().GetSize());

      for( size_t feature_id( 0); feature_id < number_features; ++feature_id)
      {
        const linal::Vector< float> &result_vector( m_Codebook( winning_nodes( feature_id)).GetResultVector());
        std::copy( result_vector.Begin(), result_vector.End(), results[ feature_id]);
      }

      return FeatureDataSet< float>( results);
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> KohonenNetworkAverage::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_RescaleInput);
        return PredictWithoutRescaling( feature).DeScale();
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

    //! @brief set the nodes up according to the map dimensions
    //! @param NORMALIZED_DATA data that can be used to set the initial positions of the nodes
    void KohonenNetworkAverage::InitializeNodes
    (
      const descriptor::Dataset &NORMALIZED_DATA
    )
    {
      // create a list with all the positions
      storage::List< linal::Vector< float> > positions;
      const size_t dimension( m_MapDimensions.GetSize());
      BCL_Assert( dimension, "Kohonen networks cannot be 0D!")

      // create an initial position vector
      linal::Vector< double> position( dimension, 0.0);

      while( position( 0) < m_MapDimensions( 0))
      {
        // add the current position to the positions list, converting it to float along the way
        positions.PushBack( linal::Vector< float>( position.Begin(), position.End()));

        // walk from the end of position back to the beginning to find the next point
        for
        (
          double *itr_position( position[ dimension - 1]),
                 *itr_position_end( position.Begin()),
                 *itr_map_dim( m_MapDimensions[ dimension - 1]);
          ; // condition in loop
          --itr_position, --itr_map_dim
        )
        {
          *itr_position += double( 1.0);
          if( *itr_position >= *itr_map_dim && itr_position != itr_position_end) // wrap around
          {
            *itr_position -= *itr_map_dim;
          }
          else
          {
            break;
          }
        }
      }

      // create the code book
      m_Codebook = storage::Vector< KohonenNode>( positions.GetSize());

      // iterate over the nodes and setup their positions and feature/results properly
      storage::List< linal::Vector< float> >::iterator itr_position( positions.Begin());

      // if no data was given, set up the network to have all 0 features and results
      if( NORMALIZED_DATA.IsEmpty())
      {
        // if there is a valid rescale function, use it to determine the correct dimensions
        const size_t feature_size
        (
          m_RescaleInput.IsDefined()
          ? m_RescaleInput->GetSize()
          : 0
        );
        const size_t result_size( 0);

        // create 0-initialized vectors for the average feature and result for each node
        linal::Vector< float> initial_feature( feature_size, float( 0.0)), initial_result( result_size, float( 0.0));

        for
        (
          storage::Vector< KohonenNode>::iterator itr( m_Codebook.Begin()), itr_end( m_Codebook.End());
          itr != itr_end;
          ++itr, ++itr_position
        )
        {
          *itr = KohonenNode( *itr_position, initial_feature, initial_result);
        }
      }
      else // data was given, use it to set up the initial positions
      {
        size_t data_number( 0);
        const size_t number_data( NORMALIZED_DATA.GetSize());
        // use the positions from the normalized data vector
        for
        (
          storage::Vector< KohonenNode>::iterator itr( m_Codebook.Begin()), itr_end( m_Codebook.End());
          itr != itr_end;
          ++itr, ++itr_position, ++data_number
        )
        {
          const size_t wrapped_data_number( data_number % number_data);
          *itr =
            KohonenNode
            (
              *itr_position,
              NORMALIZED_DATA.GetFeaturesPtr()->operator()( wrapped_data_number),
              NORMALIZED_DATA.GetResultsPtr()->operator()( wrapped_data_number)
            );
        }
      }
    }

    //! @brief reset sets the weight of all nodes to 0
    void KohonenNetworkAverage::Reset()
    {
      for
      (
        storage::Vector< KohonenNode>::iterator itr( m_Codebook.Begin()), itr_end( m_Codebook.End());
        itr != itr_end;
        ++itr
      )
      {
        // reset the weight of each node (positions, features, and result averages remain the same)
        itr->Reset();
      }
    }

    //! @brief gets the winning node
    //! @param VECTOR the sample of data from the training data
    //! @param HINT a hint for the index of the winning node; doubles speed on average if set to previous winner
    //! @return the winning node, paired with the error
    size_t KohonenNetworkAverage::GetIndexOfWinningNode
    (
      const linal::VectorConstInterface< float> &VECTOR,
      const size_t &HINT
    ) const
    {
      BCL_Assert( HINT < m_Codebook.GetSize(), "Hint node doesn't exist!");
      size_t winner( HINT);

      // Go through all code vectors
      double best_square_distance( linal::SquareDistance( m_Codebook( HINT).GetFeatureVector(), VECTOR));

      for( size_t counter( 0), number_codes( m_Codebook.GetSize()); counter < number_codes; ++counter)
      {
        if( HINT == counter)
        {
          // HINT was already used
          continue;
        }
        // get a reference to the current feature
        const linal::Vector< float> &feature( m_Codebook( counter).GetFeatureVector());
        BCL_Assert( feature.GetSize() == VECTOR.GetSize(), "Different vector sizes!");

        // compute the square distance between VECTOR and the current node's feature
        // because computing the distance is costly, only go far enough to see whether the current distance is longer
        // than the shortest distance
        double square_distance( 0.0);
        for
        (
          const float *itr_feature( feature.Begin()), *itr_feature_end( feature.End()), *itr_vector( VECTOR.Begin());
          square_distance < best_square_distance && itr_feature != itr_feature_end;
          ++itr_feature, ++itr_vector
        )
        {
          square_distance += math::Sqr( *itr_feature - *itr_vector);
        }

        // If distance is smaller than previous distances keep the new node
        if( square_distance < best_square_distance)
        {
          winner = counter;
          best_square_distance = square_distance;
        }
      }

      // return best node position
      return winner;
    }

    //! @brief adds each node from the other network to this network. The other network is probably a partial network.
    //! @param OTHER another Kohonen network to add from
    //! @return the resulting network
    KohonenNetworkAverage &KohonenNetworkAverage::operator +=( const KohonenNetworkAverage &OTHER)
    {
      storage::Vector< KohonenNode>::const_iterator from( OTHER.m_Codebook.Begin());
      for
      (
        storage::Vector< KohonenNode>::iterator to( m_Codebook.Begin()), to_end( m_Codebook.End());
        to != to_end;
        ++to, ++from
      )
      {
        *to += *from;
      }

      return *this;
    }

    //! @brief adds each node from the other network to this network. The other network is probably a partial network.
    //! @param OTHER another Kohonen network to add from
    //! @return the resulting network
    KohonenNetworkAverage &KohonenNetworkAverage::AddNetworkWithWeight
    (
      const KohonenNetworkAverage &OTHER,
      const float &WEIGHT
    )
    {
      storage::Vector< KohonenNode>::const_iterator from( OTHER.m_Codebook.Begin());
      for
      (
        storage::Vector< KohonenNode>::iterator to( m_Codebook.Begin()), to_end( m_Codebook.End());
        to != to_end;
        ++to, ++from
      )
      {
        to->AddNodeWithWeight( *from, WEIGHT);
      }

      return *this;
    }

    //! @brief give centroid values to nodes with no associated nearest features
    KohonenNetworkAverage &KohonenNetworkAverage::FixEmptyNodes()
    {
      for
      (
        storage::Vector< KohonenNode>::iterator to( m_Codebook.Begin()), to_end( m_Codebook.End());
        to != to_end;
        ++to
      )
      {
        if( to->GetWeight() > 0.0)
        {
          continue;
        }
        KohonenNode nearby;
        for
        (
          storage::Vector< KohonenNode>::const_iterator itr( m_Codebook.Begin()), itr_end( m_Codebook.End());
          itr != itr_end;
          ++itr
        )
        {
          if( itr->GetWeight())
          {
            nearby.MapData
            (
              itr->GetFeatureVector(),
              itr->GetResultVector(),
              1.0 / linal::SquareDistance( to->GetPosition(), itr->GetPosition())
            );
          }
        }
        to->MapData
        (
          nearby.GetFeatureVector(),
          nearby.GetResultVector(),
          0.0
        );
      }
      return *this;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &KohonenNetworkAverage::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_RescaleInput, ISTREAM);
      io::Serialize::Read( m_Codebook, ISTREAM);
      io::Serialize::Read( m_MapDimensions, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &KohonenNetworkAverage::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_RescaleInput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Codebook, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_MapDimensions, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief thread worker function for finding the winning indices for a data set with the network
    //! @param DATA the data to operate on
    //! @param RANGE the designated range of the training data to work on
    //! @param WINNING_NODES a vector where the winning node indices will be stored
    void KohonenNetworkAverage::GetWinningNodeIndicesHelper
    (
      const FeatureDataSetInterface< float> &DATA,
      const storage::Pair< math::Range< size_t>, util::SiPtr< const storage::Vector< size_t> > > &RANGE_AND_ORDER,
      storage::Pair
      <
        util::SiPtr< storage::Vector< size_t> >,
        util::SiPtr< storage::Vector< sched::Mutex> >
      > &WINNING_NODES_AND_MUTECES
    ) const
    {
      // get the actual range
      const math::Range< size_t> &range( RANGE_AND_ORDER.First());
      const storage::Vector< size_t> &order( *RANGE_AND_ORDER.Second());
      storage::Vector< size_t> &winning_indices( *WINNING_NODES_AND_MUTECES.First());
      storage::Vector< sched::Mutex> &muteces( *WINNING_NODES_AND_MUTECES.Second());

      if( !muteces.IsEmpty())
      {
        // threaded case
        // associate the data with the winning nodes in this range.
        for
        (
          size_t data_index( range.GetMin()), data_index_last( range.GetMax());
          data_index < data_index_last;
          ++data_index
        )
        {
          // get the real node
          const size_t node( order( data_index));
          // get the previous winning node
          muteces( node).Lock();
          const size_t prev_winning_node( winning_indices( node));
          muteces( node).Unlock();
          // find the newest winning node
          const size_t winning_node( GetIndexOfWinningNode( DATA( node), prev_winning_node));
          muteces( node).Lock();
          winning_indices( node) = winning_node;
          muteces( node).Unlock();
        }
      }
      else
      {
        // serial case
        for
        (
          size_t data_index( range.GetMin()), data_index_last( range.GetMax());
          data_index < data_index_last;
          ++data_index
        )
        {
          // get the real node
          const size_t node( order( data_index));
          // update the winning index
          winning_indices( node) = GetIndexOfWinningNode( DATA( node), winning_indices( node));
        }
      }

    }

    //! @brief determines the winning node indices of a normalized data set
    //! @param NORMALIZED_DATA all the training data, already normalized
    //! @param RANGE the range of training data to determine the winning indices for
    //! @param ORDER the order in which to visit the nodes
    //! @param PREVIOUS_WINNERS previous rounds node winners, will be updated
    //! @note this function is threaded for performance
    void KohonenNetworkAverage::GetWinningNodeIndices
    (
      const FeatureDataSetInterface< float> &NORMALIZED_DATA,
      const math::Range< size_t> &RANGE,
      const storage::Vector< size_t> &ORDER,
      storage::Vector< size_t> &PREVIOUS_WINNERS,
      storage::Vector< sched::Mutex> &MUTECES
    ) const
    {
      // construct the standardized range so that we don't have to worry about borders
      const math::Range< size_t> std_range( RANGE.StandardizeRange());

      storage::Vector< size_t> order;
      if( ORDER.IsEmpty())
      {
        order = storage::CreateIndexVector( NORMALIZED_DATA.GetNumberFeatures());
      }
      const storage::Vector< size_t> &order_ref( ORDER.IsEmpty() ? order : ORDER);

      const size_t max_data( std::min( std_range.GetMax(), order_ref.GetSize()));

      // construct a new range limited to the range of data available in normalized data
      math::Range< size_t> range
      (
        math::RangeBorders::e_LeftClosed, std_range.GetMin(), max_data, math::RangeBorders::e_RightOpen
      );

      // if there is no data, return an empty vector
      if( range.IsEmpty())
      {
        return;
      }

      // size of dataset
      const size_t data_set_size( range.GetWidth());

      // group for scheduler
      const size_t group_id( 0);

      const size_t number_of_available_cpus( std::min( sched::GetNumberCPUs(), data_set_size));

      // data interval for jobs
      const size_t interval( data_set_size / number_of_available_cpus);

      // calculate how many threads will operate on interval + 1 features
      const size_t number_threads_with_interval_plus_one( data_set_size % number_of_available_cpus);

      // scheduler for managing concurrent jobs
      util::ShPtrVector< sched::JobInterface> schedule;

      // number of jobs
      size_t number_jobs( number_of_available_cpus);

      schedule.AllocateMemory( number_jobs);

      storage::Vector< storage::Pair< math::Range< size_t>, util::SiPtr< const storage::Vector< size_t> > > > ranges;
      ranges.AllocateMemory( number_jobs);
      if( PREVIOUS_WINNERS.GetSize() != NORMALIZED_DATA.GetNumberFeatures())
      {
        PREVIOUS_WINNERS.Resize( NORMALIZED_DATA.GetNumberFeatures(), size_t( 0));
      }

      // create ranges
      for
      (
        size_t job_number( 0), end_position( range.GetMin());
        job_number < number_jobs;
        ++job_number
      )
      {
        // save the previous end position as the current start position
        const size_t start_position( end_position);

        // add the interval
        end_position += interval;

        // add one if necessary
        if( job_number < number_threads_with_interval_plus_one)
        {
          ++end_position;
        }

        // push back this range
        ranges.PushBack
        (
          storage::Pair< math::Range< size_t>, util::SiPtr< const storage::Vector< size_t> > >
          (
            math::Range< size_t>( start_position, end_position),
            util::SiPtr< const storage::Vector< size_t> >( order_ref)
          )
        );
      }

      storage::Pair
      <
        util::SiPtr< storage::Vector< size_t> >,
        util::SiPtr< storage::Vector< sched::Mutex> >
      > winners_and_muteces( PREVIOUS_WINNERS, MUTECES);

      // create jobs
      for
      (
        storage::Vector< storage::Pair< math::Range< size_t>, util::SiPtr< const storage::Vector< size_t> > > >::const_iterator
          itr_ranges( ranges.Begin()), itr_ranges_end( ranges.End());
        itr_ranges != itr_ranges_end;
        ++itr_ranges
      )
      {
        // create and pushback jobs into scheduler
        schedule.PushBack
        (
          util::ShPtr< sched::JobInterface>
          (
            new sched::TertiaryFunctionJobWithData
            <
              const FeatureDataSetInterface< float>,
              const storage::Pair< math::Range< size_t>, util::SiPtr< const storage::Vector< size_t> > >,
              storage::Pair
              <
                util::SiPtr< storage::Vector< size_t> >,
                util::SiPtr< storage::Vector< sched::Mutex> >
              >,
              void,
              KohonenNetworkAverage
            >
            (
              group_id,
              *this,
              &KohonenNetworkAverage::GetWinningNodeIndicesHelper,
              NORMALIZED_DATA,
              *itr_ranges,
              winners_and_muteces,
              sched::JobInterface::e_READY,
              NULL
            )
          )
        );

        // submit this particular job to start a thread
        sched::GetScheduler().RunJob( schedule.LastElement());
      }

      // join all jobs ( need to be finished)
      for( size_t job_id( 0); job_id < number_jobs; ++job_id)
      {
        sched::GetScheduler().Join( schedule( job_id));
      }
    } // Calibrate

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void KohonenNetworkAverage::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_RescaleInput);
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_kohonen_node.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> KohonenNode::s_Instance
    (
      GetObjectInstances().AddInstance( new KohonenNode())
    );

    //! @brief constructor from parameters
    //! @param POSITION the location of this node within the network.
    //! @param FEATURE_VECTOR the initial feature vector for this node
    //! @param RESULT_VECTOR the initial result vector for this node
    KohonenNode::KohonenNode
    (
      const linal::Vector< float> &POSITION,
      const linal::Vector< float> &FEATURE_VECTOR,
      const linal::Vector< float> &RESULT_VECTOR
    ) :
      m_Position( POSITION),
      m_FeatureAverage( FEATURE_VECTOR),
      m_ResultAverage( RESULT_VECTOR)
    {
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &KohonenNode::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_Position, ISTREAM);
      io::Serialize::Read( m_FeatureAverage, ISTREAM);
      io::Serialize::Read( m_ResultAverage, ISTREAM);
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &KohonenNode::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_Position, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_FeatureAverage, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ResultAverage, OSTREAM, INDENT);
      return OSTREAM;
    }

    //! @brief adds the data from another node to the current node.
    //! @param OTHER the node to add from
    //! @return a reference to this node
    KohonenNode &KohonenNode::operator +=( const KohonenNode &OTHER)
    {
      if( m_FeatureAverage.GetWeight() == 0.0)
      {
        if( OTHER.m_FeatureAverage.GetWeight() > 0.0)
        {
          m_FeatureAverage = OTHER.m_FeatureAverage;
          m_ResultAverage = OTHER.m_ResultAverage;
        }
      }
      else if( OTHER.GetWeight() > 0.0)
      {
        m_FeatureAverage.AddWeightedObservation
        (
          OTHER.m_FeatureAverage.GetAverage(),
          OTHER.m_FeatureAverage.GetWeight()
        );
        m_ResultAverage.AddWeightedObservation
        (
          OTHER.m_ResultAverage.GetAverage(),
          OTHER.m_ResultAverage.GetWeight()
        );
      }
      return *this;
    }

    //! @brief adds the data from another node to the current node with a specified weight
    //! @param OTHER the node to add from
    //! @param WEIGHT the weight to given the reference vector from the newly added node
    //! @return a reference to this node
    KohonenNode &KohonenNode::AddNodeWithWeight( const KohonenNode &OTHER, const float &WEIGHT)
    {
      if( WEIGHT != 0.0)
      {
        m_FeatureAverage.AddWeightedObservation( OTHER.m_FeatureAverage.GetAverage(), WEIGHT);
        m_ResultAverage.AddWeightedObservation( OTHER.m_ResultAverage.GetAverage(), WEIGHT);
      }
      return *this;
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_leverage_matrix.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> LeverageMatrix::s_Instance
    (
      GetObjectInstances().AddInstance( new LeverageMatrix())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    LeverageMatrix::LeverageMatrix()
    {
    }

    //! @brief parameter constructor
    //! @param WEIGHTS weights used in the optimization
    LeverageMatrix::LeverageMatrix
    (
      const linal::MatrixConstInterface< float> &WEIGHTS,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE
    ) :
      m_Weights( WEIGHTS),
      m_Rescaler( RESCALE)
    {
    }

    //! @brief Clone is the virtual copy constructor
    LeverageMatrix *LeverageMatrix::Clone() const
    {
      return new LeverageMatrix( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &LeverageMatrix::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &LeverageMatrix::GetAlias() const
    {
      static const std::string s_alias( "Leverage");
      return s_alias;
    }

    //! @brief get the output feature size for this model
    //! @return the output feature size for this model
    size_t LeverageMatrix::GetNumberOutputs() const
    {
      return 1;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void LeverageMatrix::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      // no rescaling is ever performed by linear regression, so just remove any rescaling on the dataset
      if( m_Rescaler.IsDefined())
      {
        FEATURE.Rescale( *m_Rescaler);
      }
      else
      {
        FEATURE.DeScale();
      }
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> LeverageMatrix::PredictWithoutRescaling( const FeatureDataSetInterface< float> &FEATURE) const
    {
      linal::Matrix< float> hat_product( FEATURE.GetMatrix() * m_Weights);
      linal::Matrix< float> result( FEATURE.GetMatrix().GetNumberRows(), size_t( 1), float( 0.0));
      for( size_t i( 0), n_rows( result.GetNumberRows()); i < n_rows; ++i)
      {
        result( i, 0) = linal::ScalarProduct( hat_product.GetRow( i), FEATURE.GetMatrix().GetRow( i));
      }
      return result;
    }

    //! @brief get the weights used for this model
    //! @return the weights used for this model
    const linal::Matrix< float> &LeverageMatrix::GetWeights() const
    {
      return m_Weights;
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> LeverageMatrix::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( m_Rescaler.IsDefined() && ( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_Rescaler))
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_Rescaler);
        return PredictWithoutRescaling( feature);
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &LeverageMatrix::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_Weights, ISTREAM);
      io::Serialize::Read( m_Rescaler, ISTREAM);
      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return output stream which was written to
    std::ostream &LeverageMatrix::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_Weights, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Rescaler, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer LeverageMatrix::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Computes the leverage of given features with respect to the original training dataset"
      );
      return parameters;
    }

  //////////////////////
  // helper functions //
  //////////////////////

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_meta_data_storage_file.h"

// includes from bcl - sorted alphabetically
#include "descriptor/bcl_descriptor_dataset.h"
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! format to convert Key to string
    const util::Format MetaDataStorageFile::s_KeyToString
    (
      util::Format().W( 6).R().Fill( '0')
    );

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> MetaDataStorageFile::s_Instance
    (
      util::Enumerated< MetaDataStorageInterface>::AddInstance( new MetaDataStorageFile())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new MetaDataStorageFile
    MetaDataStorageFile *MetaDataStorageFile::Clone() const
    {
      return new MetaDataStorageFile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &MetaDataStorageFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &MetaDataStorageFile::GetAlias() const
    {
      static const std::string s_Name( "File");
      return s_Name;
    }

    //! @brief initialize the smallmolecule storage
    //! @return true if initialize was successful
    bool MetaDataStorageFile::Initialize()
    {
      util::ShPtr< io::Directory> sp_dir( new io::Directory( m_DirectoryName));

      // check if directory exists or could be created
      if( sp_dir->DoesExist() || sp_dir->Make())
      {
        m_Directory = sp_dir;
        return true;
      }

      BCL_MessageCrt( "Could not create directory: " + m_DirectoryName);
      // end
      return false;
    }

    //! @brief number of objects in source
    //! @return number of objects with that source prefix
    size_t MetaDataStorageFile::GetSize() const
    {
      return GetAllKeys().GetSize();
    }

    //! @brief get all keys for given source
    //! @return all keys of given prefix
    storage::Vector< std::string> MetaDataStorageFile::GetAllKeys() const
    {
      // directory content
      const storage::List< io::DirectoryEntry> dir_entries
      (
        m_Directory->ListEntries
        (
          io::Directory::e_File,
          m_FilePrefix,
          "." + GetExtensionTypeDescriptor( e_MetaData)
        )
      );

      // keys
      storage::Vector< std::string> keys;

      // iterate over all entries
      for( storage::List< io::DirectoryEntry>::const_iterator itr( dir_entries.Begin()), itr_end( dir_entries.End()); itr != itr_end; ++itr)
      {
        // filename without extension
        std::string key( io::File::RemoveLastExtension( itr->GetName()));

        // extract the key
        key.erase( 0, std::string( m_FilePrefix).length());

        // key needs to be valid
        if( IsValidKey( key))
        {
          keys.PushBack( key);
        }
      }

      return keys;
    }

  ////////////////
  // operations //
  ////////////////

//    //! @brief get smallmolecule
//    //! @param SOURCE prefix of smallmolecule eg. "prefix1"
//    //! @param KEY key identifier for specific smallmolecule in given source like "00001"
//    //! @return shptr to smallmolecule of interest, undefined if there is no such smallmolecule
//    util::ShPtr< Interface> MetaDataStorageFile::Retrieve( const std::string &KEY) const
//    {
//      // model
//      util::ShPtr< Interface> sp_interface;
//
//      const std::string key( s_KeyToString( KEY));
//
//      // check the key
//      if( !IsValidKey( key))
//      {
//        return sp_interface;
//      }
//
//      // filename and check if exists
//      const std::string filename( Filename( key, e_MetaData));
//
//      // check if file with that name exists
//      if( !io::DirectoryEntry( filename).DoesExist())
//      {
//        return sp_interface;
//      }
//
//      // result stored with model
//      float result;
//
//      // create factory and retrieve protein model
//      io::IFStream input;
//      io::File::MustOpenIFStream( input, filename);
//      io::Serialize::Read( result, input);
//      util::ObjectDataLabel descriptors( input);
//      io::Serialize::Read( sp_interface, input);
//
//      io::File::CloseClearFStream( input);
//
//      // if smaller result is better
//      if( m_BestResultSmallest)
//      {
//        // get key to best result so far
//        m_KeyToBestModelByResult = result < m_ResultToBestModel ? key : m_KeyToBestModelByResult;
//      }
//      else
//      {
//        // get key to best result so far
//        m_KeyToBestModelByResult = result > m_ResultToBestModel ? key : m_KeyToBestModelByResult;
//      }
//
//      // return retrieved smallmolecule
//      return sp_interface;
//    }

    //! @brief get descriptor set of model by key
    //! @param KEY key identifier for specific model/descriptor set in given source
    //! @return shptr to descriptorset of interest
    util::ObjectDataLabel MetaDataStorageFile::RetrieveDescriptorSet( const std::string &KEY) const
    {
      // check the key
      if( !IsValidKey( KEY))
      {
        return util::ObjectDataLabel();
      }

      // filename and check if exists
      const std::string filename( Filename( KEY, e_Descriptor));

      // check if file with that name exists
      if( !io::DirectoryEntry( filename).DoesExist())
      {
        return util::ObjectDataLabel();
      }

      // result stored with model
      float result;

      // create factory and retrieve protein model
      io::IFStream input;
      io::File::MustOpenIFStream( input, filename);
      io::Serialize::Read( result, input);
      util::ObjectDataLabel descriptors( input);
      io::File::CloseClearFStream( input);

      // return retrieved descriptor set
      return descriptors;
    }

    //! @brief retrieve best descriptor set by result
    //! @return shptr to best descriptor set of model with best score/result
    util::ObjectDataLabel MetaDataStorageFile::RetrieveBestDescriptorSetByResult() const
    {
      // if there is no key to best model determined yet
      if( m_KeyToBestModelByResult.empty())
      {
        // acquire all keys for that source and return ensemble
        storage::List< util::ObjectDataLabel> models = RetrieveAllDescriptors();
      }

      // best model
      return RetrieveDescriptorSet( m_KeyToBestModelByResult);
    }

    //! @brief get ensemble of descriptors associated each associated with a model
    //! @return shptr to list of descriptors of interest
    storage::List< util::ObjectDataLabel> MetaDataStorageFile::RetrieveAllDescriptors() const
    {
      // get all keys
      storage::Vector< std::string> keys( GetAllKeys());

      // list for retrieved descriptors
      storage::List< util::ObjectDataLabel> descriptors;

      for
      (
        storage::Vector< std::string>::const_iterator itr_keys( keys.Begin()), itr_keys_end( keys.End());
        itr_keys != itr_keys_end;
        ++itr_keys
      )
      {
        // insert descriptor set according to key
        descriptors.PushBack( RetrieveDescriptorSet( *itr_keys));
      }

      // return list with descriptors
      return descriptors;
    }

    //! @brief store model with additional information in filesystem
    //! @param RESULT result value that was evaluated by objective function with current model
    //! @param DESCRIPTORS descriptors used to create dataset the current model was trained on
    //! @param METHOD_NAME name of iterate label that defines the used machine learning algorithm
    //! @param OBJECTIVE_FUNCTION name of objective function label that defines the objective function
    //! @return key of stored model
    std::string MetaDataStorageFile::Store
    (
      const float RESULT,
      const util::ObjectDataLabel &DESCRIPTORS,
      const util::ObjectDataLabel &METHOD_NAME,
      const util::ObjectDataLabel &OBJECTIVE_FUNCTION
    )
    {
      // initialize current keys
      size_t current_key( 0);

      // get all keys
      storage::Vector< std::string> keys( GetAllKeys());

      // if there are keys existent for that source
      if( !keys.IsEmpty())
      {
        // kind largest key
        storage::Vector< std::string>::const_iterator itr( std::max_element( keys.Begin(), keys.End()));

        // convert string to number
        current_key = util::ConvertStringToNumericalValue< size_t>( *itr);
      }

      std::string key;
      while( true)
      {
        key = s_KeyToString( current_key);
        io::DirectoryEntry filename( Filename( key, e_MetaData));

        // check if a file with the name exists
        const bool file_exists( filename.DoesExist());

        // in the mean time, somebody might have written to the directory
        if( file_exists)
        {
          // advance to next key
          ++current_key;
          continue;
        }

        // initialize stream
        io::OFStream write;

        // write out meta data
        io::File::MustOpenOFStream( write, filename.GetFullName());
        io::Serialize::Write( RESULT, write) << std::endl;
        io::Serialize::Write( m_Round, write) << std::endl;
        io::Serialize::Write( m_Iteration, write) << std::endl;
        io::Serialize::Write( m_CrossValidationIdIndependent, write) << std::endl;
        io::Serialize::Write( m_CrossValidationIdMonitoring, write) << std::endl;
        io::Serialize::Write( m_CrossValidationTotalChunks, write) << std::endl;
        io::File::CloseClearFStream( write);

        // write out descriptors
        filename = io::DirectoryEntry( Filename( key, e_Descriptor));
        io::File::MustOpenOFStream( write, filename.GetFullName());
        io::Serialize::Write( DESCRIPTORS, write);
        io::File::CloseClearFStream( write);

        // stop while loop since meta data was written
        break;
      }

      // invalidate best key
      m_KeyToBestModelByResult = "";

      // increment key
      return key;
    }

    //! @brief store model with additional information in filesystem
    //! @param RESULT result value that was evaluated by objective function with current model
    //! @param DESCRIPTORS descriptors used to create dataset the current model was trained on
    //! @param KEY prefered key for model to store
    //! @param EXPERIMENTAL_PREDICTED_VALUES serialized object of a list with pairs of
    //!        experimental and predicted values
    //! @param METHOD_NAME name of iterate label that defines the used machine learning algorithm
    //! @param OBJECTIVE_FUNCTION name of objective function label that defines the objective function
    //! @return key of stored model
    std::string MetaDataStorageFile::Store
    (
      const float RESULT,
      const util::ObjectDataLabel &DESCRIPTORS,
      const util::ObjectDataLabel &METHOD_NAME,
      const util::ObjectDataLabel &OBJECTIVE_FUNCTION,
      const std::string &KEY
    )
    {
      // check that key is valid
      if( !IsValidKey( KEY))
      {
        return std::string();
      }

      // Filename(  and check if exists
      io::DirectoryEntry filename( Filename( KEY, e_MetaData));

      if( filename.DoesExist())
      {
        return std::string();
      }

      // initialize stream
      io::OFStream write;

      // write out meta data
      io::File::MustOpenOFStream( write, filename.GetFullName());
      io::Serialize::Write( RESULT, write) << std::endl;
      io::Serialize::Write( m_Round, write) << std::endl;
      io::Serialize::Write( m_Iteration, write) << std::endl;
      io::Serialize::Write( m_CrossValidationIdIndependent, write) << std::endl;
      io::Serialize::Write( m_CrossValidationIdMonitoring, write) << std::endl;
      io::Serialize::Write( m_CrossValidationTotalChunks, write) << std::endl;
      io::File::CloseClearFStream( write);

      // write out descriptors
      filename = io::DirectoryEntry( Filename( KEY, e_Descriptor));
      io::File::MustOpenOFStream( write, filename.GetFullName());
      io::Serialize::Write( DESCRIPTORS, write);
      io::File::CloseClearFStream( write);

      // invalidate best key
      m_KeyToBestModelByResult = "";

      // end
      return KEY;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &MetaDataStorageFile::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_Directory, ISTREAM);

      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM outputstream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &MetaDataStorageFile::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_Directory, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief construct complete filename from key
    //! @brief KEY the key for that protein
    //! @brief filename of form {path}/FilePrefix{KEY}.FileExtension
    std::string MetaDataStorageFile::Filename( const std::string &KEY, const ExtensionType &TYPE) const
    {
      return m_Directory->AppendFilename( m_FilePrefix + KEY + io::File::GetExtensionDelimiter() + GetExtensionTypeDescriptor( TYPE));
    }

    //! @brief check if key is valid string
    //! @param KEY the key to be checked
    //! @return true if the key is valid
    bool MetaDataStorageFile::IsValidKey( const std::string &KEY)
    {
      // check that the key is of size 6 and that it is an unsigned integer
      return KEY.length() == 6 && util::LengthOfUnsignedIntegerType( KEY) == 6;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer MetaDataStorageFile::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Storage of meta data on file system"
      );
      parameters.AddInitializer
      (
        "directory",
        "directory containing meta data information about cross-validation or descriptor selection",
        io::Serialization::GetAgent( &m_DirectoryName),
        "dir_storage_meta_data"
      );
      parameters.AddInitializer
      (
        "prefix",
        "file prefix to model name eg. MYPREFIX000000." + GetExtensionTypeDescriptor( e_MetaData),
        io::Serialization::GetAgent( &m_FilePrefix),
        ""
      );
      parameters.AddInitializer
      (
        "round",
        "round of descriptor selection",
        io::Serialization::GetAgent( &m_Round),
        "0"
      );
      parameters.AddInitializer
      (
        "iteration",
        "iteration number of particular assembly of descriptor groups",
        io::Serialization::GetAgent( &m_Iteration),
        "0"
      );
      parameters.AddInitializer
      (
        "cv_ind_id",
        "cross-validation id of chunk assigned as independent dataset",
        io::Serialization::GetAgent( &m_CrossValidationIdIndependent),
        "0"
      );
      parameters.AddInitializer
      (
        "cv_mon_id",
        "cross-validation id of chunk assigned as monitoring dataset",
        io::Serialization::GetAgent( &m_CrossValidationIdMonitoring),
        "1"
      );
      parameters.AddInitializer
      (
        "cv_total",
        "number of total cross-validation chunks",
        io::Serialization::GetAgent( &m_CrossValidationTotalChunks),
        "10"
      );

      return parameters;
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool MetaDataStorageFile::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      // invoke method initialize
      return Initialize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_multiple_linear_regression.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> MultipleLinearRegression::s_Instance
    (
      GetObjectInstances().AddInstance( new MultipleLinearRegression())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    MultipleLinearRegression::MultipleLinearRegression()
    {
    }

    //! @brief parameter constructor
    //! @param WEIGHTS weights used in the optimization
    MultipleLinearRegression::MultipleLinearRegression( const linal::MatrixConstInterface< float> &WEIGHTS) :
      m_Weights( WEIGHTS)
    {
    }

    //! @brief Clone is the virtual copy constructor
    MultipleLinearRegression *MultipleLinearRegression::Clone() const
    {
      return new MultipleLinearRegression( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &MultipleLinearRegression::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &MultipleLinearRegression::GetAlias() const
    {
      static const std::string s_alias( "Regression");
      return s_alias;
    }

    //! @brief get the output feature size for this model
    //! @return the output feature size for this model
    size_t MultipleLinearRegression::GetNumberOutputs() const
    {
      return m_Weights.GetNumberCols();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void MultipleLinearRegression::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      // no rescaling is ever performed by linear regression, so just remove any rescaling on the dataset
      FEATURE.DeScale();
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> MultipleLinearRegression::PredictWithoutRescaling( const FeatureDataSetInterface< float> &FEATURE) const
    {
      BCL_Assert
      (
        FEATURE.GetFeatureSize() == m_Weights.GetNumberRows(),
        "Weights are of wrong dimensions (" + util::Format()( m_Weights.GetNumberRows())
        + ") for the given feature set (" + util::Format()( FEATURE.GetFeatureSize()) + " rows)"
      );
      return FeatureDataSet< float>( FEATURE.GetMatrix() * m_Weights);
    }

    //! @brief get the weights used for this model
    //! @return the weights used for this model
    const linal::Matrix< float> &MultipleLinearRegression::GetWeights() const
    {
      return m_Weights;
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> MultipleLinearRegression::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      return PredictWithoutRescaling( FEATURE);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &MultipleLinearRegression::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_Weights, ISTREAM);

      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return output stream which was written to
    std::ostream &MultipleLinearRegression::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_Weights, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer MultipleLinearRegression::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Multiplies a given feature matrix by another matrix to determine output"
      );
      // TODO: Add data member
      return parameters;
    }

  //////////////////////
  // helper functions //
  //////////////////////

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_const_reference.h"
#include "linal/bcl_linal_operations_interface.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_feature_data_reference.h"
#include "sched/bcl_sched_binary_function_job_with_data.h"
#include "sched/bcl_sched_scheduler_interface.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range for neural network transfer functions
    const math::Range< float> NeuralNetwork::s_DefaultInputRange( -1, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetwork::s_Instance
    (
      GetObjectInstances().AddInstance( new NeuralNetwork())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    NeuralNetwork::NeuralNetwork()
    {
    }

    //! @brief construct from all necessary parameters
    //! @param RESCALE_INPUT
    //! @param RESCALE_OUTPUT
    //! @param BIAS
    //! @param WEIGHT
    //! @param TRANSFER_FUNCTION
    NeuralNetwork::NeuralNetwork
    (
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_INPUT,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_OUTPUT,
      const storage::Vector< linal::Vector< float> > &BIAS,
      const storage::Vector< linal::Matrix< float> > &WEIGHT,
      const util::Implementation< TransferFunctionInterface> &TRANSFER_FUNCTION
    ) :
      m_TransferFunction( TRANSFER_FUNCTION),
      m_RescaleInput( RESCALE_INPUT),
      m_RescaleOutput
      (
        RESCALE_OUTPUT.IsDefined()
        ? RESCALE_OUTPUT
        : util::ShPtr< RescaleFeatureDataSet>
          (
            new RescaleFeatureDataSet
            (
              linal::Matrix< float>( size_t( 1), WEIGHT.LastElement().GetNumberRows()),
              math::Range< float>(),
              RescaleFeatureDataSet::e_None
            )
          )
      ),
      m_Bias( BIAS),
      m_Weight( WEIGHT)
    {
      SetImplicitArchitecture();
    }

    //! @brief copy constructor
    //! @param NETWORK the nn that will be copied
    NeuralNetwork::NeuralNetwork( const NeuralNetwork &NETWORK) :
      m_TransferFunction( NETWORK.m_TransferFunction),
      m_RescaleInput( NETWORK.m_RescaleInput),
      m_RescaleOutput( NETWORK.m_RescaleOutput),
      m_Bias( NETWORK.m_Bias),
      m_Weight( NETWORK.m_Weight),
      m_Architecture(),
      m_Mutex(),
      m_Allocated( NETWORK.m_Allocated),
      m_Available( NETWORK.m_Available)
    {
      m_Mutex.Lock();
      m_Available.Reset();
      m_Allocated.Reset();
      m_Mutex.Unlock();

      SetImplicitArchitecture();
    }

    //! copy constructor
    NeuralNetwork *NeuralNetwork::Clone() const
    {
      return new NeuralNetwork( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetwork::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetwork::GetAlias() const
    {
      static const std::string s_Name( "NeuralNetwork");
      return s_Name;
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns architecture of this neural network
    //! @return first entry is the number of input neurons
    //!         middle entries are the number of hidden neurons in each layer
    //!         last number in vector is the number of output neurons
    const storage::Vector< size_t> &NeuralNetwork::GetArchitecture() const
    {
      return m_Architecture;
    }

    //! @brief set the architecture architecture of this neural network
    //! @param ARCHITECTURE first entry is the number of input neurons
    //!         middle entries are the number of hidden neurons in each layer
    //!         last number in vector is the number of output neurons
    void NeuralNetwork::SetArchitecture( const storage::Vector< size_t> &ARCHITECTURE)
    {
      m_Architecture = ARCHITECTURE;
      m_Bias.Reset();
      m_Weight.Reset();

      for( size_t i( 1); i < ARCHITECTURE.GetSize(); ++i)
      {
        m_Bias.PushBack( linal::Vector< float>( ARCHITECTURE( i)));
        m_Weight.PushBack( linal::Matrix< float>( ARCHITECTURE( i), ARCHITECTURE( i - 1)));
      }
    }

    //! get number inputs
    size_t NeuralNetwork::GetNumberInputs() const
    {
      return m_Weight.FirstElement().GetNumberCols();
    }

    //! get number output neurons
    size_t NeuralNetwork::GetNumberOutputs() const
    {
      return m_Weight.LastElement().GetNumberRows();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void NeuralNetwork::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !m_RescaleInput.IsDefined())
      {
        FEATURE.DeScale();
      }
      else if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_RescaleInput);
      }
    }

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> NeuralNetwork::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURE
    ) const
    {
      const size_t num_outs( m_Bias.LastElement().GetSize());
      const size_t num_ins( FEATURE.GetFeatureSize());
      const size_t num_points( FEATURE.GetNumberFeatures());
      BCL_Assert
      (
        num_ins == m_Weight( 0).GetNumberCols(),
        "Given " + util::Format()( num_ins) + " inputs; model expected " + util::Format()( m_Weight( 0).GetNumberCols())
      );
      linal::Matrix< float> result_matrix( num_points, num_outs);

      BCL_Assert
      (
        num_ins == m_Weight( 0).GetNumberCols(),
        "Given " + util::Format()( num_ins) + " inputs; model expected " + util::Format()( m_Weight( 0).GetNumberCols())
      );

      const size_t number_of_available_cpus( std::min( sched::GetNumberCPUs(), num_points));

      if( num_points < size_t( 12) || number_of_available_cpus == size_t( 1))
      {
        // no point in using threads if there are so few features
        PredictWithThread( FEATURE, result_matrix);

        return FeatureDataSet< float>( result_matrix, *m_RescaleOutput);
      }

      // data interval for jobs
      const size_t interval( num_points / number_of_available_cpus);

      // calculate how many threads will operate on interval + 1 features
      const size_t number_threads_with_interval_plus_one( num_points % number_of_available_cpus);

      // scheduler for managing concurrent jobs
      util::ShPtrVector< sched::JobInterface> schedule;

      // number of jobs
      size_t number_jobs( number_of_available_cpus);

      schedule.AllocateMemory( number_jobs);

      // create a vector to hold the results
      storage::List< linal::MatrixReference< float> > results;
      storage::List< FeatureDataReference< float> > inputs;

      // create ranges
      for
      (
        size_t job_number( 0), end_position( 0);
        job_number < number_jobs;
        ++job_number
      )
      {
        // save the previous end position as the current start position
        const size_t start_position( end_position);

        // add the interval
        end_position += interval;

        // add one if necessary
        if( job_number < number_threads_with_interval_plus_one)
        {
          ++end_position;
        }

        const size_t number_rows( end_position - start_position);
        results.PushBack( linal::MatrixReference< float>( number_rows, num_outs, result_matrix[ start_position]));
        inputs.PushBack(
          FeatureDataReference< float>
          (
            linal::MatrixConstReference< float>
            (
              number_rows,
              num_ins,
              FEATURE.GetMatrix()[ start_position]
            )
          )
        );

        // create and pushback jobs into scheduler
        schedule.PushBack
        (
          util::ShPtr< sched::JobInterface>
          (
            new sched::BinaryFunctionJobWithData
            <
              const FeatureDataSetInterface< float>,
              linal::MatrixInterface< float>,
              void,
              NeuralNetwork
            >
            (
              1,
              *this,
              &NeuralNetwork::PredictWithThread,
              inputs.LastElement(),
              results.LastElement(),
              sched::JobInterface::e_READY,
              NULL
            )
          )
        );

        // submit this particular job to start a thread
        sched::GetScheduler().RunJob( schedule.LastElement());
      }

      // join all jobs ( need to be finished)
      for( size_t job_id( 0); job_id < number_jobs; ++job_id)
      {
        sched::GetScheduler().Join( schedule( job_id));
      }

      return FeatureDataSet< float>( result_matrix, *m_RescaleOutput);
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predicted result vector using a model
    FeatureDataSet< float> NeuralNetwork::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( !m_RescaleInput.IsDefined() && !FEATURE.GetScaling().IsDefined())
      {
        // no rescaling to perform
        // handle neural networks that do not require rescaling
        // data is already rescaled
        return PredictWithoutRescaling( FEATURE).DeScale();
      }
      else if( !m_RescaleInput.IsDefined())
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.DeScale();
        return PredictWithoutRescaling( feature).DeScale();
      }
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_RescaleInput);
        return PredictWithoutRescaling( feature).DeScale();
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

    //! @brief predict result with model and compute input sensitivity using a rescaled feature vector
    //! @param FEATURE feature of interest, MUST be rescaled
    //! @return predicted result and input sensitivity (N_Descriptors X N_Results)
    storage::Pair< linal::Vector< float>, linal::Matrix< float> >
    NeuralNetwork::ComputeResultInputSensitivity( const linal::VectorConstInterface< float> &FEATURE) const
    {
      const size_t num_outs( m_Bias.LastElement().GetSize());
      const size_t num_ins( FEATURE.GetSize());
      linal::Vector< float> result( num_outs);
      linal::Vector< float> tmp_input_sensitivity( num_ins);
      float *itr_result( result.Begin());

      storage::List< storage::Vector< linal::Vector< float> > >::iterator
        itr_hidden( AcquireHiddenVectors()), itr_hidden_input( AcquireHiddenVectors()),
        itr_err( AcquireHiddenVectors());
      storage::Vector< linal::Vector< float> > &hidden( *itr_hidden), &hidden_input( *itr_hidden_input);
      storage::Vector< linal::Vector< float> > &errors( *itr_err);

      const size_t number_hidden_layers( hidden.GetSize());

      // input layer
      // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
      hidden_input( 0) = m_Bias( 0);
      linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), FEATURE);

      // run hidden_input through the transfer functions to get the 1st hidden layer output
      m_TransferFunction->F( hidden( 0), hidden_input( 0));

      // remaining layers
      for( size_t k( 1); k < number_hidden_layers; ++k)
      {
        // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
        hidden_input( k) = m_Bias( k);
        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( k), m_Weight( k), hidden( k - 1));

        // run hidden_input through the transfer functions to get the k+1th hidden layer output
        m_TransferFunction->F( hidden( k), hidden_input( k));
      }

      // copy the results into a results matrix
      std::copy( hidden.LastElement().Begin(), hidden.LastElement().End(), itr_result);

      // rescale the result
      linal::MatrixReference< float> result_ref( size_t( 1), num_outs, result.Begin());
      m_RescaleOutput->DeScaleMatrix( result_ref);

      // now compute input sensitivity
      linal::Matrix< float> input_sensitivity( num_ins, num_outs);

      for( size_t output( 0); output < num_outs; ++output)
      {
        // set all values in the error vector to 0, except for the output currently under investigation
        linal::Vector< float> &last_err( errors.LastElement());

        last_err = float( 0.0);
        last_err( output) = 1.0;

        // the following code performs this operation
        // errors.LastElement() = ( RESULT - hidden.LastElement())
        //                          * m_TransferFunction->dF( hidden_input.LastElement(), hidden.LastElement())
        // without creating any (expensive) temporary vectors
        m_TransferFunction->MultiplyBydF
        (
          last_err,
          hidden_input.LastElement(),
          hidden.LastElement()
        );

        // all other layers
        for( size_t i( hidden.GetSize() - 1); i > 0; --i)
        {
          // errors( i - 1) = errors( i) * m_Weight( i)
          linal::GetDefaultOperations< float>().VectorEqualsVectorTimesMatrix( errors( i - 1), errors( i), m_Weight( i));
          m_TransferFunction->MultiplyBydF
          (
            errors( i - 1),
            hidden_input( i - 1),
            hidden( i - 1)
          );
        }

        linal::GetDefaultOperations< float>().VectorEqualsVectorTimesMatrix( tmp_input_sensitivity, errors( 0), m_Weight( 0));
        for( size_t input( 0); input < num_ins; ++input)
        {
          input_sensitivity( input, output) = tmp_input_sensitivity( input);
        }
      }

//    The following code can be used to verify that the above algorithm works
//
//      BCL_MessageStd( "input sensitivity computed via analytic method: " + util::Format()( input_sensitivity));
//
//      linal::Vector< float> feature_copy( FEATURE);
//      linal::Vector< float> original_result( hidden.LastElement());
//      linal::Matrix< float> numeric_input_sensitivity( num_ins, num_outs);
//
//      for( size_t feature( 0); feature < num_ins; ++feature)
//      {
//        feature_copy( feature) += 0.01;
//        // input layer
//
//        // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
//        hidden_input( 0) = m_Bias( 0);
//        math::VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), feature_copy);
//
//        // run hidden_input through the transfer functions to get the 1st hidden layer output
//        m_TransferFunction->F( hidden( 0), hidden_input( 0));
//
//        // remaining layers
//        for( size_t k( 1); k < number_hidden_layers; ++k)
//        {
//          // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
//          hidden_input( k) = m_Bias( k);
//          math::VectorPlusEqualsMatrixTimesVector( hidden_input( k), m_Weight( k), hidden( k - 1));
//
//          // run hidden_input through the transfer functions to get the k+1th hidden layer output
//          m_TransferFunction->F( hidden( k), hidden_input( k));
//        }
//
//        feature_copy( feature) -= 0.01;
//        for( size_t out( 0); out < num_outs; ++out)
//        {
//          numeric_input_sensitivity( feature, out) = ( hidden.LastElement()( out) - original_result( out)) / 0.01;
//        }
//      }
//
//      BCL_MessageStd( "input sensitivity computed numerically: " + util::Format()( numeric_input_sensitivity));

      ReleaseHiddenVectors( itr_hidden);
      ReleaseHiddenVectors( itr_hidden_input);
      ReleaseHiddenVectors( itr_err);
      return storage::Pair< linal::Vector< float>, linal::Matrix< float> >( result, input_sensitivity);
    }

    //! @brief compute the result and deviation of that result using test-time dropout
    //! @param DATASET feature set of interest
    //! @param DROPOUT_RATIOS ratios of neurons to dropout from each layer (except output; excess layer ratios ignored)
    //! @param NREPEATS number of times to repeat dropout mask to get an estimate of the actual result
    storage::VectorND< 3, FeatureDataSet< float> > NeuralNetwork::TestWithDropout
    (
      const FeatureDataSetInterface< float> &FEATURE,
      const storage::Vector< double> &DROPOUT_RATIOS,
      const size_t &NREPEATS
    ) const
    {
      FeatureDataSet< float> rescaled( FEATURE);
      Rescale( rescaled);

      const size_t num_outs( m_Bias.LastElement().GetSize());
      const size_t num_ins( FEATURE.GetFeatureSize());
      const size_t num_points( FEATURE.GetNumberFeatures());
      BCL_Assert
      (
         num_ins == m_Weight( 0).GetNumberCols(),
        "Given " + util::Format()( num_ins) + " inputs; model expected " + util::Format()( m_Weight( 0).GetNumberCols())
      );
      linal::Matrix< float> result_matrix( num_points, num_outs);

      linal::Vector< size_t> n_to_drop( m_Architecture.GetSize() - size_t( 1), size_t( 0));
      storage::Vector< storage::Vector< size_t> > dropout_candidates( m_Architecture.GetSize() - size_t( 1));
      storage::Vector< storage::Vector< size_t> > chosen_to_drop( m_Architecture.GetSize() - size_t( 1));
      auto weightadj( m_Weight);
      for( size_t i( 0), sz( n_to_drop.GetSize()); i < sz; ++i)
      {
        n_to_drop( i) = i < DROPOUT_RATIOS.GetSize() ? size_t( DROPOUT_RATIOS( i) * m_Architecture( i)) : size_t( 0);
        dropout_candidates( i) = storage::CreateIndexVector( m_Architecture( i));
        chosen_to_drop( i).Resize( n_to_drop( i));
        weightadj( i) /= float( 1.0 - double( n_to_drop( i)) / double( weightadj( i).GetNumberCols()));
      }
      storage::List< storage::Vector< linal::Vector< float> > >::iterator
        itr_hidden( AcquireHiddenVectors()), itr_hidden_input( AcquireHiddenVectors()),
        itr_err( AcquireHiddenVectors());
      storage::Vector< linal::Vector< float> > &hidden( *itr_hidden), &hidden_input( *itr_hidden_input);

      const size_t number_hidden_layers( hidden.GetSize());

      linal::Vector< float> feature_with_dropout( num_ins, float( 0.0));
      math::RunningAverageSD< linal::Matrix< float> > ave_sd_result;
      for( size_t rep_nr( 0); rep_nr < NREPEATS; ++rep_nr)
      {
        util::GetLogger().LogStatus( "Sampling #" + util::Format()( rep_nr) + " / " + util::Format()( NREPEATS));
        for( size_t i( 0), sz( n_to_drop.GetSize()); i < sz; ++i)
        {
          storage::Vector< size_t> &chosen_array( chosen_to_drop( i));
          // shuffle the first n-indices in the m_NeuronIndices array
          storage::Vector< size_t> &indices_array( dropout_candidates( i));
          const size_t layer_size( indices_array.GetSize());
          for( size_t neuron( 0), d( n_to_drop( i)); neuron < d; ++neuron)
          {
            // TODO check whether this should be -1 or not
            std::swap( indices_array( neuron), indices_array( random::GetGlobalRandom().Random( neuron, layer_size - 1)));
            chosen_array( neuron) = indices_array( neuron);
          }
          chosen_array.Sort( std::less< size_t>());
        }
        for( size_t ftr_nr( 0); ftr_nr < num_points; ++ftr_nr)
        {
          // input layer
          // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
          hidden_input( 0) = m_Bias( 0);

          if( n_to_drop( 0))
          {
            auto tvec( rescaled( ftr_nr));
            std::copy( tvec.Begin(), tvec.End(), feature_with_dropout.Begin());
            const storage::Vector< size_t> &drop_ids( chosen_to_drop( 0));
            for
            (
              storage::Vector< size_t>::const_iterator itr_drop( drop_ids.Begin()), itr_drop_end( drop_ids.End());
              itr_drop != itr_drop_end;
              ++itr_drop
            )
            {
              feature_with_dropout( *itr_drop) = 0.0;
            }
            //feature_with_dropout /= float( 1.0 - double( drop_ids.GetSize()) / double( num_ins));
            linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), weightadj( 0), feature_with_dropout);
          }
          else
          {
            linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), weightadj( 0), rescaled( ftr_nr));
          }

          // run hidden_input through the transfer functions to get the 1st hidden layer output
          m_TransferFunction->F( hidden( 0), hidden_input( 0));

          // remaining layers
          for( size_t k( 1); k < number_hidden_layers; ++k)
          {
            // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
            hidden_input( k) = m_Bias( k);
            if( n_to_drop( k))
            {
              linal::Vector< float> &hidden_dropped( hidden( k - 1));
              const storage::Vector< size_t> &drop_ids( chosen_to_drop( k));
              for
              (
                storage::Vector< size_t>::const_iterator itr_dropped( drop_ids.Begin()), itr_dropped_end( drop_ids.End());
                itr_dropped != itr_dropped_end;
                ++itr_dropped
              )
              {
                hidden_dropped( *itr_dropped) = 0.0;
              }
              //hidden_dropped /= float( 1.0 - double( drop_ids.GetSize()) / double( dropout_candidates( k).GetSize()));
            }
            linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( k), weightadj( k), hidden( k - 1));

            // run hidden_input through the transfer functions to get the k+1th hidden layer output
            m_TransferFunction->F( hidden( k), hidden_input( k));
          }
          result_matrix.GetRow( ftr_nr).CopyValues( hidden.LastElement());
        }
        m_RescaleOutput->DeScaleMatrix( result_matrix);
        ave_sd_result += result_matrix;
      }
      FeatureDataSet< float> av( ave_sd_result.GetAverage());
      FeatureDataSet< float> sd( ave_sd_result.GetStandardDeviation());
      FeatureDataSet< float> res( PredictWithoutRescaling( rescaled));
      res.DeScale();
      storage::VectorND< 3, FeatureDataSet< float> > ave_sd_results( av, sd, res);
      return ave_sd_results;
    }

    //! @brief remove output layer and set last hidden layer to output layer
    void NeuralNetwork::RemoveOutputLayer()
    {
      m_Bias.RemoveElements( m_Bias.GetSize() - 1, size_t( 1));
      m_Weight.RemoveElements( m_Weight.GetSize() - 1, size_t( 1));
      m_Architecture.RemoveElements( m_Architecture.GetSize() - 1, size_t( 1));
      m_RescaleOutput = util::ShPtr< RescaleFeatureDataSet>
                        (
                          new RescaleFeatureDataSet
                          (
                            linal::Matrix< float>( size_t( 1), m_Weight.LastElement().GetNumberRows()),
                            math::Range< float>(),
                            RescaleFeatureDataSet::e_None
                          )
                        );
      m_Mutex.Lock();
      m_Available.Reset();
      m_Allocated.Reset();
      m_Mutex.Unlock();

      // determine architecture from weights and bias
      SetImplicitArchitecture();
    }

    //! @brief remove input layer and set last hidden layer to input layer
    void NeuralNetwork::RemoveInputLayer()
    {
      m_Bias.RemoveElements( 0, size_t( 1));
      m_Weight.RemoveElements( 0, size_t( 1));
      m_Architecture.RemoveElements( 0, size_t( 1));
//      m_RescaleOutput = util::ShPtr< RescaleFeatureDataSet>
//                        (
//                          new RescaleFeatureDataSet
//                          (
//                            linal::Matrix< float>( size_t( 1), m_Weight.LastElement().GetNumberRows()),
//                            math::Range<float>(),
//                            RescaleFeatureDataSet::e_None
//                          )
//                        );
      m_Mutex.Lock();
      m_Available.Reset();
      m_Allocated.Reset();
      m_Mutex.Unlock();

      // determine architecture from weights and bias
      SetImplicitArchitecture();
    }

    //! @brief append two networks if layers are compatible
    void NeuralNetwork::Append( const util::ShPtr< NeuralNetwork> &NETWORK)
    {
      // stop if the two networks do not fit together
      BCL_Assert
      (
        m_Architecture.LastElement() == NETWORK->GetArchitecture().FirstElement(),
        "Append: networks are not compatible! " + util::Format()( m_Architecture.LastElement())
        + " to " + util::Format()( NETWORK->GetArchitecture().FirstElement())
      );

      // append all necessary components
      m_Bias.Append( NETWORK->GetBias());
      m_Weight.Append( NETWORK->GetWeight());
      m_RescaleOutput = NETWORK->m_RescaleOutput;

      m_Mutex.Lock();
      m_Available.Reset();
      m_Allocated.Reset();
      m_Mutex.Unlock();

      SetImplicitArchitecture();
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read NeuralNetwork from std::istream
    std::istream &NeuralNetwork::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_TransferFunction, ISTREAM);
      io::Serialize::Read( m_RescaleInput, ISTREAM);
      io::Serialize::Read( m_RescaleOutput, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Weight, ISTREAM);

      SetImplicitArchitecture();
      // end
      return ISTREAM;
    }

    //! write NeuralNetwork into std::ostream
    std::ostream &NeuralNetwork::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_TransferFunction, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleInput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleOutput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Weight, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetwork::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "see http://en.wikipedia.org/wiki/Neural_network"
      );

      parameters.AddInitializer
      (
        "transfer function",
        "function that translates input from neurons in the prior layer into the output of each hidden layer",
        io::Serialization::GetAgent( &m_TransferFunction),
        "Sigmoid"
      );

      parameters.AddInitializer
      (
        "architecture",
        "# of neurons in each layer (input, hidden layer(s), output), e.g. (100, 4, 4, 1)",
        io::Serialization::GetAgentContainerWithCheck( &m_Architecture, io::Serialization::GetAgentWithMin( size_t( 1)))
      );

      // TODO: Add data members

      return parameters;
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERROR_STREAM the stream to write errors to
    bool NeuralNetwork::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      // now set up the architecture
      SetArchitecture( m_Architecture);
      return true;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief determines the architecture already present in the weight matrix/vectors of the neural network
    void NeuralNetwork::SetImplicitArchitecture()
    {
      // architecture
      m_Architecture.Reset();

      if( m_Weight.GetSize())
      {
        m_Architecture.AllocateMemory( GetNumberLayers() + 1);

        // number of input neurons
        m_Architecture.PushBack( GetNumberInputs());

        // iterate over all layers to get their size
        storage::Vector< linal::Vector< float> >::const_iterator itr_bias( m_Bias.Begin());
        for
        (
          storage::Vector< linal::Matrix< float> >::const_iterator
            layer_itr( m_Weight.Begin()), layer_itr_end( m_Weight.End());
          layer_itr != layer_itr_end;
          ++layer_itr, ++itr_bias
        )
        {
          m_Architecture.PushBack( layer_itr->GetNumberRows());
          BCL_Assert( itr_bias->GetSize() == layer_itr->GetNumberRows(), "Non-matching bias to weight dimensions!");
        }
      }
    }

    //! @brief acquire a hidden input or hidden vector set (same size as m_Bias)
    //! @return iterator to the hidden input or hidden vector set
    storage::List< storage::Vector< linal::Vector< float> > >::iterator NeuralNetwork::AcquireHiddenVectors() const
    {
      m_Mutex.Lock();
      // all hidden vectors have been allocated
      if( m_Available.IsEmpty())
      {
        m_Available.PushBack( m_Bias);
      }
      // splice the last hidden vector set from the available list onto the allocated list
      storage::List< storage::Vector< linal::Vector< float> > >::iterator avail_itr( m_Available.Begin());
      m_Allocated.InternalData().splice( m_Allocated.Begin(), m_Available.InternalData(), avail_itr);

      // save the iterator to the now-allocated array set
      avail_itr = m_Allocated.Begin();
      m_Mutex.Unlock();
      return avail_itr;
    }

    //! @brief release a given hidden vector set
    //! @param ITR iterator to the hidden input or hidden vector set
    void NeuralNetwork::ReleaseHiddenVectors
    (
      const storage::List< storage::Vector< linal::Vector< float> > >::iterator &ITR
    ) const
    {
      m_Mutex.Lock();
      // splice the iterator from the allocated back onto available
      m_Available.InternalData().splice( m_Available.Begin(), m_Allocated.InternalData(), ITR);
      m_Mutex.Unlock();
    }

    //! @brief function called by each thread to accomplish threaded forward propagation
    //! @param FEATURES features to train on
    //! @param STORAGE storage for the result
    void NeuralNetwork::PredictWithThread
    (
      const FeatureDataSetInterface< float> &FEATURES,
      linal::MatrixInterface< float> &STORAGE
    ) const
    {
      storage::List< storage::Vector< linal::Vector< float> > >::iterator
        itr_hidden( AcquireHiddenVectors()), itr_hidden_input( AcquireHiddenVectors());
      storage::Vector< linal::Vector< float> > &hidden( *itr_hidden), &hidden_input( *itr_hidden_input);

      const size_t num_points( FEATURES.GetNumberFeatures());
      const size_t num_outs( m_Bias.LastElement().GetSize());
      const size_t number_hidden_layers( hidden.GetSize());

      float *itr_result( STORAGE.Begin());
      const float *itr_hidden_result( hidden.LastElement().Begin());
      const float *itr_hidden_result_end( hidden.LastElement().End());

      for( size_t feature_number( 0); feature_number < num_points; ++feature_number, itr_result += num_outs)
      {
        // input layer
        // perform hidden_input( 0) = m_Weight( 0) * FEATURE + m_Bias( 0); without creating new vectors
        hidden_input( 0) = m_Bias( 0);

        linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( 0), m_Weight( 0), FEATURES( feature_number));

        // run hidden_input through the transfer functions to get the 1st hidden layer output
        m_TransferFunction->F( hidden( 0), hidden_input( 0));

        // remaining layers
        for( size_t k( 1); k < number_hidden_layers; ++k)
        {
          // perform hidden_input( k) = m_Weight( k) * hidden(k-1) + m_Bias( k); without creating new vectors
          hidden_input( k) = m_Bias( k);
          linal::GetDefaultOperations< float>().VectorPlusEqualsMatrixTimesVector( hidden_input( k), m_Weight( k), hidden( k - 1));

          // run hidden_input through the transfer functions to get the k+1th hidden layer output
          m_TransferFunction->F( hidden( k), hidden_input( k));
        }

        // copy the results into a results matrix
        std::copy( itr_hidden_result, itr_hidden_result_end, itr_result);
      }

      ReleaseHiddenVectors( itr_hidden);
      ReleaseHiddenVectors( itr_hidden_input);
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_perturb_attenuate.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkPerturbAttenuate::s_Instance
    (
      util::Enumerated< NeuralNetworkPerturbationInterface>::AddInstance
      (
        new NeuralNetworkPerturbAttenuate()
      )
    );

    //! @brief copy constructor
    //! @return a new NeuralNetworkPerturbAttenuate copied from this instance
    NeuralNetworkPerturbAttenuate *NeuralNetworkPerturbAttenuate::Clone() const
    {
      return new NeuralNetworkPerturbAttenuate( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkPerturbAttenuate::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkPerturbAttenuate::GetAlias() const
    {
      static const std::string s_Name( "Attenuate");
      return s_Name;
    }

    //! @brief this is a function used internally to update the weights of a neural network
    //! @param WEIGHTS a matrix of weights
    void NeuralNetworkPerturbAttenuate::operator ()( linal::Matrix< float> &WEIGHTS)
    {
      // attenuate the weights
      for( linal::Matrix< float>::iterator itr( WEIGHTS.Begin()), itr_end( WEIGHTS.End()); itr != itr_end; ++itr)
      {
        // get the weight
        float &weight( *itr);

        // compute absolute weight
        const float abs_weight( math::Absolute( weight));

        // determine by how much to attenuate the weight
        const float weight_attenuation( m_Attenuation( abs_weight));

        // if the attenuation would be larger than the current weight, set the weight to zero
        if( weight_attenuation > abs_weight)
        {
          weight = 0.0;
        }
        // otherwise, add or subtract the weight attenuation to move the weight closer to 0
        else if( weight < float( 0.0))
        {
          weight += weight_attenuation;
        }
        else
        {
          weight -= weight_attenuation;
        }
      }
    } // operator ()

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkPerturbAttenuate::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Attenuates weights directly according to the absolute value of the weight"
      );
      parameters.AddInitializer
      (
        "",
        "Coefficients of polynomial used to attenuate weights, in ascending order of degree",
        io::Serialization::GetAgent( &m_Attenuation)
      );
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &NeuralNetworkPerturbAttenuate::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_Attenuation, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &NeuralNetworkPerturbAttenuate::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_Attenuation, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_perturb_max_norm.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkPerturbMaxNorm::s_Instance
    (
      util::Enumerated< NeuralNetworkPerturbationInterface>::AddInstance
      (
        new NeuralNetworkPerturbMaxNorm()
      )
    );

    //! @brief copy constructor
    //! @return a new NeuralNetworkPerturbMaxNorm copied from this instance
    NeuralNetworkPerturbMaxNorm *NeuralNetworkPerturbMaxNorm::Clone() const
    {
      return new NeuralNetworkPerturbMaxNorm( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkPerturbMaxNorm::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkPerturbMaxNorm::GetAlias() const
    {
      static const std::string s_Name( "MaxNorm");
      return s_Name;
    }

    //! @brief this is a function used internally to update the weights of a neural network
    //! @param WEIGHTS a matrix of weights
    void NeuralNetworkPerturbMaxNorm::operator ()( linal::Matrix< float> &WEIGHTS)
    {
      // perform normalization on inputs 1st: normalize all rows such that L2(row) <= m_MaxNormIncoming

      // perform the max-norm operation on the rows, simultaneously computing the column-wise sum of squares
      linal::Vector< float> col_norms( WEIGHTS.GetNumberCols(), float( 0.0));
      size_t n_inputs_normalized( 0);
      for( size_t row( 0), n_rows( WEIGHTS.GetNumberRows()); row < n_rows; ++row)
      {
        // get a reference to the row
        linal::VectorReference< float> row_ref( WEIGHTS.GetRow( row));

        // compute norm
        const float norm( row_ref.Norm());

        // if norm > m_MaxNormIncoming, scale
        if( norm > m_MaxNormIncoming)
        {
          row_ref *= m_MaxNormIncoming / norm;
          ++n_inputs_normalized;
        }

        col_norms += linal::SqrVector( row_ref);
      }

      // compute column-wise norms by taking the sqrt of col_norms
      // Then, transform col_norms into the necessary multipling vector for each row in the matrix
      size_t n_outputs_normalized( 0);
      for( size_t col( 0), n_cols( col_norms.GetSize()); col < n_cols; ++col)
      {
        const float col_norm( math::Sqrt( col_norms( col)));
        if( col_norm > m_MaxNormOutgoing)
        {
          col_norms( col) = m_MaxNormOutgoing / col_norm;
          ++n_outputs_normalized;
        }
        else
        {
          // norm is sufficiently small, set to 1 (no multiplication)
          col_norms( col) = 1.0;
        }
      }

      if( n_inputs_normalized + n_outputs_normalized)
      {
        BCL_MessageStd
        (
          "# of inputs normalized: " + util::Format()( n_inputs_normalized)
          + "# of outputs normalized: " + util::Format()( n_outputs_normalized)
        );
      }

      // check if there are any columns to normalize
      if( !n_outputs_normalized)
      {
        return;
      }

      // normalize by multiplying each row of WEIGHTS by col_norms
      for( size_t row( 0), n_rows( WEIGHTS.GetNumberRows()); row < n_rows; ++row)
      {
        // get a reference to the row
        linal::VectorReference< float> row_ref( WEIGHTS.GetRow( row));

        // multiply each row by col_norms
        linal::ElementwiseMultiply( row_ref, col_norms);
      }
    } // operator ()

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkPerturbMaxNorm::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Limits the incoming weight norm (L2) for the neuron; if weights exceed this value, all values are scaled"
      );
      parameters.AddInitializer
      (
        "in",
        "Max euclidean norm for hidden neuron input weights",
        io::Serialization::GetAgent( &m_MaxNormIncoming)
      );
      parameters.AddInitializer
      (
        "out",
        "Max euclidean norm for input neuron output to the hidden layer (normally substantially larger than in)",
        io::Serialization::GetAgent( &m_MaxNormOutgoing),
        "10"
      );
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &NeuralNetworkPerturbMaxNorm::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_MaxNormIncoming, ISTREAM);
      io::Serialize::Read( m_MaxNormOutgoing, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &NeuralNetworkPerturbMaxNorm::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_MaxNormIncoming, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_MaxNormOutgoing, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_accuracy.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationAccuracy::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationAccuracy()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationAccuracy::NeuralNetworkSelectiveBackpropagationAccuracy() :
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_IgnoreTruePredictionsBelow( 0.1),
      m_IgnoreTruePredictionsAbove( 0.9),
      m_IgnoreFalsePredictionsBelow ( 0.0),
      m_IgnoreFalsePredictionsAbove ( 1.0),
      m_PureClassification( false),
      m_BalanceError( false),
      m_FlatErrorFunction( false),
      m_ResultsSize( 1000),
      m_NumberThreads( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationAccuracy copied from this instance
    NeuralNetworkSelectiveBackpropagationAccuracy *NeuralNetworkSelectiveBackpropagationAccuracy::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationAccuracy( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationAccuracy::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationAccuracy::GetAlias() const
    {
      static const std::string s_name( "Accuracy");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationAccuracy::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      m_ResultsSize = m_ResultEndID - m_ResultStartID;
      m_NumberThreads = NUMBER_THREADS;

      // set up sizes of all internally-held members
      m_ScaledCutoffs = linal::Vector< float>( m_ResultsSize);

      m_IncorrectAboveCutoffTally.Resize( m_NumberThreads);
      m_IncorrectAboveCutoffTally.SetAllElements( linal::Vector< size_t>( m_ResultsSize, size_t( 0)));
      m_IncorrectBelowCutoffTally = m_CorrectAboveCutoffTally = m_CorrectBelowCutoffTally
          = m_IncorrectAboveCutoffTally;

      // regression task: Compute standard deviation over the selected columns
      const float cutoff( OBJECTIVE.GetThreshold());
      m_ScaledCutoffs = linal::Vector< float>( m_ResultsSize, float( 0.0));
      m_Results = RESCALED_DATA.GetResultsPtr();

      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESCALED_DATA.GetResultsPtr()->GetScaling());
      for( size_t result( 0); result < m_ResultsSize; ++result)
      {
        m_ScaledCutoffs( result) = results_scaling->RescaleValue( result, cutoff);
      }
      m_LastRoundInaccuraciesAboveCutoff = m_LastRoundInaccuraciesBelowCutoff
        = linal::Vector< float>( m_ResultsSize, float( 0.0));
      m_LastRoundRelativeErrorAboveCutoff = m_LastRoundRelativeErrorBelowCutoff
        = linal::Vector< float>( m_ResultsSize, float( 1.0));
      m_RoundNumber = 1;
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationAccuracy::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      bool backprop_should_continue( false);
      linal::VectorConstReference< float> result_row
      (
        m_ResultsSize,
        m_Results->GetMatrix()[ FEATURE_ID] + m_ResultStartID
      );
      for( size_t result_offset( 0); result_offset < m_ResultsSize; ++result_offset)
      {
        // get the actual index
        const size_t result( result_offset + m_ResultStartID);

        const float experimental( result_row( result_offset));
        const bool exp_was_above_cutoff( experimental > m_ScaledCutoffs( result_offset));
        const float prediction( PREDICTION( result));
        const float cutoff
        (
          m_PureClassification
          ? ( exp_was_above_cutoff ? std::min( float( 0.5), m_IgnoreFalsePredictionsAbove) : std::max( float( 0.5), m_IgnoreFalsePredictionsBelow))
          : m_ScaledCutoffs( result_offset)
        );
        const bool predicted_was_above_cutoff( prediction > cutoff);

        float &er( ERROR( result));

        const float inaccuracy_last_round
        (
          ( exp_was_above_cutoff ? m_LastRoundInaccuraciesAboveCutoff : m_LastRoundInaccuraciesBelowCutoff)( result_offset)
        );

        float distance_to_saturation( 0.0), cutoff_distance_to_saturation( 0);

        // tally correct / incorrect counts based on cutoff side
        if( exp_was_above_cutoff == predicted_was_above_cutoff)
        {
          if( predicted_was_above_cutoff)
          {
            ++m_CorrectAboveCutoffTally( THREAD_ID)( result_offset);
            distance_to_saturation = m_IgnoreTruePredictionsAbove - prediction;
            cutoff_distance_to_saturation = m_IgnoreTruePredictionsAbove - cutoff;
          }
          else
          {
            ++m_CorrectBelowCutoffTally( THREAD_ID)( result_offset);
            distance_to_saturation = prediction - m_IgnoreTruePredictionsBelow;
            cutoff_distance_to_saturation = cutoff - m_IgnoreTruePredictionsBelow;
          }
        }
        else
        {
          if( predicted_was_above_cutoff)
          {
            ++m_IncorrectBelowCutoffTally( THREAD_ID)( result_offset);
            distance_to_saturation = m_IgnoreFalsePredictionsAbove - prediction;
          }
          else
          {
            ++m_IncorrectAboveCutoffTally( THREAD_ID)( result_offset);
            distance_to_saturation = prediction - m_IgnoreFalsePredictionsBelow;
          }
        }

        const bool last_round_was_mostly_right( inaccuracy_last_round < 0.5);
        const bool on_correct_side( exp_was_above_cutoff == predicted_was_above_cutoff);
        if( ( on_correct_side || last_round_was_mostly_right) && distance_to_saturation <= 0.0)
        {
          // prediction beyond saturation range, and in the correct direction, or at least half the values were in the
          // correct direction last time, so assume features that yield saturated values cannot reasonably be predicted
          // from the descriptors

          // saturated result; suppose results are scaled to 0.1 - 0.9,
          // so if the predicted value was on the correct side of
          // the cutoff and is either between 0.0 - 0.1 or 0.9 - 1.0, then we're in the saturation range of the transfer
          // function and it results in an ill-conditioned network in this case to backpropagate an error term that
          // tries to pin the result exactly at 0.1 or 0.9.
          er = 0.0;
          continue;
        }
        if( er == 0.0)
        {
          continue;
        }
        backprop_should_continue = true;
        if( m_FlatErrorFunction || !on_correct_side)
        {
          er = exp_was_above_cutoff ? 1.0 : -1.0;
        }
        else if( m_PureClassification)
        {
          er = ( er > 0.0 ? 1.0 : -1.0) * math::Absolute( distance_to_saturation) / cutoff_distance_to_saturation;
        }
        if( m_BalanceError)
        {
          er *= ( exp_was_above_cutoff ? m_LastRoundRelativeErrorAboveCutoff : m_LastRoundRelativeErrorBelowCutoff)( result_offset);
        }
      }

      // return true if any part of ERROR remains to be backpropagated
      return backprop_should_continue;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationAccuracy::FinalizeRound()
    {
      // accumulate results
      for( size_t thread_number( 1); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectAboveCutoffTally( 0) += m_IncorrectAboveCutoffTally( thread_number);
        m_IncorrectBelowCutoffTally( 0) += m_IncorrectBelowCutoffTally( thread_number);
        m_CorrectAboveCutoffTally( 0)   += m_CorrectAboveCutoffTally( thread_number);
        m_CorrectBelowCutoffTally( 0)   += m_CorrectBelowCutoffTally( thread_number);
      }
      for( size_t i( 0); i < m_ResultsSize; ++i)
      {
        m_LastRoundInaccuraciesAboveCutoff( i) = float( m_IncorrectAboveCutoffTally( 0)( i)) / float( m_IncorrectAboveCutoffTally( 0)( i) + m_CorrectAboveCutoffTally( 0)( i));
        m_LastRoundInaccuraciesBelowCutoff( i) = float( m_IncorrectBelowCutoffTally( 0)( i)) / float( m_IncorrectBelowCutoffTally( 0)( i) + m_CorrectBelowCutoffTally( 0)( i));
        if( m_BalanceError)
        {
          if( m_LastRoundInaccuraciesAboveCutoff( i) > m_LastRoundInaccuraciesBelowCutoff( i))
          {
            m_LastRoundRelativeErrorAboveCutoff( i) *= 1.05;
            m_LastRoundRelativeErrorBelowCutoff( i) /= 1.05;
          }
          else
          {
            m_LastRoundRelativeErrorAboveCutoff( i) /= 1.05;
            m_LastRoundRelativeErrorBelowCutoff( i) *= 1.05;
          }
        }
        BCL_MessageStd
        (
          "Round #" + util::Format()( m_RoundNumber) + " inaccuracy below cutoff for result " + util::Format()( i)
          + " = " + util::Format()( m_LastRoundInaccuraciesBelowCutoff( i))
          + " above cutoff = " + util::Format()( m_LastRoundInaccuraciesAboveCutoff( i)) +
          (
            m_BalanceError
            ? " error ratio above cutoff: " + util::Format()( m_LastRoundRelativeErrorAboveCutoff( i))
              + " error ratio below cutoff: " + util::Format()( m_LastRoundRelativeErrorBelowCutoff( i))
            : std::string()
          )
        );
      }
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectAboveCutoffTally( thread_number) = size_t( 0);
        m_IncorrectBelowCutoffTally( thread_number) = size_t( 0);
        m_CorrectAboveCutoffTally( thread_number) = size_t( 0);
        m_CorrectBelowCutoffTally( thread_number) = size_t( 0);
      }
      ++m_RoundNumber;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationAccuracy::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Always back-propagating features that are mispredicted on either side of the cutoff. "
        "Features predicted on the currect side of the cutoff, but closer to the cutoff than the actual result, are "
        "backpropagated with a user-specified probability"
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to use this triager for",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "pure classification",
        "True if result values should be pretended to be binary, depending on their side of the cutoff",
        io::Serialization::GetAgent( &m_PureClassification),
        "False"
      );
      parameters.AddInitializer
      (
        "balance error",
        "True to weight error values so as to try to balance inaccuracies on each side of the cutoff. This is done via"
        "the use of multipliers that are adjusted in an iterative fashion",
        io::Serialization::GetAgent( &m_BalanceError),
        "False"
      );
      parameters.AddInitializer
      (
        "tolerance above",
        "Rescaled boundary above which accurate predictions are not backpropagated",
        io::Serialization::GetAgent( &m_IgnoreTruePredictionsAbove),
        "0.9"
      );
      parameters.AddInitializer
      (
        "tolerance below",
        "Rescaled boundary below which accurate predictions are not backpropagated",
        io::Serialization::GetAgent( &m_IgnoreTruePredictionsBelow),
        "0.1"
      );
      parameters.AddInitializer
      (
        "ignore false predictions below",
        "Do not backpropagate false predictions that are below this value",
        io::Serialization::GetAgent( &m_IgnoreFalsePredictionsBelow),
        "0.0"
      );
      parameters.AddInitializer
      (
        "ignore false predictions above",
        "Ddo not backpropagate false predictions that are above this value",
        io::Serialization::GetAgent( &m_IgnoreFalsePredictionsAbove),
        "1.0"
      );
      parameters.AddInitializer
      (
        "flat error function",
        "If true, all error values outside the saturation region are set to +/- 1.",
        io::Serialization::GetAgent( &m_FlatErrorFunction),
        "False"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_adaptive_tolerance.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationAdaptiveTolerance()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::NeuralNetworkSelectiveBackpropagationAdaptiveTolerance() :
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_MaxTolerance( 0.0),
      m_MinTolerance( 0.0),
      m_InitialTolerance( 0.0),
      m_ToleranceMinusError( 1.0),
      m_AdaptationRate( 0.5),
      m_ReduceErrorByTolerance( false),
      m_PureClassification( false),
      m_ResultsSize( 1000),
      m_NumberThreads( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationAdaptiveTolerance copied from this instance
    NeuralNetworkSelectiveBackpropagationAdaptiveTolerance *NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationAdaptiveTolerance( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::GetAlias() const
    {
      static const std::string s_name( "AdaptiveTolerant");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      m_ResultsSize = m_ResultEndID - m_ResultStartID;
      m_NumberThreads = NUMBER_THREADS;
      BCL_Assert( m_MinTolerance <= m_MaxTolerance, "Min tolerance must be less than max tolerance");
      m_RescaledTolerance = linal::Vector< float>( m_ResultsSize, std::max( m_InitialTolerance, m_MinTolerance));
      m_RescaledMaxTolerance = linal::Vector< float>( m_ResultsSize, m_MaxTolerance);
      m_RescaledMinTolerance = linal::Vector< float>( m_ResultsSize, m_MinTolerance);
      m_ThreadResultAverageError.Reset();
      m_ThreadResultAverageError.Resize
      (
        m_NumberThreads,
        storage::Vector< math::RunningAverage< float> >( m_ResultsSize)
      );
      m_ScaledCutoffs = linal::Vector< float>( m_ResultsSize, float( 0.0));
      m_Results = RESCALED_DATA.GetResultsPtr();

      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESCALED_DATA.GetResultsPtr()->GetScaling());
      const float rescale_to_range_width( results_scaling->GetRange().GetWidth());
      for( size_t result( 0); result < m_ResultsSize; ++result)
      {
        const float rescaled_range_ratio( results_scaling->GetRescaleRanges()( result).GetWidth() / rescale_to_range_width);
        if( m_PureClassification)
        {
          m_ScaledCutoffs( result) = results_scaling->RescaleValue( result, OBJECTIVE.GetThreshold());
        }
        m_RescaledTolerance( result) /= rescaled_range_ratio;
        m_RescaledMaxTolerance( result) /= rescaled_range_ratio;
        m_RescaledMinTolerance( result) /= rescaled_range_ratio;
      }
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      bool backprop_should_continue( false);
      // pointer to the error vector for this thread
      storage::Vector< math::RunningAverage< float> >::iterator
        itr_error_ave( m_ThreadResultAverageError( THREAD_ID).Begin());
      linal::Vector< float>::const_iterator itr_tol( m_RescaledTolerance.Begin()),
                                            itr_result( ( *m_Results)[ FEATURE_ID] + m_ResultStartID);
      for
      (
        size_t result_offset( 0);
        result_offset < m_ResultsSize;
        ++result_offset, ++itr_error_ave, ++itr_tol, ++itr_result
      )
      {
        // get the actual index
        const size_t result_id( result_offset + m_ResultStartID);
        float &er( ERROR( result_id));

        if( m_PureClassification)
        {
          // for pure classification problems, a predicted value that is farther from the cutoff (and in the same direction)
          // is not an error, and hence should not normally be backpropagated
          const float pred( PREDICTION( result_id));
          const float cutoff( m_ScaledCutoffs( result_offset));
          const int result_over_cutoff( *itr_result > cutoff);
          const int pred_over_cutoff( pred > cutoff);
          const int pred_over_result( pred >= *itr_result);
          const int sum_over( result_over_cutoff + pred_over_cutoff + pred_over_result);
          if( sum_over == 0 || sum_over == 3)
          {
            // either prediction > result > cutoff or
            // prediction < result < cutoff
            er = 0.0;
          }
        }
        const float abs_er( math::Absolute( er));
        const float tolerance( *itr_tol);
        *itr_error_ave += abs_er;
        if( abs_er <= tolerance)
        {
          er = 0.0;
        }
        else
        {
          // absolute error was beyond the tolerance; backpropagate
          backprop_should_continue = true;
          if( m_ReduceErrorByTolerance)
          {
            if( er > 0.0)
            {
              er -= tolerance;
            }
            else
            {
              er += tolerance;
            }
          }
        }
      }

      // return true if any part of ERROR remains to be backpropagated
      return backprop_should_continue;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::FinalizeRound()
    {
      // accumulate results
      for( size_t thread_number( 1); thread_number < m_NumberThreads; ++thread_number)
      {
        for( size_t i( 0); i < m_ResultsSize; ++i)
        {
          m_ThreadResultAverageError( 0)( i) += m_ThreadResultAverageError( thread_number)( i);
        }
      }

      // update tolerance
      for( size_t i( 0); i < m_ResultsSize; ++i)
      {
        const float average_error( m_ThreadResultAverageError( 0)( i).GetAverage());
        BCL_MessageVrb
        (
          "Average error for result " + util::Format()( i)
          + ": " + util::Format()( average_error)
        );
        const float target_tolerance( average_error + m_ToleranceMinusError);
        const float current_tolerance( m_RescaledTolerance( i));
        const float new_tolerance_unbounded
        (
          target_tolerance * m_AdaptationRate + current_tolerance * ( 1.0 - m_AdaptationRate)
        );
        m_RescaledTolerance( i) = std::max( m_RescaledMinTolerance( i), std::min( m_RescaledMaxTolerance( i), new_tolerance_unbounded));
      }

      // reset thread-specific arrays
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        for( size_t i( 0); i < m_ResultsSize; ++i)
        {
          m_ThreadResultAverageError( thread_number)( i).Reset();
        }
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationAdaptiveTolerance::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Always back-propagating features that are mispredicted beyond the tolerance. "
        "If the predicted value has less than the current tolerance of error, backpropagation will not occur. "
        "This implementation adapts the tolerance based on the average error (per result column) each round."
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to use this triager for",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "max tolerance",
        "Tolerance will not be allowed to go above this value.  This will also be the initial tolerance value",
        io::Serialization::GetAgent( &m_MaxTolerance)
      );
      parameters.AddInitializer
      (
        "min tolerance",
        "Tolerance will not be allowed to go below this value",
        io::Serialization::GetAgent( &m_MinTolerance),
        "0"
      );
      parameters.AddInitializer
      (
        "initial tolerance",
        "Initial tolerance value.  (If the min tolerance is larger than this value, it will be used instead)",
        io::Serialization::GetAgent( &m_InitialTolerance),
        "0"
      );
      parameters.AddInitializer
      (
        "adaptation rate",
        "Tolerance for each result will move at this rate to the current target tolerance (error multiplier * average error for this result)",
        io::Serialization::GetAgentWithRange( &m_AdaptationRate, 0.0, 1.0),
        "0.5"
      );
      parameters.AddInitializer
      (
        "tolerance offset from error",
        "this number is added to the average error per column to get the target tolerance",
        io::Serialization::GetAgent( &m_ToleranceMinusError),
        "0"
      );
      parameters.AddInitializer
      (
        "reduce error by tolerance",
        "Whether to reduce the error for backpropagated features by the tolerance when the feature is backpropagated",
        io::Serialization::GetAgent( &m_ReduceErrorByTolerance),
        "True"
      );
      parameters.AddInitializer
      (
        "pure classification",
        "True if result values should be pretended to be binary, depending on their side of the cutoff",
        io::Serialization::GetAgent( &m_PureClassification),
        "False"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_balanced.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationBalanced::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationBalanced()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationBalanced::NeuralNetworkSelectiveBackpropagationBalanced() :
      m_MinFractionPositives( 0.0),
      m_MinFractionNegatives( 0.0),
      m_BackpropAllFalsePositives( false),
      m_BackpropAllFalseNegatives( false),
      m_PureClassification( false),
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_EnrichmentCutoff( 0.0),
      m_StabilityRatio( 2),
      m_MaxNetRoundsFP( 5),
      m_MaxNetRoundsFN( 30),
      m_ResultsSize( 1000),
      m_DatasetSize( 1),
      m_NumberThreads( 0),
      m_Parity( true),
      m_RoundNumber( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationBalanced copied from this instance
    NeuralNetworkSelectiveBackpropagationBalanced *NeuralNetworkSelectiveBackpropagationBalanced::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationBalanced( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationBalanced::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationBalanced::GetAlias() const
    {
      static const std::string s_name( "Balanced");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationBalanced::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization

      // bound m_ResultEndID by the results size
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      // compute internal results size
      m_ResultsSize = m_ResultEndID - m_ResultStartID;

      m_RoundNumber = 1;

      // store required data
      m_NumberThreads = NUMBER_THREADS;
      m_Results = RESCALED_DATA.GetResultsPtr()->GetMatrix();
      m_PredictedValue = m_Results;
      m_DatasetSize = RESCALED_DATA.GetResultsPtr()->GetNumberFeatures();
      m_Parity = OBJECTIVE.GetRankingParity();

      // set up sizes of all internally-held members
      m_ScaledCutoffs = linal::Vector< float>( m_ResultsSize);
      m_MaxDelta = m_ScaledCutoffs;
      m_IncorrectTallyAboveCutoff.Resize( m_NumberThreads);
      m_IncorrectTallyAboveCutoff.SetAllElements( linal::Vector< size_t>( m_ResultsSize, size_t( 0)));
      m_IncorrectTallyBelowCutoff = m_IncorrectTallyAboveCutoff;
      m_BackpropagatedTallyAboveCutoff = m_BackpropagatedTallyBelowCutoff = m_IncorrectTallyAboveCutoff;
      m_BelowCutoffCount = m_AboveCutoffCount = linal::Vector< size_t>( m_ResultsSize, size_t( 0));
      m_BackPropPreference = linal::Matrix< float>( m_DatasetSize, m_ResultsSize, float( 1.0));
      m_NetRoundsWrong = linal::Matrix< size_t>( m_DatasetSize, m_ResultsSize, size_t( 0));
      m_ChangeLastRound = linal::Matrix< float>( m_DatasetSize, m_ResultsSize, float( 1.0));
      m_ThreadMaxDelta.Resize( m_NumberThreads);
      m_ThreadMaxDelta.SetAllElements( storage::Vector< float>( m_ResultsSize, float( 0.0)));
      m_IgnoredFalsePositives.Resize( m_NumberThreads);
      m_IgnoredFalsePositives.SetAllElements( storage::Vector< size_t>( m_ResultsSize, size_t( 0)));
      m_IgnoredFalseNegatives = m_IgnoredFalsePositives;

      // get scaling and cutoff information
      const float cutoff( OBJECTIVE.GetThreshold());
      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESCALED_DATA.GetResultsPtr()->GetScaling());

      // take data about the dataset
      for( size_t result( 0); result < m_ResultsSize; ++result)
      {
        // get the scaled cutoff
        m_ScaledCutoffs( result) = results_scaling->RescaleValue( result, cutoff);

        // compute average distance from cutoff above and below the cutoff
        math::RunningAverage< float> ave_high, ave_low;
        float max_delta_high( 0), max_delta_low( 0);

        // count the number of times results are seen above cutoff, also find the max/min of the dataset
        for( size_t i( 0); i < m_DatasetSize; ++i)
        {
          const float result_value( m_Results( i, result));
          if( result_value >= m_ScaledCutoffs( result))
          {
            ++m_AboveCutoffCount( result);
            max_delta_high = std::max( result_value, max_delta_high);
          }
          else
          {
            ++m_BelowCutoffCount( result);
            max_delta_low = std::min( result_value, max_delta_low);
          }
        }

        // Initialize the max delta with the min/max results seen in the dataset
        if( m_AboveCutoffCount( result))
        {
          m_MaxDelta( result) = max_delta_high - m_ScaledCutoffs( result);
        }
        if( m_BelowCutoffCount( result))
        {
          m_MaxDelta( result) = std::max( m_MaxDelta( result), m_ScaledCutoffs( result) - max_delta_low);
        }

        // Print off counts above and below the cutoff
        BCL_MessageStd
        (
          "For result # " + util::Format()( result + m_ResultStartID)
          + " #Above cutoff: " + util::Format()( m_AboveCutoffCount( result))
          + " #Below: " + util::Format()( m_BelowCutoffCount( result))
        );

        // determine initial backpropagation probabilities so as to balance the number of positives and negatives
        // backpropagated
        float bp_prob_high( 1.0);
        float bp_prob_low( 1.0);

        // handle datasets that have overrepresentation factors greater than 3/2.  Datasets with smaller
        // overrepresentation factors typically do not require balancing on the first round
        if( m_AboveCutoffCount( result) > m_BelowCutoffCount( result) * 3 / 2)
        {
          bp_prob_high
            = std::min
              (
                float( 1.0),
                float( m_StabilityRatio * m_BelowCutoffCount( result))
                / float( std::max( m_AboveCutoffCount( result), size_t( 1)))
              );
        }
        else if( m_AboveCutoffCount( result) * 3 / 2 < m_BelowCutoffCount( result))
        {
          bp_prob_low
            = std::min
              (
                float( 1.0),
                float( m_StabilityRatio * m_AboveCutoffCount( result))
                / float( std::max( m_BelowCutoffCount( result), size_t( 1)))
              );
        }

        // set initial backpropagation propensities deterministically, in accordance with the backpropagation
        // probabilities necessary to balance the solution.  The first instance of the class is always backpropagated
        float current_bp_low( 0.0), current_bp_high( 0.0);
        for( size_t i( 0); i < m_DatasetSize; ++i)
        {
          if( m_Results( i, result) >= m_ScaledCutoffs( result))
          {
            // positives
            current_bp_high += bp_prob_high;
            m_BackPropPreference( i, result) = current_bp_high;
            if( current_bp_high >= 0.0)
            {
              current_bp_high -= 1.0;
            }
          }
          else
          {
            // negatives
            current_bp_low += bp_prob_low;
            m_BackPropPreference( i, result) = current_bp_low;
            if( current_bp_low >= 0.0)
            {
              current_bp_low -= 1.0;
            }
          }
        }

        // avoid numerical issues that arise if there was only one class represented in the data
        if( !m_AboveCutoffCount( result))
        {
          ++m_AboveCutoffCount( result);
          --m_BelowCutoffCount( result);
        }
        else if( !m_BelowCutoffCount( result))
        {
          --m_AboveCutoffCount( result);
          ++m_BelowCutoffCount( result);
        }
      }
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationBalanced::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      // get a reference to the result and prediction row, already offset by m_ResultStartID
      linal::VectorConstReference< float> result_row( m_ResultsSize, m_Results[ FEATURE_ID] + m_ResultStartID);
      linal::VectorConstReference< float> prediction_row( m_ResultsSize, PREDICTION.Begin() + m_ResultStartID);

      // get references to the tallying arrays for this thread
      linal::Vector< size_t> &incorrect_above_tally( m_IncorrectTallyAboveCutoff( THREAD_ID));
      linal::Vector< size_t> &incorrect_below_tally( m_IncorrectTallyBelowCutoff( THREAD_ID));
      linal::Vector< size_t> &bp_above_tally( m_BackpropagatedTallyAboveCutoff( THREAD_ID));
      linal::Vector< size_t> &bp_below_tally( m_BackpropagatedTallyBelowCutoff( THREAD_ID));
      storage::Vector< float> &max_delta( m_ThreadMaxDelta( THREAD_ID));

      bool backprop_should_continue( false);

      // for each result
      for( size_t result_id( 0); result_id < m_ResultsSize; ++result_id)
      {
        // get the actual result, prediction, and cutoff
        const float actual_result_value( result_row( result_id));
        const float prediction( prediction_row( result_id));
        const float cutoff( m_ScaledCutoffs( result_id));
        float &last_round_prediction( m_PredictedValue( FEATURE_ID, result_id));

        // get reference to # of rounds wrong
        size_t &number_rounds_wrong( m_NetRoundsWrong( FEATURE_ID, result_id));

        // determine whether the value was above cutoff
        const bool is_high( actual_result_value >= cutoff);

        // compute the difference between last rounds prediction and this rounds'
        const float delta_prediction( math::Absolute( prediction - last_round_prediction));

        // keep track of how much the prediction has changed over the last round
        m_ChangeLastRound( FEATURE_ID, result_id) = delta_prediction;

        // update the threads maximum delta by this point
        max_delta( result_id) = std::max( max_delta( result_id), delta_prediction);

        // determine whether this exemplar must be backpropagated
        bool backprop_this_column( m_BackPropPreference( FEATURE_ID, result_id) >= 0.0);

        // determine whether the predicted value is on the correct side of the cutoff
        // note that for pure classification tasks, a cutoff of 0.5 is imposed.  This is only the proper behavior though
        // if the objective function is a rank classifier though
        if( is_high != ( prediction >= ( m_PureClassification ? 0.5 : cutoff)))
        {
          // update incorrect tally
          ++( is_high ? incorrect_above_tally : incorrect_below_tally)( result_id);

          // determine whether we're headed in the right direction
          const bool headed_in_right_direction
          (
            ( is_high ? prediction > last_round_prediction : prediction < last_round_prediction)
            && prediction > 0.2
            && prediction < 0.8
          );

          // determine this exemplar must be backpropagated anyway because the user set the all fp or all fn flag
          if( is_high == m_Parity)
          {
            if( m_BackpropAllFalseNegatives && m_MaxNetRoundsFN)
            {
              if( number_rounds_wrong < m_MaxNetRoundsFN || headed_in_right_direction)
              {
                backprop_this_column = true;
              }
              else
              {
                backprop_this_column = false;
                ++m_IgnoredFalseNegatives( THREAD_ID)( result_id);
                if( number_rounds_wrong >= m_MaxNetRoundsFP)
                {
                  --number_rounds_wrong;
                }
              }
            }
          }
          else if( m_BackpropAllFalsePositives && m_MaxNetRoundsFP)
          {
            if( number_rounds_wrong < m_MaxNetRoundsFP || headed_in_right_direction)
            {
              backprop_this_column = true;
            }
            else
            {
              backprop_this_column = false;
              ++m_IgnoredFalsePositives( THREAD_ID)( result_id);
              if( number_rounds_wrong >= m_MaxNetRoundsFN)
              {
                --number_rounds_wrong;
              }
            }
          }
          ++number_rounds_wrong;
        }
        else if( number_rounds_wrong)
        {
          --number_rounds_wrong;
        }

        // update last round prediction
        last_round_prediction = prediction;

        // handle the case that this column will be backpropagated
        if( backprop_this_column)
        {
          // feature must be backpropagated
          backprop_should_continue = true;

          // update backpropagated tally
          ++( is_high ? bp_above_tally : bp_below_tally)( result_id);

          // modify the backpropagated error for pure classification tasks so that all exemplars have a result of 0 or 1
          if( m_PureClassification)
          {
            ERROR( result_id + m_ResultStartID) = ( is_high ? 1.0 : 0.0) - prediction;
          }
        }
        else
        {
          // this column is not to be backpropagated; so set the error to 0 (since it is possible other columns may be
          // backpropagated, but it has already been decided not to backpropagate this column)
          //ERROR( result_id + m_ResultStartID) = 0.0;
        }
      }

      // return true if any part of ERROR remains to be backpropagated
      return backprop_should_continue;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationBalanced::FinalizeRound()
    {
      // only keep the average change over the last 10 rounds ( 10 * 9 / 2 = 45)
      if( m_AverageChange.GetWeight() > 10.0)
      {
        m_AverageChange.SetWeight( 9.0);
      }

      // update average change
      m_AverageChange += m_ChangeLastRound;

      // increment round number
      m_RoundNumber += 1;

      // accumulate results
      for( size_t thread_number( 1); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectTallyAboveCutoff( 0) += m_IncorrectTallyAboveCutoff( thread_number);
        m_IncorrectTallyBelowCutoff( 0) += m_IncorrectTallyBelowCutoff( thread_number);
        m_BackpropagatedTallyAboveCutoff( 0) += m_BackpropagatedTallyAboveCutoff( thread_number);
        m_BackpropagatedTallyBelowCutoff( 0) += m_BackpropagatedTallyBelowCutoff( thread_number);
        m_IgnoredFalseNegatives( 0) += m_IgnoredFalseNegatives( thread_number);
        m_IgnoredFalsePositives( 0) += m_IgnoredFalsePositives( thread_number);
        for( size_t i( 0); i < m_ResultsSize; ++i)
        {
          m_ThreadMaxDelta( 0)( i) = std::max( m_ThreadMaxDelta( 0)( i), m_ThreadMaxDelta( thread_number)( i));
        }
      }

      const size_t max_ignored_length( std::min( size_t( 1000), std::max( m_MaxNetRoundsFN, m_MaxNetRoundsFP)));
      linal::Vector< size_t> histogram_high( max_ignored_length + 1, size_t( 0));
      linal::Vector< size_t> histogram_low( max_ignored_length + 1, size_t( 0));
      double reset_rounds_fraction( 1.0 / double( max_ignored_length));

      const linal::Matrix< float> &avg_change( m_AverageChange.GetAverage());
      for( size_t i( 0); i < m_ResultsSize; ++i)
      {
        // store the max deltas
        m_MaxDelta( i) = m_ThreadMaxDelta( 0)( i);
        const float max_delta( m_MaxDelta( i));

        // update backpropagation preference
        for( size_t j( 0); j < m_DatasetSize; ++j)
        {
          m_BackPropPreference( j, i) = avg_change( j, i) / max_delta;
        }

        // compute the previous round's inaccuracy above / below cutoff, to display for the user
        const float inaccuracy_above( float( m_IncorrectTallyAboveCutoff( 0)( i)) / float( m_AboveCutoffCount( i)));
        const float inaccuracy_below( float( m_IncorrectTallyBelowCutoff( 0)( i)) / float( m_BelowCutoffCount( i)));

        // compute the absolute minimum and maximum number of exemplars that should be backpropagated based on the user's
        // given cutoffs
        const float max_above( m_AboveCutoffCount( i));
        const float max_below( m_BelowCutoffCount( i));
        const float min_above( m_AboveCutoffCount( i) * ( m_Parity ? m_MinFractionPositives : m_MinFractionNegatives));
        const float min_below( m_BelowCutoffCount( i) * ( !m_Parity ? m_MinFractionPositives : m_MinFractionNegatives));

        // save the # inaccurate above and below the cutoff last turn
        const float n_inaccurate_above( m_IncorrectTallyAboveCutoff( 0)( i) - ( m_Parity ? m_IgnoredFalseNegatives( 0)( i) : m_IgnoredFalsePositives( 0)( i)));
        const float n_inaccurate_below( m_IncorrectTallyBelowCutoff( 0)( i) - ( !m_Parity ? m_IgnoredFalseNegatives( 0)( i) : m_IgnoredFalsePositives( 0)( i)));

        // expected_high is the number of features that must be backpropagated next turn that are above the cutoff
        // it is calculated as 1/2 the number that were backpropagated last turn + the optimal # that should be calculated
        // to optimize enrichment (if enrichment cutoff was set) or to ensure balancing
        // Carrying 1/2 the number that were backpropagated last turn forward stabilizes the calculation against, e.g.
        // predicting all the positives correct one turn, then all the negatives correct the following turn
        float expected_high( 0.5 * m_BackpropagatedTallyAboveCutoff( 0)( i));
        float expected_low( 0.5 * m_BackpropagatedTallyBelowCutoff( 0)( i));
        if( m_EnrichmentCutoff)
        {
          // enrichment cutoff set; optimize for enrichment

          // compute the current contingency matrix for the training data using the experimental cutoff
          float false_positives( 0), false_negatives( 0);

          // Enrichment at a constant cutoff (e.g. TP+FP = Constant) = TP/C = 1-FP/C
          // taking the derivative with respect to TP and FP, it should be clear that the change in enrichment is identical
          // in value (but opposite in sign) for exchanging a TP for an FP.  Given that we currently have TP=x, and FP=y,
          // there are FN chances to increase enrichment by backpropagating a positive, and min(FN,FP) chances
          // to increase the value by BPing an FP.
          float chances_to_increase_above( 0), chances_to_increase_below( 0);
          if( m_Parity)
          {
            false_positives = n_inaccurate_below;
            false_negatives = n_inaccurate_above;
            chances_to_increase_above = false_negatives + 1.0;
            chances_to_increase_below = std::min( false_negatives, false_positives) + 1.0;
          }
          else
          {
            false_positives = n_inaccurate_above;
            false_negatives = n_inaccurate_below;
            chances_to_increase_above = std::min( false_negatives, false_positives) + 1.0;
            chances_to_increase_below = false_negatives + 1.0;
          }

          // if there are more incorrect results than the user-specified enrichment cutoff would allow,
          // then do not optimize enrichment,
          // otherwise this leads to an instability where the optimum is reached by predicting everything positive
          // Instead, just backpropagate some factor of the number inaccurate on either side of the cutoff
          if( m_EnrichmentCutoff * m_DatasetSize < ( false_negatives + false_positives))
          {
            chances_to_increase_above = n_inaccurate_above * m_StabilityRatio;
            chances_to_increase_below = n_inaccurate_below * m_StabilityRatio;
          }
          else
          {
            // optimize directly for enrichment
            chances_to_increase_above = chances_to_increase_above * m_StabilityRatio;
            chances_to_increase_below = chances_to_increase_below * m_StabilityRatio;
          }

          // bound by the user-specified limits
          chances_to_increase_above = std::min( max_above, std::max( chances_to_increase_above, min_above));
          chances_to_increase_below = std::min( max_below, std::max( chances_to_increase_below, min_below));

          // add the 1/2 the optimal value to the expected value to provide stability
          expected_high += 0.5 * chances_to_increase_above;
          expected_low += 0.5 * chances_to_increase_below;
        }
        else
        {
          float desired_high_steady_state( n_inaccurate_above * m_StabilityRatio);
          float desired_low_steady_state( n_inaccurate_below * m_StabilityRatio);

          // bound by the user-specified limits
          desired_high_steady_state = std::min( max_above, std::max( desired_high_steady_state, min_above));
          desired_low_steady_state = std::min( max_below, std::max( desired_low_steady_state, min_below));

          expected_high += 0.5 * desired_high_steady_state;
          expected_low += 0.5 * desired_low_steady_state;
        }

        // sort the next round's backpropagation preferences above and below the cutoff
        // Compute m_BackPropPreference( j, i) -= x with x such that the number of exemplars with
        // m_BackPropPreference( j, i) >= 0 is the desired number of exemplars for that class (expected above or below)
        storage::Vector< float> high_values, low_values;
        high_values.AllocateMemory( m_AboveCutoffCount( i));
        low_values.AllocateMemory( m_BelowCutoffCount( i));
        for( size_t j( 0); j < m_DatasetSize; ++j)
        {
          const float result( m_Results( j, i));
          if( result >= m_ScaledCutoffs( i))
          {
            high_values.PushBack( m_BackPropPreference( j, i));
          }
          else
          {
            low_values.PushBack( m_BackPropPreference( j, i));
          }
        }
        // sort the backpropagation preferences above and below this results' cutoff
        high_values.Sort( std::greater< float>());
        low_values.Sort( std::greater< float>());

        // choose the cutoff for exemplars below cutoff such that there are expected_low number of values >= 0
        const float cutoff_low
        (
          low_values.IsEmpty()
          ? float( 0.0)
          : low_values( std::min( std::max( size_t( expected_low + 0.5), size_t( 1)) - 1, low_values.GetSize() - 1))
        );

        // choose the cutoff for exemplars abpve cutoff such that there are expected_high number of values >= 0
        const float cutoff_high
        (
          high_values.IsEmpty()
          ? float( 0.0)
          : high_values( std::min( std::max( size_t( expected_high + 0.5), size_t( 1)) - 1, high_values.GetSize() - 1))
        );

        // subtract the cutoff
        for( size_t j( 0); j < m_DatasetSize; ++j)
        {
          const float result( m_Results( j, i));
          if( result >= m_ScaledCutoffs( i))
          {
            m_BackPropPreference( j, i) -= cutoff_high;
            ++histogram_high( std::min( m_NetRoundsWrong( j, i), max_ignored_length));
          }
          else
          {
            m_BackPropPreference( j, i) -= cutoff_low;
            ++histogram_low( std::min( m_NetRoundsWrong( j, i), max_ignored_length));
          }
          // reset a certain fraction of those features at the limit to prevent all features from ending up at the limits
          if( m_NetRoundsWrong( j, i) >= max_ignored_length && random::GetGlobalRandom().Double() < reset_rounds_fraction)
          {
            m_NetRoundsWrong( j, i) = 0;
          }
        }

        BCL_MessageVrb
        (
          "Last round inaccuracy on result # " + util::Format()( i + m_ResultStartID)
          + " above cutoff = " + util::Format()( inaccuracy_above)
          + " below cutoff = " + util::Format()( inaccuracy_below)
          + " # Incorrect above/below: " + util::Format()( n_inaccurate_above)
          + " " + util::Format()( n_inaccurate_below)
          + " # BP above / below: " + util::Format()( m_BackpropagatedTallyAboveCutoff( 0)( i))
          + " " + util::Format()( m_BackpropagatedTallyBelowCutoff( 0)( i))
          + " Max change: " + util::Format()( m_MaxDelta( i))
          + " Ignored FN / FP: " + util::Format()( m_IgnoredFalseNegatives( 0)( i))
          + " " + util::Format()( m_IgnoredFalsePositives( 0)( i))
        );
      }

      BCL_MessageVrb( "Histogram of # of rounds wrong for those above cutoff: " + util::Format()( histogram_high));
      BCL_MessageVrb( "Histogram of # of rounds wrong for those below cutoff: " + util::Format()( histogram_low));

      // reset thread-specific arrays
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectTallyAboveCutoff( thread_number) = size_t( 0);
        m_IncorrectTallyBelowCutoff( thread_number) = size_t( 0);
        m_BackpropagatedTallyAboveCutoff( thread_number) = size_t( 0);
        m_BackpropagatedTallyBelowCutoff( thread_number) = size_t( 0);
        m_ThreadMaxDelta( thread_number).SetAllElements( 0.0);
        m_IgnoredFalseNegatives( thread_number) = size_t( 0);
        m_IgnoredFalsePositives( thread_number) = size_t( 0);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationBalanced::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Backpropagates preferrentially according to sensitivity (how rapidly the exemplar's predicted value is changing) "
        "Can also force backpropagation of all false negatives and/or positives, and can specify an enrichment cutoff if"
        " a ranked classifier is desired"
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to use this triager for",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "minp",
        "Minimum fraction of positives to backpropagate.  Sometimes to force training on every positive",
        io::Serialization::GetAgentWithRange( &m_MinFractionPositives, float( 0.0), float( 1.0)),
        "0.0"
      );
      parameters.AddInitializer
      (
        "minn",
        "Minimum fraction of negatives to backpropagate. Used to stabilize network training",
        io::Serialization::GetAgentWithRange( &m_MinFractionNegatives, float( 0.0), float( 1.0)),
        "0.0"
      );
      parameters.AddInitializer
      (
        "max fp rounds",
        "# of net rounds wrong after which to stop sensitivity-independent backpropagation of false positives ",
        io::Serialization::GetAgent( &m_MaxNetRoundsFP),
        "5"
      );
      parameters.AddInitializer
      (
        "max fn rounds",
        "# of net rounds wrong after which to stop sensitivity-independent backpropagation of false negatives ",
        io::Serialization::GetAgent( &m_MaxNetRoundsFN),
        "30"
      );
      parameters.AddInitializer
      (
        "all fp",
        "Set to true to always backpropagate all false positives.  Sometimes useful to improve training for enrichment",
        io::Serialization::GetAgent( &m_BackpropAllFalsePositives),
        "False"
      );
      parameters.AddInitializer
      (
        "all fn",
        "Set to true to always backpropagate all false negatives.  Sometimes useful to improve training for enrichment",
        io::Serialization::GetAgent( &m_BackpropAllFalseNegatives),
        "False"
      );
      parameters.AddInitializer
      (
        "enrichment max",
        "flag to deliberately optimize for enrichment with the given cutoff, leave 0 to focus on balancing rather than enrichment",
        io::Serialization::GetAgentWithRange( &m_EnrichmentCutoff, float( 0.0), float( 1.0)),
        "0.0"
      );
      parameters.AddInitializer
      (
        "pure classification",
        "flag; set to ignore the result values when backpropagating, instead, backprop 0 or 1 (for above cutoff)",
        io::Serialization::GetAgent( &m_PureClassification),
        "False"
      );
      parameters.AddInitializer
      (
        "stability",
        "Ratio of FP+FN backpropagated due to incorrectness to those backpropped due to rapid change. "
        "Use 1 if the data are very clean and representative of the overall space; larger if not",
        io::Serialization::GetAgent( &m_StabilityRatio),
        "2"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_default.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationDefault::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationDefault()
      )
    );

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationDefault copied from this instance
    NeuralNetworkSelectiveBackpropagationDefault *NeuralNetworkSelectiveBackpropagationDefault::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationDefault( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationDefault::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationDefault::GetAlias() const
    {
      static const std::string s_Name( "All");
      return s_Name;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationDefault::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Backpropagate all data, all the time");
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_hybrid.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_statistics.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationHybrid::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationHybrid()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationHybrid::NeuralNetworkSelectiveBackpropagationHybrid() :
      m_PureClassification( false),
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_EnrichmentCutoff( 0.0),
      m_StabilityRatio( 2),
      m_SensitivityWeight( 0.5),
      m_FalsePositiveBonus( 0),
      m_FalseNegativeBonus( 0),
      m_StartWithAll( false),
      m_ResultsSize( 1000),
      m_DatasetSize( 1),
      m_NumberThreads( 0),
      m_Parity( true),
      m_RoundNumber( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationHybrid copied from this instance
    NeuralNetworkSelectiveBackpropagationHybrid *NeuralNetworkSelectiveBackpropagationHybrid::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationHybrid( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationHybrid::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationHybrid::GetAlias() const
    {
      static const std::string s_name( "Hybrid");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationHybrid::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization

      // bound m_ResultEndID by the results size
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      // compute internal results size
      m_ResultsSize = m_ResultEndID - m_ResultStartID;

      m_RoundNumber = 1;

      // store required data
      m_NumberThreads = NUMBER_THREADS;
      m_Results = RESCALED_DATA.GetResultsPtr()->GetMatrix();
      m_PredictedValue = m_Results;
      m_DatasetSize = RESCALED_DATA.GetResultsPtr()->GetNumberFeatures();
      m_Parity = OBJECTIVE.GetRankingParity();

      // set up sizes of all internally-held members
      m_ScaledCutoffs = linal::Vector< float>( m_ResultsSize);
      m_MaxDelta = m_ScaledCutoffs;
      m_IncorrectTallyAboveCutoff.Resize( m_NumberThreads);
      m_IncorrectTallyAboveCutoff.SetAllElements( linal::Vector< size_t>( m_ResultsSize, size_t( 0)));
      m_IncorrectTallyBelowCutoff = m_IncorrectTallyAboveCutoff;
      m_BackpropagatedTallyAboveCutoff = m_BackpropagatedTallyBelowCutoff = m_IncorrectTallyAboveCutoff;
      m_BelowCutoffCount = m_AboveCutoffCount = linal::Vector< size_t>( m_ResultsSize, size_t( 0));
      m_BackPropPreference = linal::Matrix< float>( m_DatasetSize, m_ResultsSize, float( 1.0));
      m_ChangeLastRound = linal::Matrix< float>( m_DatasetSize, m_ResultsSize, float( 1.0));
      m_AverageChange = linal::Matrix< float>( m_DatasetSize, m_ResultsSize, float( 0.0));
      m_ThreadMaxDelta.Resize( m_NumberThreads);
      m_ThreadMaxDelta.SetAllElements( storage::Vector< float>( m_ResultsSize, float( 0.0)));
      m_ThresholdBPHigh = m_ThresholdBPLow = m_SensitivityScalingHigh = m_SensitivityScalingLow = linal::Vector< float>( m_ResultsSize, 0.5);

      // get scaling and cutoff information
      const float cutoff( OBJECTIVE.GetThreshold());
      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESCALED_DATA.GetResultsPtr()->GetScaling());

      // take data about the dataset
      for( size_t result( 0); result < m_ResultsSize; ++result)
      {
        // get the scaled cutoff
        m_ScaledCutoffs( result) = results_scaling->RescaleValue( result, cutoff);

        // compute average distance from cutoff above and below the cutoff
        math::RunningAverage< float> ave_high, ave_low;
        float max_delta_high( 0), max_delta_low( 0);

        // count the number of times results are seen above cutoff, also find the max/min of the dataset
        for( size_t i( 0); i < m_DatasetSize; ++i)
        {
          const float result_value( m_Results( i, result));
          if( result_value >= m_ScaledCutoffs( result))
          {
            ++m_AboveCutoffCount( result);
            max_delta_high = std::max( result_value, max_delta_high);
          }
          else
          {
            ++m_BelowCutoffCount( result);
            max_delta_low = std::min( result_value, max_delta_low);
          }
        }

        // Initialize the max delta with the min/max results seen in the dataset
        if( m_AboveCutoffCount( result))
        {
          m_MaxDelta( result) = max_delta_high - m_ScaledCutoffs( result);
        }
        if( m_BelowCutoffCount( result))
        {
          m_MaxDelta( result) = std::max( m_MaxDelta( result), m_ScaledCutoffs( result) - max_delta_low);
        }

        // Print off counts above and below the cutoff
        BCL_MessageStd
        (
          "For result # " + util::Format()( result + m_ResultStartID)
          + " #Above cutoff: " + util::Format()( m_AboveCutoffCount( result))
          + " #Below: " + util::Format()( m_BelowCutoffCount( result))
        );

        // determine initial backpropagation probabilities so as to balance the number of positives and negatives
        // backpropagated
        float bp_prob_high( 1.0);
        float bp_prob_low( 1.0);

        // handle datasets that have overrepresentation factors greater than 3/2.  Datasets with smaller
        // overrepresentation factors typically do not require balancing on the first round
        if( m_AboveCutoffCount( result) > m_BelowCutoffCount( result) * 3 / 2)
        {
          bp_prob_high
            = std::min
              (
                float( 1.0),
                float( m_StabilityRatio * m_BelowCutoffCount( result))
                / float( std::max( m_AboveCutoffCount( result), size_t( 1)))
              );
        }
        else if( m_AboveCutoffCount( result) * 3 / 2 < m_BelowCutoffCount( result))
        {
          bp_prob_low
            = std::min
              (
                float( 1.0),
                float( m_StabilityRatio * m_AboveCutoffCount( result))
                / float( std::max( m_BelowCutoffCount( result), size_t( 1)))
              );
        }

        // set initial backpropagation propensities deterministically, in accordance with the backpropagation
        // probabilities necessary to balance the solution.  The first instance of the class is always backpropagated
        float current_bp_low( 0.0), current_bp_high( 0.0);
        for( size_t i( 0); i < m_DatasetSize; ++i)
        {
          if( m_Results( i, result) >= m_ScaledCutoffs( result))
          {
            // positives
            current_bp_high += bp_prob_high;
            m_BackPropPreference( i, result) = current_bp_high;
            if( current_bp_high >= 0.0)
            {
              current_bp_high -= 1.0;
            }
          }
          else
          {
            // negatives
            current_bp_low += bp_prob_low;
            m_BackPropPreference( i, result) = current_bp_low;
            if( current_bp_low >= 0.0)
            {
              current_bp_low -= 1.0;
            }
          }
        }

        // avoid numerical issues that arise if there was only one class represented in the data
        if( !m_AboveCutoffCount( result))
        {
          ++m_AboveCutoffCount( result);
          --m_BelowCutoffCount( result);
        }
        else if( !m_BelowCutoffCount( result))
        {
          --m_AboveCutoffCount( result);
          ++m_BelowCutoffCount( result);
        }
      }
      m_EffectiveCutoffs = m_ScaledCutoffs;
      if( m_PureClassification)
      {
        m_EffectiveCutoffs = float( 0.5);
      }
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationHybrid::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      // get a reference to the result and prediction row, already offset by m_ResultStartID
      linal::VectorConstReference< float> result_row( m_ResultsSize, m_Results[ FEATURE_ID] + m_ResultStartID);
      linal::VectorConstReference< float> prediction_row( m_ResultsSize, PREDICTION.Begin() + m_ResultStartID);

      // get references to the tallying arrays for this thread
      linal::Vector< size_t> &incorrect_above_tally( m_IncorrectTallyAboveCutoff( THREAD_ID));
      linal::Vector< size_t> &incorrect_below_tally( m_IncorrectTallyBelowCutoff( THREAD_ID));
      linal::Vector< size_t> &bp_above_tally( m_BackpropagatedTallyAboveCutoff( THREAD_ID));
      linal::Vector< size_t> &bp_below_tally( m_BackpropagatedTallyBelowCutoff( THREAD_ID));
      storage::Vector< float> &max_delta( m_ThreadMaxDelta( THREAD_ID));

      bool backprop_should_continue( false);

      // for each result
      for( size_t result_id( 0); result_id < m_ResultsSize; ++result_id)
      {
        // get the actual result, prediction, and cutoff
        const float actual_result_value( result_row( result_id));
        const float prediction( prediction_row( result_id));
        const float cutoff( m_ScaledCutoffs( result_id));
        const float effective_cutoff( m_EffectiveCutoffs( result_id));
        float &last_round_prediction( m_PredictedValue( FEATURE_ID, result_id));

        // determine whether the value was above cutoff
        const bool is_high( actual_result_value >= cutoff);

        // compute the difference between last rounds prediction and this rounds'
        const float delta_prediction( math::Absolute( prediction - last_round_prediction));
        const float delta_cutoff( math::Absolute( prediction - effective_cutoff));

        // keep track of how much the prediction has changed over the last round
        m_ChangeLastRound( FEATURE_ID, result_id) = delta_prediction;

        // update the threads maximum delta by this point
        max_delta( result_id) = std::max( max_delta( result_id), delta_prediction);

        // determine whether this example was predicted incorrectly
        const bool is_predicted_wrong( is_high != ( prediction >= effective_cutoff));

        // determine whether this exemplar must be backpropagated
        //bool backprop_this_column( m_BackPropPreference( FEATURE_ID, result_id) >= 0.0);
        bool backprop_this_column( m_RoundNumber == 1 && m_BackPropPreference( FEATURE_ID, result_id) >= 0);
        if( is_high)
        {
          const float bonus( is_predicted_wrong && m_Parity ? m_FalseNegativeBonus : 0.0);
          const float sensitivity_addition
          (
            m_SensitivityWeight * m_SensitivityScalingHigh( result_id)
            * ( m_AverageChange( FEATURE_ID, result_id) + delta_prediction) / 2.0
          );
          const float importance( delta_cutoff - bonus - sensitivity_addition);
          m_BackPropPreference( FEATURE_ID, result_id) = importance;
          if( m_RoundNumber > 1 || m_StartWithAll)
          {
            backprop_this_column = importance <= m_ThresholdBPHigh( result_id);
          }
        }
        else
        {
          const float bonus( is_predicted_wrong && !m_Parity ? m_FalsePositiveBonus : 0.0);
          const float sensitivity_addition
          (
            m_SensitivityWeight * m_SensitivityScalingLow( result_id)
            * ( m_AverageChange( FEATURE_ID, result_id) + delta_prediction) / 2.0
          );
          const float importance( delta_cutoff - bonus - sensitivity_addition);
          m_BackPropPreference( FEATURE_ID, result_id) = importance;
          if( m_RoundNumber > 1 || m_StartWithAll)
          {
            backprop_this_column = importance <= m_ThresholdBPLow( result_id);
          }
        }

        // determine whether the predicted value is on the correct side of the cutoff
        // note that for pure classification tasks, a cutoff of 0.5 is imposed.  This is only the proper behavior though
        // if the objective function is a rank classifier though
        if( is_predicted_wrong)
        {
          // update incorrect tally
          ++( is_high ? incorrect_above_tally : incorrect_below_tally)( result_id);
        }

        // update last round prediction
        last_round_prediction = prediction;

        // handle the case that this column will be backpropagated
        if( backprop_this_column)
        {
          // feature must be backpropagated
          backprop_should_continue = true;

          // update backpropagated tally
          ++( is_high ? bp_above_tally : bp_below_tally)( result_id);

          // modify the backpropagated error for pure classification tasks so that all exemplars have a result of 0 or 1
          if( m_PureClassification)
          {
            ERROR( result_id + m_ResultStartID) = ( is_high ? 1.0 : 0.0) - prediction;
          }
        }
        else
        {
          // this column is not to be backpropagated; so set the error to 0 (since it is possible other columns may be
          // backpropagated, but it has already been decided not to backpropagate this column)
          ERROR( result_id + m_ResultStartID) = 0.0;
        }
      }

      // return true if any part of ERROR remains to be backpropagated
      return backprop_should_continue;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationHybrid::FinalizeRound()
    {
      // only keep the average change over the last 10 rounds ( 10 * 9 / 2 = 45)
      if( m_ChangeAverager.GetWeight() > 10.0)
      {
        m_ChangeAverager.SetWeight( 9.0);
      }

      // update average change
      m_ChangeAverager += m_ChangeLastRound;

      // increment round number
      m_RoundNumber += 1;

      // accumulate results
      for( size_t thread_number( 1); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectTallyAboveCutoff( 0) += m_IncorrectTallyAboveCutoff( thread_number);
        m_IncorrectTallyBelowCutoff( 0) += m_IncorrectTallyBelowCutoff( thread_number);
        m_BackpropagatedTallyAboveCutoff( 0) += m_BackpropagatedTallyAboveCutoff( thread_number);
        m_BackpropagatedTallyBelowCutoff( 0) += m_BackpropagatedTallyBelowCutoff( thread_number);
        for( size_t i( 0); i < m_ResultsSize; ++i)
        {
          m_ThreadMaxDelta( 0)( i) = std::max( m_ThreadMaxDelta( 0)( i), m_ThreadMaxDelta( thread_number)( i));
        }
      }

      const linal::Matrix< float> &avg_change( m_ChangeAverager.GetAverage());
      m_AverageChange = avg_change;
      for( size_t i( 0); i < m_ResultsSize; ++i)
      {
        // store the max deltas
        m_MaxDelta( i) = m_ThreadMaxDelta( 0)( i);

        // compute the previous round's inaccuracy above / below cutoff, to display for the user
        const float inaccuracy_above( float( m_IncorrectTallyAboveCutoff( 0)( i)) / float( m_AboveCutoffCount( i)));
        const float inaccuracy_below( float( m_IncorrectTallyBelowCutoff( 0)( i)) / float( m_BelowCutoffCount( i)));

        // compute the absolute minimum and maximum number of exemplars that should be backpropagated based on the user's
        // given cutoffs
        const float max_above( m_AboveCutoffCount( i));
        const float max_below( m_BelowCutoffCount( i));

        // save the # inaccurate above and below the cutoff last turn
        const float n_inaccurate_above( m_IncorrectTallyAboveCutoff( 0)( i));
        const float n_inaccurate_below( m_IncorrectTallyBelowCutoff( 0)( i));

        // expected_high is the number of features that must be backpropagated next turn that are above the cutoff
        // it is calculated as 1/2 the number that were backpropagated last turn + the optimal # that should be calculated
        // to optimize enrichment (if enrichment cutoff was set) or to ensure balancing
        // Carrying 1/2 the number that were backpropagated last turn forward stabilizes the calculation against, e.g.
        // predicting all the positives correct one turn, then all the negatives correct the following turn
        float expected_high( 0.75 * m_BackpropagatedTallyAboveCutoff( 0)( i));
        float expected_low( 0.75 * m_BackpropagatedTallyBelowCutoff( 0)( i));
        if( m_EnrichmentCutoff)
        {
          // enrichment cutoff set; optimize for enrichment

          // compute the current contingency matrix for the training data using the experimental cutoff
          float false_positives( 0), false_negatives( 0);

          // Enrichment at a constant cutoff (e.g. TP+FP = Constant) = TP/C = 1-FP/C
          // taking the derivative with respect to TP and FP, it should be clear that the change in enrichment is identical
          // in value (but opposite in sign) for exchanging a TP for an FP.  Given that we currently have TP=x, and FP=y,
          // there are FN chances to increase enrichment by backpropagating a positive, and min(FN,FP) chances
          // to increase the value by BPing an FP.
          float chances_to_increase_above( 0), chances_to_increase_below( 0);
          if( m_Parity)
          {
            false_positives = n_inaccurate_below;
            false_negatives = n_inaccurate_above;
            chances_to_increase_above = false_negatives + 1.0;
            chances_to_increase_below = std::min( false_negatives, false_positives) + 1.0;
            if( m_EnrichmentCutoff * m_DatasetSize > max_above && ( false_positives + false_negatives) < m_EnrichmentCutoff * m_DatasetSize)
            {
              chances_to_increase_below = 1;
            }
          }
          else
          {
            false_positives = n_inaccurate_above;
            false_negatives = n_inaccurate_below;
            chances_to_increase_above = std::min( false_negatives, false_positives) + 1.0;
            chances_to_increase_below = false_negatives + 1.0;
            if( m_EnrichmentCutoff * m_DatasetSize > max_below && ( false_positives + false_negatives) < m_EnrichmentCutoff * m_DatasetSize)
            {
              chances_to_increase_above = 1;
            }
          }

          // if there are more incorrect results than the user-specified enrichment cutoff would allow,
          // then do not optimize enrichment,
          // otherwise this leads to an instability where the optimum is reached by predicting everything positive
          // Instead, just backpropagate some factor of the number inaccurate on either side of the cutoff
          if( m_EnrichmentCutoff * m_DatasetSize < ( false_negatives + false_positives))
          {
            chances_to_increase_above = n_inaccurate_above * m_StabilityRatio;
            chances_to_increase_below = n_inaccurate_below * m_StabilityRatio;
          }
          else
          {
            chances_to_increase_above = std::max( chances_to_increase_above, float( 1));
            chances_to_increase_below = std::max( chances_to_increase_below, float( 1));

            // compute the effective stability ratio
            const float effective_stability_ratio
            (
              std::min
              (
                std::min( m_StabilityRatio, max_above / chances_to_increase_above),
                max_below / chances_to_increase_below
              )
            );

            // optimize directly for enrichment
            chances_to_increase_above *= effective_stability_ratio;
            chances_to_increase_below *= effective_stability_ratio;
          }

          // bound by the user-specified limits
          chances_to_increase_above = std::min( max_above, std::max( chances_to_increase_above, float( 1)));
          chances_to_increase_below = std::min( max_below, std::max( chances_to_increase_below, float( 1)));

          // add the 1/4 the optimal value to the expected value to provide stability
          expected_high += 0.25 * chances_to_increase_above;
          expected_low += 0.25 * chances_to_increase_below;
        }
        else
        {
          float desired_high_steady_state( n_inaccurate_above * m_StabilityRatio);
          float desired_low_steady_state( n_inaccurate_below * m_StabilityRatio);

          // bound by the user-specified limits
          desired_high_steady_state = std::min( max_above, std::max( desired_high_steady_state, float( 0)));
          desired_low_steady_state = std::min( max_below, std::max( desired_low_steady_state, float( 0)));

          expected_high += 0.25 * desired_high_steady_state;
          expected_low += 0.25 * desired_low_steady_state;
        }

        // sort the next round's backpropagation preferences above and below the cutoff
        // Compute m_BackPropPreference( j, i) -= x with x such that the number of exemplars with
        // m_BackPropPreference( j, i) >= 0 is the desired number of exemplars for that class (expected above or below)

        // also compute the average distance to cutoff above and below cutoff and the average change above and below cutoff
        // to compute the sensitivity scaling

        math::RunningAverage< float> ave_dist_to_cutoff_high, ave_dist_to_cutoff_low;
        math::RunningAverage< float> ave_change_high, ave_change_low;

        storage::Vector< float> high_values, low_values;
        high_values.AllocateMemory( m_AboveCutoffCount( i));
        low_values.AllocateMemory( m_BelowCutoffCount( i));
        for( size_t j( 0); j < m_DatasetSize; ++j)
        {
          const float result( m_Results( j, i));
          const float dist_to_cutoff( math::Absolute( m_PredictedValue( j, i) - m_EffectiveCutoffs( i)));
          if( result >= m_ScaledCutoffs( i))
          {
            high_values.PushBack( m_BackPropPreference( j, i));
            if( m_BackPropPreference( j, i) <= m_ThresholdBPHigh( i))
            {
              ave_dist_to_cutoff_high += dist_to_cutoff;
              ave_change_high += m_AverageChange( j, i);
            }
          }
          else
          {
            low_values.PushBack( m_BackPropPreference( j, i));
            if( m_BackPropPreference( j, i) <= m_ThresholdBPLow( i))
            {
              ave_dist_to_cutoff_low += dist_to_cutoff;
              ave_change_low += m_AverageChange( j, i);
            }
          }
        }

        m_SensitivityScalingHigh( i) = std::max( float( 0.001), ave_dist_to_cutoff_high.GetAverage()) / std::max( float( 0.001), ave_change_high.GetAverage());
        m_SensitivityScalingLow( i) = std::max( float( 0.001), ave_dist_to_cutoff_low.GetAverage()) / std::max( float( 0.001), ave_change_low.GetAverage());

        // sort the backpropagation preferences above and below this results' cutoff
        high_values.Sort( std::less< float>());
        low_values.Sort( std::less< float>());

        // choose the cutoff for exemplars below cutoff such that there are expected_low number of values >= 0
        const float cutoff_low
        (
          low_values.IsEmpty()
          ? float( 0.5)
          : low_values( std::min( std::max( size_t( expected_low + 0.5), size_t( 1)) - 1, low_values.GetSize() - 1))
        );

        // choose the cutoff for exemplars abpve cutoff such that there are expected_high number of values >= 0
        const float cutoff_high
        (
          high_values.IsEmpty()
          ? float( 0.5)
          : high_values( std::min( std::max( size_t( expected_high + 0.5), size_t( 1)) - 1, high_values.GetSize() - 1))
        );

        m_ThresholdBPHigh( i) = cutoff_high;
        m_ThresholdBPLow( i) = cutoff_low;

        BCL_MessageVrb
        (
          "Last round inaccuracy on result # " + util::Format()( i + m_ResultStartID)
          + " above cutoff = " + util::Format()( inaccuracy_above)
          + " below cutoff = " + util::Format()( inaccuracy_below)
          + " # Incorrect above/below: " + util::Format()( n_inaccurate_above)
          + " " + util::Format()( n_inaccurate_below)
          + " # BP above / below: " + util::Format()( m_BackpropagatedTallyAboveCutoff( 0)( i))
          + " " + util::Format()( m_BackpropagatedTallyBelowCutoff( 0)( i))
          + " Cutoff Low: " + util::Format()( m_ThresholdBPLow( i))
          + " Cutoff high: " + util::Format()( m_ThresholdBPHigh( i))
        );
      }

      // reset thread-specific arrays
      for( size_t thread_number( 0); thread_number < m_NumberThreads; ++thread_number)
      {
        m_IncorrectTallyAboveCutoff( thread_number) = size_t( 0);
        m_IncorrectTallyBelowCutoff( thread_number) = size_t( 0);
        m_BackpropagatedTallyAboveCutoff( thread_number) = size_t( 0);
        m_BackpropagatedTallyBelowCutoff( thread_number) = size_t( 0);
        m_ThreadMaxDelta( thread_number).SetAllElements( 0.0);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationHybrid::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Backpropagates preferrentially according to sensitivity, distance from cutoff, "
        "classification status (correct/incorrect), and cutoff side. "
        "See full explanation and advice about parameters at "
        "https://structbio.vanderbilt.edu:8443/display/MeilerLab/BclANNParameterSelection"
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to consider",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "enrichment max",
        "flag to deliberately optimize for enrichment with the given cutoff, leave 0 to focus on balancing rather than enrichment",
        io::Serialization::GetAgentWithRange( &m_EnrichmentCutoff, float( 0.0), float( 1.0)),
        "0.0"
      );
      parameters.AddInitializer
      (
        "pure classification",
        "flag; set to ignore the result values when backpropagating, instead, backprop 0 or 1 (for above cutoff)",
        io::Serialization::GetAgent( &m_PureClassification),
        "False"
      );
      parameters.AddInitializer
      (
        "stability",
        "Ratio of FP+FN backpropagated due to incorrectness to those backpropped due to rapid change. "
        "Use 1 if the data are very clean and representative of the overall space; larger if not",
        io::Serialization::GetAgent( &m_StabilityRatio),
        "2"
      );
      parameters.AddInitializer
      (
        "sensitivity weight",
        "Weighting of sensitivity. The only other weight term (which is 1-sensitivity weight) accounts for outliers and"
        " false predictions about the cutoff",
        io::Serialization::GetAgent( &m_SensitivityWeight),
        "0.5"
      );
      parameters.AddInitializer
      (
        "fn bonus",
        "Give a bonus to all false negatives BP tendancy",
        io::Serialization::GetAgent( &m_FalseNegativeBonus),
        "0"
      );
      parameters.AddInitializer
      (
        "fp bonus",
        "Give a bonus to all false positives BP tendancy",
        io::Serialization::GetAgent( &m_FalsePositiveBonus),
        "0"
      );
      parameters.AddInitializer
      (
        "start with all",
        "BP all features in the first round",
        io::Serialization::GetAgent( &m_StartWithAll),
        "False"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_leading_sequence.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_operations.h"
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_data_set_select_columns.h"
#include "model/bcl_model_feature_label_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationLeadingSequence::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationLeadingSequence()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationLeadingSequence::NeuralNetworkSelectiveBackpropagationLeadingSequence() :
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_ResultsSize( 1000),
      m_DatasetSize( 1),
      m_BackPropagationRound( false)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationLeadingSequence copied from this instance
    NeuralNetworkSelectiveBackpropagationLeadingSequence *NeuralNetworkSelectiveBackpropagationLeadingSequence::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationLeadingSequence( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationLeadingSequence::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationLeadingSequence::GetAlias() const
    {
      static const std::string s_name( "LeadingSequence");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationLeadingSequence::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization

      // bound m_ResultEndID by the results size
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      // compute internal results size
      m_ResultsSize = m_ResultEndID - m_ResultStartID;

      m_DatasetSize = RESCALED_DATA.GetResultsPtr()->GetNumberFeatures();
      m_MappingIds.Reset();
      m_GroupId = storage::Vector< size_t>( m_DatasetSize);

      DataSetSelectColumns sequence_id_selector;
      // get the feature labels so that sequences can be identified
      const FeatureDataSet< char> &ids( *RESCALED_DATA.GetIdsPtr());

      if( !m_DbIdLabel.GetValue().empty() || m_DbIdLabel.GetNumberArguments())
      {
        const FeatureLabelSet model_feature_labels( *ids.GetFeatureLabelSet());
        sequence_id_selector =
          DataSetSelectColumns
          (
            model_feature_labels.GetSize(),
            model_feature_labels.GetPropertyIndices
            (
              util::ObjectDataLabel( m_DbIdLabel.GetValue(), m_DbIdLabel.GetArguments())
            )
          );
      }
      else
      {
        // no labels were specified, use them all
        sequence_id_selector =
          DataSetSelectColumns( ids.GetFeatureSize(), storage::CreateIndexVector( ids.GetFeatureSize()));
      }

      BCL_Assert( sequence_id_selector.GetOutputFeatureSize(), "Could not find " + m_DbIdLabel.ToString() + " in IDs!");

      // locate all the sequence boundaries and ids
      std::string this_sequence_id( sequence_id_selector.GetOutputFeatureSize(), ' ');
      storage::Map< std::string, size_t> map_uniqueid_index;
      size_t n_groups( 0);
      for( size_t i( 0); i < m_DatasetSize; ++i)
      {
        sequence_id_selector( ids( i), &this_sequence_id[ 0]);
        if( !map_uniqueid_index.Has( this_sequence_id))
        {
          m_GroupId( i) = map_uniqueid_index[ this_sequence_id] = n_groups++;
        }
        else
        {
          m_GroupId( i) = map_uniqueid_index[ this_sequence_id];
        }
      }
      m_MappingIds.Resize( n_groups);
      for( size_t i( 0); i < m_DatasetSize; ++i)
      {
        m_MappingIds( m_GroupId( i)).PushBack( i);
      }

      m_LastRoundHighestIndex = m_CurrentRoundHighestIndex =
        storage::Vector< linal::Vector< size_t> >
        (
          n_groups,
          linal::Vector< size_t>( m_ResultsSize, util::GetUndefined< size_t>())
        );

      m_LastRoundHighestPrediction = m_CurrentRoundHighestPrediction =
        storage::Vector< linal::Vector< float> >
        (
          n_groups,
          linal::Vector< float>( m_ResultsSize, math::GetLowestBoundedValue< float>())
        );
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD*_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationLeadingSequence::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      size_t unique_id( m_GroupId( FEATURE_ID));
      if( m_BackPropagationRound)
      {
        // get a reference to the result and prediction row, already offset by m_ResultStartID
        linal::VectorConstReference< float> prediction_row( m_ResultsSize, PREDICTION.Begin() + m_ResultStartID);

        bool backprop_should_continue( false);
        for( size_t result_id( 0); result_id < m_ResultsSize; ++result_id)
        {
          const size_t &last_round_max_index( m_CurrentRoundHighestIndex( unique_id)( result_id));
          if( PREDICTION( result_id) > m_CurrentRoundHighestPrediction( unique_id)( result_id))
          {
            m_CurrentRoundHighestPrediction( unique_id)( result_id) = PREDICTION( result_id);
          }
          ERROR( result_id) *=
            PREDICTION( result_id)
            /
            std::max
            (
              float( 1.0e-10),
              m_CurrentRoundHighestPrediction( unique_id)( result_id)
            );
          ERROR( result_id) = std::min( 1.0f, std::max( ERROR( result_id), -1.0f));
        }
        return true;
      }

      // get a reference to the result and prediction row, already offset by m_ResultStartID
      linal::VectorConstReference< float> prediction_row( m_ResultsSize, PREDICTION.Begin() + m_ResultStartID);

      // for each result
      for( size_t result_id( 0); result_id < m_ResultsSize; ++result_id)
      {
        // get the actual result, prediction, and cutoff
        const float prediction( prediction_row( result_id));

        float &this_round_prediction( m_CurrentRoundHighestPrediction( unique_id)( result_id));

        size_t &current_round_max_index( m_CurrentRoundHighestIndex( unique_id)( result_id));

        if( prediction > this_round_prediction)
        {
          current_round_max_index = FEATURE_ID;
          this_round_prediction = prediction;
        }
      }
      // return true if any part of ERROR remains to be backpropagated
      return false;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationLeadingSequence::FinalizeRound()
    {
      if( m_BackPropagationRound)
      {
        m_CurrentRoundHighestIndex.SetAllElements
        (
          linal::Vector< size_t>( m_ResultsSize, util::GetUndefined< size_t>())
        );

        m_CurrentRoundHighestPrediction.SetAllElements
        (
          linal::Vector< float>( m_ResultsSize, math::GetLowestBoundedValue< float>())
        );
      }
      m_BackPropagationRound = false;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationLeadingSequence::FinalizeConformation()
    {
      if( m_BackPropagationRound)
      {
        m_CurrentRoundHighestIndex.SetAllElements
        (
          linal::Vector< size_t>( m_ResultsSize, util::GetUndefined< size_t>())
        );

        m_CurrentRoundHighestPrediction.SetAllElements
        (
          linal::Vector< float>( m_ResultsSize, math::GetLowestBoundedValue< float>())
        );
      }
      m_BackPropagationRound = !m_BackPropagationRound;
    }

    //! @brief function to calculate constitution mapping when given the order vector of dataset presentation
    //! @param ORDER vector containing order in which neural network is presented with training dataset
    void NeuralNetworkSelectiveBackpropagationLeadingSequence::ConstitutionMapping
    (
      storage::Vector< size_t> &ORDER
    )
    {
      storage::Vector< size_t> order_vector;
      order_vector.AllocateMemory( ORDER.GetSize());
      for
      (
        storage::Vector< size_t>::const_iterator itr( ORDER.Begin()), itr_end( ORDER.End());
          itr != itr_end;
        ++itr
      )
      {
        size_t cur_index( *itr);
        size_t unique_id( m_GroupId( cur_index));

        if( m_MappingIds( unique_id).FirstElement() == cur_index)
        {
            order_vector.Append( m_MappingIds( unique_id));
        }
      }
      ORDER = order_vector;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationLeadingSequence::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Backpropagates preferrentially according to sensitivity, distance from cutoff, "
        "classification status (correct/incorrect), and cutoff side. "
        "See full explanation and advice about parameters at "
        "https://structbio.vanderbilt.edu:8443/display/MeilerLab/BclANNParameterSelection"
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to consider",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "label",
        "label to use",
        io::Serialization::GetAgent( &m_DbIdLabel),
        ""
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_selective_backpropagation_tolerance.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_const_reference.h"
#include "math/bcl_math_running_average_sd.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkSelectiveBackpropagationTolerance::s_Instance
    (
      util::Enumerated< NeuralNetworkSelectiveBackpropagationInterface>::AddInstance
      (
        new NeuralNetworkSelectiveBackpropagationTolerance()
      )
    );

    //! @brief default constructor
    NeuralNetworkSelectiveBackpropagationTolerance::NeuralNetworkSelectiveBackpropagationTolerance() :
      m_ResultStartID( 0),
      m_ResultEndID( 1000),
      m_Tolerance( 0.0),
      m_Noise( 0.0),
      m_ReduceErrorByTolerance( false),
      m_ResultsSize( 1000),
      m_NumberThreads( 0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkSelectiveBackpropagationTolerance copied from this instance
    NeuralNetworkSelectiveBackpropagationTolerance *NeuralNetworkSelectiveBackpropagationTolerance::Clone() const
    {
      return new NeuralNetworkSelectiveBackpropagationTolerance( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkSelectiveBackpropagationTolerance::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkSelectiveBackpropagationTolerance::GetAlias() const
    {
      static const std::string s_name( "Tolerant");
      return s_name;
    }

    //! @brief Initialize this object with the rescaled dataset and other important parameters
    //! @param RESCALED_DATA the already-rescaled training dataset
    //! @param OBJECTIVE the actual objective function
    //! @param THREADS # of threads that may be accessing this object at once
    void NeuralNetworkSelectiveBackpropagationTolerance::Initialize
    (
      const descriptor::Dataset &RESCALED_DATA,
      const ObjectiveFunctionInterface &OBJECTIVE,
      const size_t &NUMBER_THREADS
    )
    {
      // initialization
      if( m_ResultEndID > RESCALED_DATA.GetResultSize())
      {
        m_ResultEndID = RESCALED_DATA.GetResultSize();
      }
      m_ResultsSize = m_ResultEndID - m_ResultStartID;
      m_NumberThreads = NUMBER_THREADS;

      // set up sizes of all internally-held members
      m_RescaledNoise = linal::Vector< float>( m_ResultsSize, m_Noise);

      // regression task: Compute standard deviation over the selected columns
      m_RescaledTolerance = linal::Vector< float>( m_ResultsSize, m_Tolerance);
      const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESCALED_DATA.GetResultsPtr()->GetScaling());
      const float rescale_to_range_width( results_scaling->GetRange().GetWidth());
      for( size_t result( 0); result < m_ResultsSize; ++result)
      {
        const float rescaled_range_ratio( results_scaling->GetRescaleRanges()( result).GetWidth() / rescale_to_range_width);
        m_RescaledTolerance( result) /= rescaled_range_ratio;
        m_RescaledNoise( result) /= rescaled_range_ratio;
      }
    } // Initialize

    //! @brief select whether or not to backpropagate the current feature, and edit the error vector, if desired
    //! @param PREDICTION the prediction that was made by the neural network
    //! @param ERROR Reference to the error vector, (should already have been set to RESULT - PREDICTION)
    //! @param FEATURE_ID id of this feature in the dataset
    //! @param THREAD_ID id of this thread
    //! @return true if the feature should be backpropagated
    bool NeuralNetworkSelectiveBackpropagationTolerance::ShouldBackpropagate
    (
      const linal::VectorConstInterface< float> &PREDICTION,
      linal::VectorInterface< float> &ERROR,
      const size_t &FEATURE_ID,
      const size_t &THREAD_ID
    )
    {
      bool backprop_should_continue( false);
      // regression target
      for( size_t result_offset( 0); result_offset < m_ResultsSize; ++result_offset)
      {
        // get the actual index
        float &er( ERROR( result_offset));
        const float tolerance( m_RescaledTolerance( result_offset));
        if( math::Absolute( er) <= tolerance)
        {
          er = 0.0;
        }
        else
        {
          backprop_should_continue = true;
          if( m_ReduceErrorByTolerance)
          {
            backprop_should_continue = true;
            if( er > 0.0)
            {
              er -= tolerance;
            }
            else
            {
              er += tolerance;
            }
          }
          // add noise, if desirable
          if( m_RescaledNoise( result_offset))
          {
            er += random::GetGlobalRandom().RandomGaussian( 0.0, m_RescaledNoise( result_offset));
          }
        }
      }

      // return true if any part of ERROR remains to be backpropagated
      return backprop_should_continue;
    }

    //! @brief finalize the current round; occurs only after all threads were already joined
    void NeuralNetworkSelectiveBackpropagationTolerance::FinalizeRound()
    {
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkSelectiveBackpropagationTolerance::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Always back-propagating features that are mispredicted beyond the tolerance. "
        "Can also add noise to the results"
      );
      parameters.AddInitializer
      (
        "begin",
        "result columns to use this triager for",
        io::Serialization::GetAgent( &m_ResultStartID),
        "0"
      );
      parameters.AddInitializer
      (
        "end",
        "1+max result column to consider",
        io::Serialization::GetAgent( &m_ResultEndID),
        "1000"
      );
      parameters.AddInitializer
      (
        "noise",
        "Std of noise to add to errors"
        "This is related to the uncertainty in the experimental values; though even with perfect experimental values, it "
        "should be non-zero for training general models",
        io::Serialization::GetAgent( &m_Noise),
        "0.0"
      );
      parameters.AddInitializer
      (
        "tolerance",
        "If < than this amount of error above the cutoff, do not backpropagate under any circumstances",
        io::Serialization::GetAgent( &m_Tolerance),
        "0"
      );
      parameters.AddInitializer
      (
        "reduce error by tolerance",
        "Whether to reduce the error for backpropagated features by the tolerance when the feature is backpropagated",
        io::Serialization::GetAgent( &m_ReduceErrorByTolerance),
        "True"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_update_weights_bounded_simple_propagation.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkUpdateWeightsBoundedSimplePropagation::s_Instance
    (
      util::Enumerated< NeuralNetworkUpdateWeightsInterface>::AddInstance
      (
        new NeuralNetworkUpdateWeightsBoundedSimplePropagation()
      )
    );

    //! @brief default constructor
    NeuralNetworkUpdateWeightsBoundedSimplePropagation::NeuralNetworkUpdateWeightsBoundedSimplePropagation() :
      m_Alpha( 0.5),
      m_Eta( 10.0),
      m_Min( math::GetLowestBoundedValue< float>()),
      m_Max( math::GetHighestBoundedValue< float>())
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkUpdateWeightsBoundedSimplePropagation copied from this instance
    NeuralNetworkUpdateWeightsBoundedSimplePropagation *NeuralNetworkUpdateWeightsBoundedSimplePropagation::Clone() const
    {
      return new NeuralNetworkUpdateWeightsBoundedSimplePropagation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkUpdateWeightsBoundedSimplePropagation::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkUpdateWeightsBoundedSimplePropagation::GetAlias() const
    {
      static const std::string s_Name( "BoundedSimple");
      return s_Name;
    }

    //! @brief initialize the update weights; should initialize any internal data structures to the right size
    //! @param SIZE the number of weights/changes/slopes/prevslopes to update
    void NeuralNetworkUpdateWeightsBoundedSimplePropagation::Initialize( const size_t &SIZE)
    {
      m_ChangeSlopes = linal::Vector< float>( SIZE, float( 0.0));
    }

    //! @brief Set the changes array; this is intended solely for testing purposes
    //! @param CHANGES the new changes array
    void NeuralNetworkUpdateWeightsBoundedSimplePropagation::SetChanges( const linal::VectorConstInterface< float> &CHANGES)
    {
      m_ChangeSlopes = CHANGES;
    }

    //! @brief Get the changes array; this is intended solely for testing purposes
    //! @return the changes array
    const linal::Vector< float> &NeuralNetworkUpdateWeightsBoundedSimplePropagation::GetChanges() const
    {
      return m_ChangeSlopes;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief this is a function used internally to update the weights of a neural network
    //! @param WEIGHTS a vector iterator (const pointer) to the first weight to be updated
    //! @param SLOPES  a vector iterator (const pointer) to the first slope to be updated
    void NeuralNetworkUpdateWeightsBoundedSimplePropagation::operator ()
    (
      float *const &WEIGHTS,
      float *const &SLOPES
    )
    {
      const size_t number_connection_weights( m_ChangeSlopes.GetSize());
      float *const changes( m_ChangeSlopes.Begin());
      // simple propagation algorithm
      for( size_t i( 0); i < number_connection_weights; ++i)
      {
        if( util::IsNaN( SLOPES[ i]))
        {
          continue;
        }

        changes[ i] = m_Eta * SLOPES[ i] + m_Alpha * changes[ i];
        WEIGHTS[ i] += changes[ i];
        SLOPES[ i] = 0;
        if( WEIGHTS[ i] > m_Max)
        {
          WEIGHTS[ i] = m_Max;
        }
        else if( WEIGHTS[ i] < m_Min)
        {
          WEIGHTS[ i] = m_Min;
        }
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkUpdateWeightsBoundedSimplePropagation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses simple backpropagation (see http://en.wikipedia.org/wiki/Backpropagation)"
      );

      parameters.AddInitializer
      (
        "eta",
        "learning rate",
        io::Serialization::GetAgentWithRange( &m_Eta, 0.0, 1.0),
        "0.1"
      );
      parameters.AddInitializer
      (
        "alpha",
        "momentum; how long a change in weights persists",
        io::Serialization::GetAgentWithRange( &m_Alpha, 0.0, 1.0),
        "0.5"
      );
      parameters.AddInitializer
      (
        "min",
        "minimum value for the weight; if it goes below this, it will be reset to this",
        io::Serialization::GetAgent( &m_Min),
        util::Format()( math::GetLowestBoundedValue< float>())
      );
      parameters.AddInitializer
      (
        "max",
        "maximum value for the weight; if it goes above this, it will be reset to this",
        io::Serialization::GetAgent( &m_Max),
        util::Format()( math::GetHighestBoundedValue< float>())
      );
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &NeuralNetworkUpdateWeightsBoundedSimplePropagation::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_Alpha, ISTREAM);
      io::Serialize::Read( m_Eta, ISTREAM);
      io::Serialize::Read( m_ChangeSlopes, ISTREAM);
      io::Serialize::Read( m_Min, ISTREAM);
      io::Serialize::Read( m_Max, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &NeuralNetworkUpdateWeightsBoundedSimplePropagation::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_Alpha, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Eta, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ChangeSlopes, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Min, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Max, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_update_weights_resilient_propagation.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkUpdateWeightsResilientPropagation::s_Instance
    (
      util::Enumerated< NeuralNetworkUpdateWeightsInterface>::AddInstance
      (
        new NeuralNetworkUpdateWeightsResilientPropagation()
      )
    );

    //! @brief default constructor
    NeuralNetworkUpdateWeightsResilientPropagation::NeuralNetworkUpdateWeightsResilientPropagation() :
      m_PreviousSlopes(),
      m_MaxWeightChange( 50.0),
      m_MinWeightChange( 0.001)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkUpdateWeightsResilientPropagation copied from this instance
    NeuralNetworkUpdateWeightsResilientPropagation *NeuralNetworkUpdateWeightsResilientPropagation::Clone() const
    {
      return new NeuralNetworkUpdateWeightsResilientPropagation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkUpdateWeightsResilientPropagation::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkUpdateWeightsResilientPropagation::GetAlias() const
    {
      static const std::string s_Name( "Resilient");
      return s_Name;
    }

    //! @brief initialize the update weights; should initialize any internal data structures to the right size
    //! @param SIZE the number of weights/changes/slopes/prevslopes to update
    void NeuralNetworkUpdateWeightsResilientPropagation::Initialize( const size_t &SIZE)
    {
      // set the previous slopes to 0.0125
      m_PreviousSlopes = linal::Vector< float>( SIZE, 0.0125);
      m_ChangeSlopes = linal::Vector< float>( SIZE, 0.0);
    } // Initialize

    //! @brief Set the changes array; this is intended solely for testing purposes
    //! @param CHANGES the new changes array
    void NeuralNetworkUpdateWeightsResilientPropagation::SetChanges( const linal::VectorConstInterface< float> &CHANGES)
    {
      m_ChangeSlopes = CHANGES;
    }

    //! @brief Get the changes array; this is intended solely for testing purposes
    //! @return the changes array
    const linal::Vector< float> &NeuralNetworkUpdateWeightsResilientPropagation::GetChanges() const
    {
      return m_ChangeSlopes;
    }

    //! @brief this is a function used internally to update the weights of a neural network
    //! @param WEIGHTS a vector iterator (const pointer) to the first weight to be updated
    //! @param SLOPES  a vector iterator (const pointer) to the first slope to be updated
    void NeuralNetworkUpdateWeightsResilientPropagation::operator ()( float *const &WEIGHTS, float *const &SLOPES)
    {
      const float increase_factor( 1.2);
      const float decrease_factor( 0.5);

      float *const changes( m_ChangeSlopes.Begin());
      float *const previous_slopes( m_PreviousSlopes.Begin());

      // computes new change
      for( size_t index( 0), size( m_PreviousSlopes.GetSize()); index < size; ++index)
      {
        float &slope( SLOPES[ index]);
        if( util::IsNaN( slope))
        {
          continue;
        }

        float &weight( WEIGHTS[ index]);
        float &change( changes[ index]);
        float &previous_slope( previous_slopes[ index]);
        const float same_sign( previous_slope * slope);

        // this slope and previous slope have the same sign, increase the change accordingly
        if( same_sign > std::numeric_limits< float>::min())
        {
          change = std::min( std::max( change, m_MinWeightChange) * increase_factor, m_MaxWeightChange);
          weight += slope < float( 0.0) ? -change : change;
        }
        // this slope and previous slope have different signs
        else if( same_sign < -std::numeric_limits< float>::min())
        {
          // change may not be zero because then the training will stop
          change = std::max( change, m_MinWeightChange) * decrease_factor;
          slope = 0.0;
        }
        // either the current slope or the previous slope was zero
        // usually its the previous slope that is zero, which happens if, in the last iteration, slope and
        // previous slope had opposite signs
        else
        {
          weight += slope < float( 0.0) ? -change : change;
        }

        // the previous slope for the next round
        previous_slope = slope;
      }
    } // ResilientUpdateWeights

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkUpdateWeightsResilientPropagation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses the resilient propagation (rprop) algorithm (see http://en.wikipedia.org/wiki/Rprop)"
      );
      parameters.AddInitializer
      (
        "max change",
        "max weight change allowed per iteration",
        io::Serialization::GetAgent( &m_MaxWeightChange),
        "50.0"
      );
      parameters.AddInitializer
      (
        "min change",
        "min weight change allowed per iteration",
        io::Serialization::GetAgent( &m_MinWeightChange),
        "0.001"
      );
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &NeuralNetworkUpdateWeightsResilientPropagation::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_PreviousSlopes, ISTREAM);
      io::Serialize::Read( m_ChangeSlopes, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &NeuralNetworkUpdateWeightsResilientPropagation::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_PreviousSlopes, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ChangeSlopes, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_neural_network_update_weights_simple_propagation.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> NeuralNetworkUpdateWeightsSimplePropagation::s_Instance
    (
      util::Enumerated< NeuralNetworkUpdateWeightsInterface>::AddInstance
      (
        new NeuralNetworkUpdateWeightsSimplePropagation()
      )
    );

    //! @brief default constructor
    NeuralNetworkUpdateWeightsSimplePropagation::NeuralNetworkUpdateWeightsSimplePropagation() :
      m_Alpha( 0.5),
      m_Eta( 10.0)
    {
    }

    //! @brief copy constructor
    //! @return a new NeuralNetworkUpdateWeightsSimplePropagation copied from this instance
    NeuralNetworkUpdateWeightsSimplePropagation *NeuralNetworkUpdateWeightsSimplePropagation::Clone() const
    {
      return new NeuralNetworkUpdateWeightsSimplePropagation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &NeuralNetworkUpdateWeightsSimplePropagation::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &NeuralNetworkUpdateWeightsSimplePropagation::GetAlias() const
    {
      static const std::string s_Name( "Simple");
      return s_Name;
    }

    //! @brief initialize the update weights; should initialize any internal data structures to the right size
    //! @param SIZE the number of weights/changes/slopes/prevslopes to update
    void NeuralNetworkUpdateWeightsSimplePropagation::Initialize( const size_t &SIZE)
    {
      m_ChangeSlopes = linal::Vector< float>( SIZE, float( 0.0));
    }

    //! @brief Set the changes array; this is intended solely for testing purposes
    //! @param CHANGES the new changes array
    void NeuralNetworkUpdateWeightsSimplePropagation::SetChanges( const linal::VectorConstInterface< float> &CHANGES)
    {
      m_ChangeSlopes = CHANGES;
    }

    //! @brief Get the changes array; this is intended solely for testing purposes
    //! @return the changes array
    const linal::Vector< float> &NeuralNetworkUpdateWeightsSimplePropagation::GetChanges() const
    {
      return m_ChangeSlopes;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief this is a function used internally to update the weights of a neural network
    //! @param WEIGHTS a vector iterator (const pointer) to the first weight to be updated
    //! @param SLOPES  a vector iterator (const pointer) to the first slope to be updated
    void NeuralNetworkUpdateWeightsSimplePropagation::operator ()
    (
      float *const &WEIGHTS,
      float *const &SLOPES
    )
    {
      const size_t number_connection_weights( m_ChangeSlopes.GetSize());
      float *const changes( m_ChangeSlopes.Begin());
      // simple propagation algorithm
      for( size_t i( 0); i < number_connection_weights; ++i)
      {
        if( util::IsNaN( SLOPES[ i]))
        {
          continue;
        }

        changes[ i] = m_Eta * SLOPES[ i] + m_Alpha * changes[ i];
        WEIGHTS[ i] += changes[ i];
        SLOPES[ i] = 0;
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer NeuralNetworkUpdateWeightsSimplePropagation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses simple backpropagation (see http://en.wikipedia.org/wiki/Backpropagation)"
      );

      parameters.AddInitializer
      (
        "eta",
        "learning rate",
        io::Serialization::GetAgentWithRange( &m_Eta, 0.0, 1.0),
        "0.1"
      );
      parameters.AddInitializer
      (
        "alpha",
        "momentum; how long a change in weights persists",
        io::Serialization::GetAgentWithRange( &m_Alpha, 0.0, 1.0),
        "0.5"
      );
      return parameters;
    }

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &NeuralNetworkUpdateWeightsSimplePropagation::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_Alpha, ISTREAM);
      io::Serialize::Read( m_Eta, ISTREAM);
      io::Serialize::Read( m_ChangeSlopes, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &NeuralNetworkUpdateWeightsSimplePropagation::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_Alpha, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Eta, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_ChangeSlopes, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_accuracy.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionAccuracy::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionAccuracy()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionAccuracy::ObjectiveFunctionAccuracy() :
      m_ActivityCutoff( 0.0)
    {
    }

    //! @brief constructor
    ObjectiveFunctionAccuracy::ObjectiveFunctionAccuracy( const float &ACTIVITY_CUTOFF) :
      m_ActivityCutoff( ACTIVITY_CUTOFF)
    {
    }

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionAccuracy *ObjectiveFunctionAccuracy::Clone() const
    {
      return new ObjectiveFunctionAccuracy( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionAccuracy::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionAccuracy::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      // count the number of predictions that were on the same side of the cutoff as the experimental predictions
      size_t accurate_prediction_count( 0);

      // iterate over all experimental and predicted values and check for accuracy
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // iterate over each result
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          // check if experimental was above or below the cutoff
          const bool experimental( EXPERIMENTAL( counter)( result_number) < m_ActivityCutoff);
          // check if predicted was above or below the cutoff
          const bool prediction( PREDICTED( counter)( result_number) < m_ActivityCutoff);

          // if experimental and prediction belong to same group, the prediction was correct
          if( experimental == prediction)
          {
            ++accurate_prediction_count;
          }
        }
      }

      // return accuracy
      return float( accurate_prediction_count) / float( data_set_size) / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionAccuracy::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the fraction of predictions that were correct using the given cutoff"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoff),
        "0"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_accuracy_with_excluded_range.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionAccuracyWithExcludedRange::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionAccuracyWithExcludedRange()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionAccuracyWithExcludedRange::ObjectiveFunctionAccuracyWithExcludedRange() :
      m_ActivityCutoff( 0.0),
      m_ActivityCutoffLow( 0.0),
      m_ActivityCutoffHigh( 0.0)
    {
    }

    //! @brief constructor
    ObjectiveFunctionAccuracyWithExcludedRange::ObjectiveFunctionAccuracyWithExcludedRange( const float &ACTIVITY_CUTOFF) :
      m_ActivityCutoff( ACTIVITY_CUTOFF),
      m_ActivityCutoffLow( ACTIVITY_CUTOFF),
      m_ActivityCutoffHigh( ACTIVITY_CUTOFF)
    {
    }

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionAccuracyWithExcludedRange *ObjectiveFunctionAccuracyWithExcludedRange::Clone() const
    {
      return new ObjectiveFunctionAccuracyWithExcludedRange( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionAccuracyWithExcludedRange::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionAccuracyWithExcludedRange::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      linal::Matrix< char> prediction_classification( data_set_size, result_size, '\0');

      // iterate over all experimental and predicted values and check for accuracy
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // iterate over each result
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          const float res( EXPERIMENTAL( counter)( result_number));
          if( res < m_ActivityCutoffLow || res > m_ActivityCutoffHigh)
          {
            // check if experimental was above or below the cutoff
            const bool experimental( res < m_ActivityCutoff);
            // check if predicted was above or below the cutoff
            const bool prediction( PREDICTED( counter)( result_number) < m_ActivityCutoff);

            // if experimental and prediction belong to same group, the prediction was correct
            prediction_classification( counter, result_number) = char( experimental == prediction ? 'P' : 'n');
          }
        }
      }

      // return accuracy
      return prediction_classification;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionAccuracyWithExcludedRange::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      // count the number of predictions that were on the same side of the cutoff as the experimental predictions
      size_t accurate_prediction_count( 0);

      // count the number of predictions that were outside the specified range
      size_t count_outside_range( 0);

      // iterate over all experimental and predicted values and check for accuracy
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // iterate over each result
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          const float res( EXPERIMENTAL( counter)( result_number));
          if( res < m_ActivityCutoffLow || res > m_ActivityCutoffHigh)
          {
            // check if experimental was above or below the cutoff
            const bool experimental( EXPERIMENTAL( counter)( result_number) < m_ActivityCutoff);
            // check if predicted was above or below the cutoff
            const bool prediction( PREDICTED( counter)( result_number) < m_ActivityCutoff);

            // if experimental and prediction belong to same group, the prediction was correct
            if( experimental == prediction)
            {
              ++accurate_prediction_count;
            }
            ++count_outside_range;
          }
        }
      }

      // return accuracy
      return float( accurate_prediction_count) / float( count_outside_range) / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionAccuracyWithExcludedRange::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the fraction of predictions that were correct using the given cutoff"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "cutoff low",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoffLow),
        "0"
      );
      parameters.AddInitializer
      (
        "cutoff high",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoffHigh),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_auc_roc_curve.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionAucRocCurve::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionAucRocCurve()
      )
    );

    //! copy constructor
    ObjectiveFunctionAucRocCurve *ObjectiveFunctionAucRocCurve::Clone() const
    {
      return new ObjectiveFunctionAucRocCurve( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ObjectiveFunctionAucRocCurve::GetAlias() const
    {
      static const std::string s_Name( "AucRocCurve");
      return s_Name;
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionAucRocCurve::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );
      BCL_Assert
      (
        m_FPRCutoffRange.GetMin() < m_FPRCutoffRange.GetMax(),
        "Min fpr must be < max fpr for integration range"
      );

      // sum the integrals for each result
      float sum_integral( 0);

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>
            (
              PREDICTED( counter)( result_number),
              EXPERIMENTAL( counter)( result_number)
            )
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);
        // weighted auc under roc curve
        double local_integral
        (
          roc_curve.Integral
          (
            m_AucWeightingFunction,
            &math::ContingencyMatrix::GetFalsePositiveRate,
            &math::ContingencyMatrix::GetTruePositiveRate,
            m_FPRCutoffRange,
            m_XAxisLogScale
          )
        );
        sum_integral += local_integral;
        BCL_MessageStd
        (
          "weighted auc for result # " + util::Format()( result_number) + ": " + util::Format()( local_integral) + "; "
          + util::Format()( local_integral / m_ExpectedResult) + " x the naive predictor result"
        );
      }

      // compute the average
      sum_integral /= float( result_size);

      BCL_MessageStd
      (
        "average weighted auc: " + util::Format()( sum_integral) + "; "
        + util::Format()( sum_integral / m_ExpectedResult) + " x the naive predictor result"
      );

      // return average integral of roc curve
      return sum_integral;
    }

    //! @brief get the desired hit rate (e.g. fraction of positive predictions)
    //! @return the desired hit rate, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; 0.01 means, for example, that only the top 1%
    //!       of values will be considered
    float ObjectiveFunctionAucRocCurve::GetDesiredHitRate() const
    {
      BCL_Assert( m_NumberFeatures || !m_XAxisLogScale, "SetData needs to be called before GetDesiredHitRate");

      if( !m_XAxisLogScale)
      {
        // check for default parameters for Min/Max; in which case the default behavior of dynamically selecting
        // based on actual features is likely best
        if( m_FPRCutoffRange.GetMin() <= float( 0.0) && m_FPRCutoffRange.GetMax() >= float( 1.0))
        {
          return util::GetUndefined< float>();
        }
        // return the midpoint of the range
        return m_FPRCutoffRange.GetMiddle();
      }

      // determine adjusted min point
      const float log_fpr_min( std::log( std::max( m_FPRCutoffRange.GetMin(), 1.0 / double( m_NumberFeatures))));
      const float log_fpr_max( std::log( std::max( m_FPRCutoffRange.GetMax(), 1.0 / double( m_NumberFeatures))));

      // return log-mid point
      return std::exp( 0.5 * ( log_fpr_min + log_fpr_max));
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionAucRocCurve::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      m_NumberFeatures = DATA.GetNumberFeatures();

      // list of pairs to compute the expected value
      storage::List< storage::Pair< double, bool> > values_fake;
      values_fake.PushBack( storage::Pair< double, bool>( -1.0, false));
      values_fake.PushBack( storage::Pair< double, bool>( 1.0, true));
      // compute expected value
      math::ROCCurve roc_curve( values_fake);
      m_ExpectedResult =
        roc_curve.Integral
        (
          m_AucWeightingFunction,
          &math::ContingencyMatrix::GetFalsePositiveRate,
          &math::ContingencyMatrix::GetTruePositiveRate,
          m_FPRCutoffRange,
          m_XAxisLogScale
        );
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionAucRocCurve::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the area under the receiver-operator curve"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "x_axis_log",
        "calculate AUC while x axis is on log scale",
        io::Serialization::GetAgent( &m_XAxisLogScale),
        "0"
      );
      parameters.AddInitializer
      (
        "polynomial",
        "coefficient of the polynomial, in increasing degree, e.g. polynomial(1, 2) = 1+2x",
        io::Serialization::GetAgent( &m_AucWeightingFunction),
        "(1)"
      );
      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      parameters.AddInitializer
      (
        "min fpr",
        "minimum fpr to begin integration at. "
        "Values smaller than 1/(P+N) will automatically be rounded up to 1/(P+N) if log scaling is set",
        io::Serialization::GetAgentWithRange( &m_FPRCutoffRange.GetMin(), 0.0, 1.0),
        "0"
      );
      parameters.AddInitializer
      (
        "max fpr",
        "maximum fpr to end integration at. ",
        io::Serialization::GetAgentWithRange( &m_FPRCutoffRange.GetMax(), 0.0, 1.0),
        "1"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_binary_operation.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionBinaryOperation::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionBinaryOperation()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionBinaryOperation *ObjectiveFunctionBinaryOperation::Clone() const
    {
      return new ObjectiveFunctionBinaryOperation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionBinaryOperation::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      m_ObjectiveLeft->SetData( DATA, IDS);
      m_ObjectiveRight->SetData( DATA, IDS);
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionBinaryOperation::GetImprovementType() const
    {
      // ensure that all the members are defined
      BCL_Assert( m_ObjectiveLeft.IsDefined(), "LHS was undefined");
      BCL_Assert( m_ObjectiveRight.IsDefined(), "RHS was undefined");
      BCL_Assert( m_Op.IsDefined(), "Operation was undefined");

      // get the improvement type on each side
      const opti::ImprovementType lhs_type( m_ObjectiveLeft->GetImprovementType());
      const opti::ImprovementType rhs_type( m_ObjectiveRight->GetImprovementType());

      if( m_Op.GetAlias() == "+")
      {
        BCL_Assert
        (
          lhs_type == rhs_type,
          "Improvement type is ill defined: adding objectives with opposite signs of improvement"
        );
        return lhs_type;
      }
      else if( m_Op.GetAlias() == "-")
      {
        BCL_Assert
        (
          lhs_type != rhs_type,
          "Improvement type is ill defined: subtracting objectives with same signs of improvement"
        );
        return lhs_type;
      }
      else if( m_Op.GetAlias() == "*")
      {
        BCL_Assert
        (
          lhs_type == rhs_type,
          "Improvement type is ill defined: multiplying objectives with opposite signs of improvement"
        );
        return lhs_type;
      }
      else if( m_Op.GetAlias() == "/")
      {
        BCL_Assert
        (
          lhs_type != rhs_type,
          "Improvement type is ill defined: dividing objectives with same signs of improvement"
        );
        return lhs_type;
      }
      BCL_Assert
      (
        m_ObjectiveLeft->GetRankingParity() == m_ObjectiveRight->GetRankingParity(),
        "Objectives with opposite ranking parities cannot be combined!"
      );
      BCL_Exit( "Undefined improvement type for op: " + m_Op.GetAlias(), -1);
      return lhs_type;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionBinaryOperation::GetGoalType() const
    {
      BCL_Assert( m_ObjectiveLeft.IsDefined(), "LHS was undefined");
      BCL_Assert( m_ObjectiveRight.IsDefined(), "RHS was undefined");
      if( m_ObjectiveLeft->GetGoalType() == m_ObjectiveRight->GetGoalType())
      {
        return m_ObjectiveLeft->GetGoalType();
      }
      return e_Other;
    }

    //! @brief get the threshold, for classification type objectives
    //! @return the threshold, for classification type objectives
    float ObjectiveFunctionBinaryOperation::GetThreshold() const
    {
      if( m_ObjectiveLeft->GetThreshold() == m_ObjectiveRight->GetThreshold())
      {
        return m_ObjectiveLeft->GetThreshold();
      }
      return util::GetUndefined< float>();
    }

    //! @brief set the threshold
    //! @param THRESHOLD threshold that divides output classes
    void ObjectiveFunctionBinaryOperation::SetThreshold( const float &THRESHOLD)
    {
      m_ObjectiveLeft->SetThreshold( THRESHOLD);
      m_ObjectiveRight->SetThreshold( THRESHOLD);
    }

    //! @brief get the parity, for rank classification type objectives
    //! @return the parity, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; true means the objective is most interested
    //!       in prediction of values higher than the threshold, false means below threshold
    bool ObjectiveFunctionBinaryOperation::GetRankingParity() const
    {
      BCL_Assert
      (
        m_ObjectiveLeft->GetRankingParity() == m_ObjectiveRight->GetRankingParity(),
        "Objectives with opposite ranking parities cannot be combined!"
      );
      return m_ObjectiveLeft->GetRankingParity();
    }

    //! @brief get the desired hit rate (e.g. fraction of positive predictions)
    //! @return the desired hit rate, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; 0.01 means, for example, that only the top 1%
    //!       of values will be considered
    float ObjectiveFunctionBinaryOperation::GetDesiredHitRate() const
    {
      const float desired_left( m_ObjectiveLeft->GetDesiredHitRate());
      const float desired_right( m_ObjectiveRight->GetDesiredHitRate());
      if( !util::IsDefined( desired_left))
      {
        return desired_right;
      }
      else if( !util::IsDefined( desired_right))
      {
        return desired_left;
      }
      // both sides defined, take the average
      return 0.5 * ( desired_right + desired_left);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionBinaryOperation::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      linal::Matrix< char> prediction_classification
      (
        m_ObjectiveLeft->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED)
      );
      const linal::Matrix< char> prediction_classification_rhs
      (
        m_ObjectiveRight->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED)
      );

      // combine the matrices using the max operation, this prefers false predictions over true, and any prediction over none
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // iterate over each result
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          prediction_classification( counter, result_number) =
            std::max
            (
              prediction_classification( counter, result_number),
              prediction_classification_rhs( counter, result_number)
            );
        }
      }

      // return the classification
      return prediction_classification;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionBinaryOperation::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // compute the left hand side
      float value_left( m_ObjectiveLeft->operator ()( EXPERIMENTAL, PREDICTED));
      const float value_right( m_ObjectiveRight->operator ()( EXPERIMENTAL, PREDICTED));

      // carry out the operation
      m_Op->operator ()( value_left, value_right);

      // return the resulting value, now stored in value_left
      return value_left;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionBinaryOperation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Performs an arithmetic operation on the outputs of two objective functions"
      );

      parameters.AddInitializer
      (
        "lhs",
        "left hand side of the operation",
        io::Serialization::GetAgent( &m_ObjectiveLeft)
      );

      parameters.AddInitializer
      (
        "rhs",
        "right hand side of the operation",
        io::Serialization::GetAgent( &m_ObjectiveRight)
      );

      parameters.AddInitializer
      (
        "op",
        "operation to carry out on the lhs and rhs",
        io::Serialization::GetAgent( &m_Op)
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_bootstrap.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_data_set_select_columns.h"
#include "model/bcl_model_feature_label_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionBootstrap::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionBootstrap()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    ObjectiveFunctionBootstrap *ObjectiveFunctionBootstrap::Clone() const
    {
      return new ObjectiveFunctionBootstrap( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief default constructor
    ObjectiveFunctionBootstrap::ObjectiveFunctionBootstrap() :
      m_NumberBootstraps( 2000),
      m_ConfidenceInterval( 0.95)
    {
    }

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ObjectiveFunctionBootstrap::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionBootstrap::GetImprovementType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetImprovementType();
      }
      return opti::e_LargerIsBetter;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionBootstrap::GetGoalType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetGoalType();
      }
      return e_Other;
    }

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionBootstrap::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      return m_Objective->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionBootstrap::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      m_RealIds = IDS;
      m_Objective->SetData( DATA, m_RealIds);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionBootstrap::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      const size_t dataset_size( EXPERIMENTAL.GetNumberFeatures());
      FeatureDataSet< float> experimental_sample( EXPERIMENTAL);
      FeatureDataSet< float> predicted_sample( PREDICTED);
      FeatureDataSet< char> ids_sample( dataset_size, m_RealIds.GetFeatureSize(), char( 0));

      math::RunningAverageSD< float> ave_val;
      storage::Vector< float> objective_output;
      objective_output.AllocateMemory( m_NumberBootstraps);
      util::Implementation< ObjectiveFunctionInterface> obj_copy( m_Objective);
      for( size_t run_number( 0); run_number < m_NumberBootstraps; ++run_number)
      {
        for( size_t row( 0); row < dataset_size; ++row)
        {
          const size_t selected_row( random::GetGlobalRandom().Random( dataset_size - 1));
          experimental_sample.GetRawMatrix().ReplaceRow( row, EXPERIMENTAL.GetMatrix().GetRow( selected_row));
          predicted_sample.GetRawMatrix().ReplaceRow( row, PREDICTED.GetMatrix().GetRow( selected_row));
          ids_sample.GetRawMatrix().ReplaceRow( row, m_RealIds.GetMatrix().GetRow( selected_row));
        }
        obj_copy->SetData( experimental_sample, ids_sample);
        const float result( m_Objective->operator ()( experimental_sample, predicted_sample));
        ave_val += result;
        objective_output.PushBack( result);
        util::GetLogger().LogStatus
        (
          util::Format()( run_number) + " / " + util::Format()( m_NumberBootstraps) + " bootstraps finished "
          + util::Format().FFP( 2)( 100.0 * float( run_number + 1) / float( m_NumberBootstraps)) + "% complete"
        );
      }
      const size_t conf_low( ( 1.0 - m_ConfidenceInterval) * m_NumberBootstraps);
      const size_t conf_hi( m_NumberBootstraps - conf_low);
      objective_output.Sort( std::less< float>());

      BCL_MessageCrt
      (
        "ObjFunction: " + m_Objective->GetString()
        + " Ave: " + util::Format()( ave_val.GetAverage())
        + " SD: " + util::Format()( ave_val.GetStandardDeviation())
        + " " + util::Format().FFP( 3)( 100.0 * m_ConfidenceInterval)
        + "% confidence interval: "
        + util::Format()( objective_output( conf_low)) + " - " + util::Format()( objective_output( conf_hi))
      );

      return ave_val.GetAverage();
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionBootstrap::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Computes bootstrap (sampling with replacement) computation for the objective function, reports average value "
        "confidence intervals"
      );

      parameters.AddInitializer
      (
        "repeats",
        "number of times to repeat the bootstrap to obtain a confidence interval",
        io::Serialization::GetAgent( &m_NumberBootstraps),
        "2000"
      );

      parameters.AddInitializer
      (
        "function",
        "core objective function to use",
        io::Serialization::GetAgent( &m_Objective)
      );

      parameters.AddInitializer
      (
        "confidence interval",
        "confidence interval to report upper and lower bounds for the objective function (95%)",
        io::Serialization::GetAgentWithRange( &m_ConfidenceInterval, 0.5, 1.0),
        "0.95"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_categorical_max.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_reference.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionCategoricalMax::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionCategoricalMax()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionCategoricalMax *ObjectiveFunctionCategoricalMax::Clone() const
    {
      return new ObjectiveFunctionCategoricalMax( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionCategoricalMax::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionCategoricalMax::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      size_t results_size( DATA.GetFeatureSize());
      size_t dataset_size( DATA.GetNumberFeatures());
      linal::MatrixConstReference< float> results_ref( DATA.GetMatrix());
      linal::Vector< size_t> classification_store( results_size, size_t( 0));
      size_t number_classifications( 0);

      util::SiPtr< const RescaleFeatureDataSet> rescale( DATA.GetScaling());
      {
        size_t classification_id( 0);
        const float *itr_result( results_ref[ 0]);
        if( rescale.IsDefined())
        {
          for( size_t result_id( 0); result_id < results_size; ++result_id, ++itr_result)
          {
            if( rescale->DescaleValue( result_id, *itr_result) > 0.5)
            {
              classification_store( classification_id) = result_id;
              ++classification_id;
            }
          }
        }
        else
        {
          for( size_t result_id( 0); result_id < results_size; ++result_id, ++itr_result)
          {
            if( *itr_result > 0.5)
            {
              classification_store( classification_id) = result_id;
              ++classification_id;
            }
          }
        }
        m_ResultsBestIndex = linal::Matrix< size_t>( dataset_size, classification_id);
        classification_store = classification_store.CreateSubVector( classification_id);
        number_classifications = classification_id;
        m_ResultsBestIndex.ReplaceRow( 0, classification_store);
        m_ClassBoundaries = classification_store;
      }
      for( size_t feature( 0); feature < dataset_size; ++feature)
      {
        size_t classification_id( 0);
        const float *itr_result( results_ref[ feature]);
        for( size_t result_id( 0); result_id < results_size; ++result_id, ++itr_result)
        {
          if
          (
            ( rescale.IsDefined() && rescale->DescaleValue( result_id, *itr_result) > 0.5)
            || ( !rescale.IsDefined() && *itr_result > 0.5)
          )
          {
            BCL_Assert
            (
              classification_id < number_classifications,
              "MultiClassification requires a constant # of classifications per feature row, 1st row had "
              + util::Format()( number_classifications) + " but row " + util::Format()( feature) + " had "
              + util::Format()( classification_id + 1) + " or more "
            );
            classification_store( classification_id) = result_id;
            m_ClassBoundaries( classification_id)
              = std::min( m_ClassBoundaries( classification_id), classification_store( classification_id));
            ++classification_id;
          }
        }
        BCL_Assert
        (
          classification_id == number_classifications,
          "MultiClassification requires a constant # of classifications per feature row, 1st row had "
          + util::Format()( number_classifications) + " but row " + util::Format()( feature) + " had "
          + util::Format()( classification_id)
        );
        m_ResultsBestIndex.ReplaceRow( feature, classification_store);
      }
      {
        storage::Vector< size_t> class_bounds( m_ClassBoundaries.Begin() + 1, m_ClassBoundaries.End());
        class_bounds.PushBack( results_size);
        m_ClassBoundaries = linal::Vector< size_t>( class_bounds.Begin(), class_bounds.End());
      }
      m_PredictedClasses = m_ResultsBestIndex;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief classify. Obtain a matrix of '0'/'1' values (as char, for memory) as to whether the objective function is
    //!        satisfied with each result.  Denote results that the objective function ignores by \0
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with '0'/'1'/\0 values indicating Bad/Good/Not considered prediction
    linal::Matrix< char> ObjectiveFunctionCategoricalMax::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        result_size == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );
      BCL_Assert
      (
        m_ClassBoundaries.GetSize() && result_size == m_ClassBoundaries.Last(),
        "SetData was not called (based on class boundaries: " + util::Format()( m_ClassBoundaries)
        + "; max should be: " + util::Format()( result_size)
      );

      linal::Matrix< char> prediction_classification( data_set_size, result_size, '\0');

      if( &EXPERIMENTAL != &PREDICTED)
      {
        BCL_Assert
        (
          data_set_size == m_ResultsBestIndex.GetNumberRows(),
          "Set data was not called (using different experimental and predicted"
        );
        // iterate over all experimental and predicted values and check for agreement on which index is largest
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          // get the vector containing the original maxes
          linal::VectorConstReference< size_t> results_best_index( m_ResultsBestIndex.GetRow( counter));

          size_t index_start( 0);
          const size_t *itr_best( results_best_index.Begin()), *itr_best_end( results_best_index.End());
          const size_t *itr_category( m_ClassBoundaries.Begin());
          const float *prediction( PREDICTED[ counter]);
          char *pred_classification_row( prediction_classification[ counter]);

          // track overall position on this row
          size_t col( 0);

          for( ; itr_best != itr_best_end; ++itr_best, ++itr_category)
          {
            // get the value from what should be the max column
            const size_t desired_max( *itr_best);

            // get the value out of the prediction
            const float desired_prediction_value( prediction[ desired_max]);

            // initially, set the classification of the max to P (TP); it will be set to n (FN) if it is found not
            // to be the max
            pred_classification_row[ desired_max] = 'P';

            // get the actual maximum
            for
            (
              const float *itr_prediction( prediction + index_start), *itr_prediction_end( prediction + *itr_category);
              itr_prediction != itr_prediction_end;
              ++itr_prediction, ++col
            )
            {
              if( *itr_prediction < desired_prediction_value)
              {
                pred_classification_row[ col] = 'N';
              }
              else if( *itr_prediction > desired_prediction_value || col != desired_max)
              {
                pred_classification_row[ col] = 'p';
                pred_classification_row[ desired_max] = 'n';
              }
            }

            index_start = *itr_category;
          }
        }
      }
      else
      {
        // just go by the real maximum
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          // get the vector containing the original maxes
          linal::VectorConstReference< size_t> results_best_index( m_ResultsBestIndex.GetRow( counter));

          size_t index_start( 0);
          const size_t *itr_category( m_ClassBoundaries.Begin()), *itr_category_end( m_ClassBoundaries.End());
          const float *prediction( PREDICTED[ counter]);
          char *pred_classification_row( prediction_classification[ counter]);

          // track overall position on this row
          size_t col( 0);

          for( ; itr_category != itr_category_end; ++itr_category)
          {
            // get the value from what should be the max column
            const size_t desired_max
            (
              std::max_element( prediction + index_start, prediction + *itr_category) - prediction
            );

            // get the value out of the prediction
            const float desired_prediction_value( prediction[ desired_max]);

            // initially, set the classification of the max to P (TP); it will be set to n (FN) if it is found not
            // to be the max
            pred_classification_row[ desired_max] = 'P';

            // get the actual maximum
            for
            (
              const float *itr_prediction( prediction + index_start), *itr_prediction_end( prediction + *itr_category);
              itr_prediction != itr_prediction_end;
              ++itr_prediction, ++col
            )
            {
              if( *itr_prediction < desired_prediction_value)
              {
                pred_classification_row[ col] = 'N';
              }
              else if( *itr_prediction > desired_prediction_value || col != desired_max)
              {
                pred_classification_row[ col] = 'p';
                pred_classification_row[ desired_max] = 'n';
              }
            }

            index_start = *itr_category;
          }
        }
      }
      return prediction_classification;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    float ObjectiveFunctionCategoricalMax::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );
      BCL_Assert
      (
        data_set_size == m_ResultsBestIndex.GetNumberRows(),
        "Set data was not called"
      );

      // count the number of predictions that were on the same side of the cutoff as the experimental predictions
      linal::Vector< size_t> accurate_prediction_count( m_ClassBoundaries.GetSize(), size_t( 0));
      storage::Vector< linal::Matrix< size_t> > contingency_matrices( m_ClassBoundaries.GetSize());
      size_t last_class_start( 0), class_set_number( 0);
      for
      (
        auto itr_pre( m_ClassBoundaries.Begin()), itr_pre_end( m_ClassBoundaries.End());
        itr_pre != itr_pre_end;
        ++itr_pre, ++class_set_number
      )
      {
        const size_t class_size( *itr_pre - last_class_start);
        contingency_matrices( class_set_number) = linal::Matrix< size_t>( class_size, class_size, size_t( 0));
        last_class_start = *itr_pre;
      }

      // iterate over all experimental and predicted values and check for agreement on which index is largest
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // get the vector containing the original maxes
        linal::VectorConstReference< size_t> results_best_index( m_ResultsBestIndex.GetRow( counter));
        linal::VectorReference< size_t> predicted_best_index( m_PredictedClasses.GetRow( counter));

        size_t index_start( 0);
        const size_t *itr_best( results_best_index.Begin()), *itr_best_end( results_best_index.End());
        size_t *itr_predicted( predicted_best_index.Begin());
        const size_t *itr_category( m_ClassBoundaries.Begin());
        storage::Vector< linal::Matrix< size_t> >::iterator itr_contingency_matrices( contingency_matrices.Begin());
        size_t *itr_accurate( accurate_prediction_count.Begin());
        const float *prediction( PREDICTED[ counter]);

        for( ; itr_best != itr_best_end; ++itr_best, ++itr_category, ++itr_accurate, ++itr_contingency_matrices, ++itr_predicted)
        {
          // get the value from what should be the max column
          const size_t desired_max( *itr_best);

          // get the actual maximum
          const size_t actual_max
          (
            std::max_element( prediction + index_start, prediction + *itr_category) - prediction
          );
          *itr_predicted = actual_max;

          // compare the predicted max with the desired max for this class to determine if it was accurate
          if( actual_max == desired_max)
          {
            ++*itr_accurate;
          }

          // update the contingency matrix
          ++( *itr_contingency_matrices)( desired_max - index_start, actual_max - index_start);

          // update start index for the next category
          index_start = *itr_category;
        }
      }

      if( util::GetMessenger().GetCurrentMessageLevel() >= util::Message::e_Verbose)
      {
        std::ostringstream oss;
        oss << "Accuracies By Class:\t";
        for
        (
          size_t class_boundary( 0), n_class_boundaries( m_ClassBoundaries.GetSize());
          class_boundary < n_class_boundaries;
          ++class_boundary
        )
        {
          oss << class_boundary << ":\t"
              << float( accurate_prediction_count( class_boundary)) / float( data_set_size) << '\t';
        }
        util::GetLogger() << oss.str();
        oss.str( std::string());
        oss << "Contingency matrices by Class:\n";
        for
        (
          size_t class_boundary( 0), n_class_boundaries( m_ClassBoundaries.GetSize());
          class_boundary < n_class_boundaries;
          ++class_boundary
        )
        {
          oss << "\nClass " << class_boundary << " Counts\n";

          // get the current matrix
          const linal::Matrix< size_t> &contingency_matrix( contingency_matrices( class_boundary));
          const size_t subclasses( contingency_matrix.GetNumberRows());

          // write the header
          oss << "vPredicted >Experimental\t";
          for( size_t sub_class_a( 0); sub_class_a < subclasses; ++sub_class_a)
          {
            oss << sub_class_a << '\t';
          }
          oss << '\n';
          for( size_t sub_class_a( 0); sub_class_a < subclasses; ++sub_class_a)
          {
            oss << sub_class_a << '\t';
            for( size_t sub_class_b( 0); sub_class_b < subclasses; ++sub_class_b)
            {
              oss << contingency_matrix( sub_class_a, sub_class_b) << '\t';
            }
            oss << '\n';
          }
        }
        util::GetLogger() << oss.str();
      }

      // return accuracy
      return double( accurate_prediction_count.Sum()) / double( data_set_size * m_ClassBoundaries.GetSize());
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionCategoricalMax::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the fraction of predictions that were correct for results which are series of one or more blocks of "
        "classifications, with the threshold for a true classification set at 0.5.  For example, "
        "(0 0.1 0.4 1) represents an output with a single classification (as only one value is above 0.5, while "
        "(1 0 0 0 0 1 0 0) represents an output with multiple classifications.  The number of classification categories "
        "must remain constant in a given dataset; e.g. (0 1 0 1) cannot belong to the same dataset as (0 0 0 1) for this "
        "objective function.  Detection of classification boundaries is performed automatically, and each classification "
        "block is given the same weight, so for X classification blocks, the output will range from 0 to X.  Message level "
        "can be set to verbose to enable output of contingency matrices for all classes."
      );
      parameters.AddDataMember
      (
        "borders",
        io::Serialization::GetAgent( &m_ClassBoundaries)
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_objective_function_constant.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionConstant::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance( new ObjectiveFunctionConstant())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionConstant::ObjectiveFunctionConstant() :
      m_ConstantValue( float( 0)),
      m_ImprovementType( opti::e_SmallerEqualIsBetter)
    {
    }

    //! @brief default constructor
    ObjectiveFunctionConstant::ObjectiveFunctionConstant
    (
      const float CONST_VALUE,
      opti::ImprovementType IMPROVE,
      GoalEnum GOAL,
      const float &CUTOFF
    ) :
      m_ConstantValue( CONST_VALUE),
      m_ImprovementType( IMPROVE),
      m_GoalType( GOAL),
      m_Threshold( CUTOFF)
    {
    }

    //! copy constructor
    ObjectiveFunctionConstant *ObjectiveFunctionConstant::Clone() const
    {
      return new ObjectiveFunctionConstant( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ObjectiveFunctionConstant::GetAlias() const
    {
      static const std::string s_Name( "Constant");
      return s_Name;
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionConstant::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // no data available, return constant result
      return m_ConstantValue;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionConstant::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Returns a constant value as objective function result; used to weight objective functions or provide an offset"
      );
      parameters.AddInitializer
      (
        "value",
        "constant value to return",
        io::Serialization::GetAgent( &m_ConstantValue),
        "3.40282e+38"
      );
      parameters.AddInitializer
      (
        "direction",
        "Determines whether an increasing or decreasing value indicates improvement",
        io::Serialization::GetAgent( &m_ImprovementType),
        "SmallerIsBetter"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_contingency_matrix_measure.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix_measures.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionContingencyMatrixMeasure::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionContingencyMatrixMeasure()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionContingencyMatrixMeasure::ObjectiveFunctionContingencyMatrixMeasure() :
      m_ActivityCutoff( 0.0),
      m_PositivesAboveThreshold( false),
      m_OptimizePredictedCutoff( false)
    {
    }

    //! @brief constructor taking all necessary parameters
    //! @param ACTIVITY_CUTOFF the activity cutoff
    //! @param MEASURE the measure to compute
    ObjectiveFunctionContingencyMatrixMeasure::ObjectiveFunctionContingencyMatrixMeasure
    (
      const float &ACTIVITY_CUTOFF,
      const math::ContingencyMatrixMeasures &MEASURE,
      const bool &POSITIVES_ABOVE_THRESHOLD,
      const bool &OPTIMIZE_CUTOFF
    ) :
      m_ActivityCutoff( ACTIVITY_CUTOFF),
      m_Measure( MEASURE),
      m_PositivesAboveThreshold( POSITIVES_ABOVE_THRESHOLD),
      m_OptimizePredictedCutoff( OPTIMIZE_CUTOFF)
    {
    }

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionContingencyMatrixMeasure *ObjectiveFunctionContingencyMatrixMeasure::Clone() const
    {
      return new ObjectiveFunctionContingencyMatrixMeasure( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionContingencyMatrixMeasure::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionContingencyMatrixMeasure::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      // compute the average of the measure over all outputs
      math::RunningAverage< float> average_measure;

      // generate an output message to inform the user of progress
      std::ostringstream output_info;

      // iterate over each result
      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        if( !m_OptimizePredictedCutoff)
        {
          // count the number of predictions of each type in TP/FP/TN/FN
          size_t tp( 0), tn( 0), fp( 0), fn( 0);
          // iterate over all experimental and predicted values and check for accuracy
          for( size_t counter( 0); counter < data_set_size; ++counter)
          {
            if( util::IsDefined( EXPERIMENTAL( counter)( result_number)))
            {
              // check if experimental was above or below the cutoff
              const bool experimental( EXPERIMENTAL( counter)( result_number) >= m_ActivityCutoff);
              // check if predicted was above or below the cutoff
              const bool prediction( PREDICTED( counter)( result_number) >= m_ActivityCutoff);

              // if experimental and prediction belong to same group, the prediction was correct
              if( experimental == prediction)
              {
                ++( experimental == m_PositivesAboveThreshold ? tp : tn);
              }
              else
              {
                ++( prediction == m_PositivesAboveThreshold ? fp : fn);
              }
            }
          }
          const math::ContingencyMatrix matrix( tp, fp, fn, tn);
          if( matrix.GetTotal())
          {
            const float measure( ( *m_Measure)( matrix));
            average_measure += measure;
            output_info << "\tResult " << result_number << " TP/TN/FP/FN: "
                        << tp << ' ' << tn << ' ' << fp << ' ' << fn << ' '
                        << m_Measure.GetAlias() << ": " << measure;
          }
        }
        else
        {
          // find the optimal value for the measure
          storage::List< storage::Pair< double, double> > results_list;
          // iterate over all experimental and predicted values and check for accuracy
          for( size_t counter( 0); counter < data_set_size; ++counter)
          {
            if( util::IsDefined( EXPERIMENTAL( counter)( result_number)))
            {
              results_list.PushBack
              (
                storage::Pair< double, double>
                (
                  PREDICTED( counter)( result_number),
                  EXPERIMENTAL( counter)( result_number)
                )
              );
            }
          }
          // create a roc curve
          math::ROCCurve curve( results_list, m_ActivityCutoff, m_PositivesAboveThreshold);

          if( !curve.GetSortedCounts().IsEmpty())
          {
            // find the optimal value for the given contingency matrix measure
            // first, get the last element of the curve, which is needed for computing the
            const math::ROCCurve::Point &roc_end_point( curve.GetSortedCounts().LastElement());

            std::pair< storage::Vector< math::ROCCurve::Point>::const_iterator, double>
              best_cutoff_and_metric_value( curve.GetMaxima( *m_Measure));
            const math::ContingencyMatrix best_contingency_matrix
            (
              best_cutoff_and_metric_value.first->GetContingencyMatrix( roc_end_point)
            );
            const float best_metric_value( best_cutoff_and_metric_value.second);
            const float best_cutoff( best_cutoff_and_metric_value.first->GetCutoff());

            // output the best contingency matrix info
            average_measure += best_metric_value;
            output_info << "\tResult " << result_number << " TP/TN/FP/FN: "
                        << best_contingency_matrix.GetNumberTruePositives() << ' '
                        << best_contingency_matrix.GetNumberTrueNegatives() << ' '
                        << best_contingency_matrix.GetNumberFalsePositives() << ' '
                        << best_contingency_matrix.GetNumberFalseNegatives() << ' '
                        << m_Measure.GetAlias() << ": " << best_metric_value
                        << " @cutoff " << best_cutoff;
          }
        }
      }

      BCL_MessageStd( output_info.str());

      // return accuracy
      return average_measure.GetAverage();
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionContingencyMatrixMeasure::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates any contingency matrix measure at a specific cutoff. "
        "Multiple output columns will result in the average of the measure being reported."
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_ActivityCutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "measure",
        "contingency matrix measure to consider for each result",
        io::Serialization::GetAgent( &m_Measure)
      );
      parameters.AddInitializer
      (
        "parity",
        "true if positives are above the cutoff",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold)
      );
      parameters.AddInitializer
      (
        "adjustable cutoff",
        "Useful when the predicted cutoff is allowed to be different than the experimental cutoff. This is only "
        "appropriate for measures that consider all aspects of the contingency matrix; e.g. FPR is always 0 at the "
        " start of the ROC curve. In this case, the predicted cutoff will be optimized",
        io::Serialization::GetAgent( &m_OptimizePredictedCutoff),
        "False"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_cutoff_from_percentile.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_data_set_select_columns.h"
#include "model/bcl_model_feature_label_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionCutoffFromPercentile::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionCutoffFromPercentile()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    ObjectiveFunctionCutoffFromPercentile *ObjectiveFunctionCutoffFromPercentile::Clone() const
    {
      return new ObjectiveFunctionCutoffFromPercentile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief default constructor
    ObjectiveFunctionCutoffFromPercentile::ObjectiveFunctionCutoffFromPercentile() :
      m_Percentile( 0.5)
    {
    }

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ObjectiveFunctionCutoffFromPercentile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionCutoffFromPercentile::GetImprovementType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetImprovementType();
      }
      return opti::e_LargerIsBetter;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionCutoffFromPercentile::GetGoalType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetGoalType();
      }
      return e_Other;
    }

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionCutoffFromPercentile::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      return m_Objective->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionCutoffFromPercentile::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      storage::Vector< float> data_copy( DATA.GetMatrix().Begin(), DATA.GetMatrix().End());
      if( m_Objective->GetRankingParity())
      {
        data_copy.Sort( std::greater< float>());
      }
      else
      {
        data_copy.Sort( std::less< float>());
      }
      float threshold
      (
        data_copy( std::min( size_t( m_Percentile * data_copy.GetSize()), data_copy.GetSize() - size_t( 1)))
      );
      auto scaling_ptr( DATA.GetScaling());
      if( scaling_ptr.IsDefined())
      {
        threshold = scaling_ptr->DescaleValue( 0, threshold);
      }
      m_Objective->SetThreshold( threshold);
      m_Objective->SetData( DATA, IDS);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionCutoffFromPercentile::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      return m_Objective->operator()( EXPERIMENTAL, PREDICTED);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionCutoffFromPercentile::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Sets the threshold/cutoff/division between classes of a user-specified objective function based on a percentile "
        "of the data"
      );

      parameters.AddInitializer
      (
        "percentile",
        "percentile of the data to consider in the positive class for setting the cutoff",
        io::Serialization::GetAgentWithRange( &m_Percentile, 0.0, 1.0),
        "0.5"
      );
      parameters.AddInitializer
      (
        "",
        "Objective function to set the threshold of",
        io::Serialization::GetAgent( &m_Objective)
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_enrichment.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionEnrichment::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionEnrichment()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionEnrichment::ObjectiveFunctionEnrichment() :
      m_Cutoff( 0.0),
      m_OverSamplingFactor( 1.0),
      m_PositivesAboveThreshold( true)
    {
    }

    //! @brief constructor from monitor data
    ObjectiveFunctionEnrichment::ObjectiveFunctionEnrichment
    (
      const float CUTOFF,
      const float OVERSAMPLING_FACTOR,
      const bool POSITIVES_ABOVE_THRESHOLD
    ) :
      m_Cutoff( CUTOFF),
      m_OverSamplingFactor( OVERSAMPLING_FACTOR),
      m_PositivesAboveThreshold( POSITIVES_ABOVE_THRESHOLD)
    {
    }

    //! copy constructor
    ObjectiveFunctionEnrichment *ObjectiveFunctionEnrichment::Clone() const
    {
      return new ObjectiveFunctionEnrichment( *this);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionEnrichment::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      // sum the enrichments for each result
      float sum_enrichment( 0);

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>
            (
              PREDICTED( counter)( result_number),
              EXPERIMENTAL( counter)( result_number)
            )
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);

        // write out contingency matrix for top fraction of predicted in roc curve
        BCL_MessageDbg
        (
          "FPR cutoff: "
          + util::Format()( m_FalsePositiveRateCutoff)
          + util::Format()( " oversampling factor: ")
          + util::Format()( m_OverSamplingFactor)
          + " ConMatx: "
          + util::Format()( roc_curve.ContingencyMatrixFraction( m_FalsePositiveRateCutoff))
        );

        // sum enrichment of top fraction according to oversampling factor
        sum_enrichment += roc_curve.ContingencyMatrixFraction( m_FalsePositiveRateCutoff).GetOversampledEnrichment( m_OverSamplingFactor);
      }

      // return the average enrichment
      return sum_enrichment / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionEnrichment::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the enrichment = Precision / Positive Sampling Rate; Precision = #True-Positives / (#True-Positives + #False-Positives), Positive Sampling Rate = #Positives / Sample-Size"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "oversampling factor",
        "factor how many times a group of compounds is repeated",
        io::Serialization::GetAgent( &m_OverSamplingFactor),
        "1"
      );
      parameters.AddInitializer
      (
        "FPR cutoff",
        "false positive rate (FPR) cutoff for which enrichment should be calculated",
        io::Serialization::GetAgent( &m_FalsePositiveRateCutoff),
        "0.03"
      );
      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_enrichment_average.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionEnrichmentAverage::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionEnrichmentAverage()
      )
    );

    //! copy constructor
    ObjectiveFunctionEnrichmentAverage *ObjectiveFunctionEnrichmentAverage::Clone() const
    {
      return new ObjectiveFunctionEnrichmentAverage( *this);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionEnrichmentAverage::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes, experimental size: "
        + util::Format()( EXPERIMENTAL.GetFeatureSize()) + ", predicted: " + util::Format()( PREDICTED.GetFeatureSize())
      );

      // sum the enrichments for each result
      float sum_enrichment_averages( 0);

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>( PREDICTED( counter)( result_number), EXPERIMENTAL( counter)( result_number))
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);

        // average of enrichment values
        math::RunningAverage< float> enrichment_average;

        // calculate enrichment by sampling of different cutoffs
        for( float cutoff( m_EnrichmentCutOffStepSize); cutoff <= m_EnrichmentCutOffMax; cutoff += m_EnrichmentCutOffStepSize)
        {
          const math::ContingencyMatrix matrix( roc_curve.ContingencyMatrixFraction( cutoff));
          const float enrichment( matrix.GetEnrichment());

          // if the enrichment cutoff step size is too small, then there may be 0 predicted actives/inactives, thus making
          // the enrichment undefined.  Therefore, only add defined enrichments
          if( util::IsDefined( enrichment))
          {
            BCL_MessageDbg
            (
              "#cutoff: " + util::Format()( cutoff)
              + " enrichment: " + util::Format()( enrichment) +
              " FP/TP/FN/TN: "
              + util::Format()( matrix.GetNumberFalsePositives()) + "/"
              + util::Format()( matrix.GetNumberTruePositives()) + "/"
              + util::Format()( matrix.GetNumberFalseNegatives()) + "/"
              + util::Format()( matrix.GetNumberTrueNegatives()) + "/"
            );

            // sum enrichments
            enrichment_average += enrichment;
          }
        }

        // add the averaged enrichment as objective function score
        sum_enrichment_averages += enrichment_average.GetAverage();
      }

      // return averaged enrichment as objective function score
      return sum_enrichment_averages / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionEnrichmentAverage::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the average enrichment over a range of cutoffs"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "enrichment max",
        "maximum cutoff to use for calculating enrichment",
        io::Serialization::GetAgentWithRange( &m_EnrichmentCutOffMax, 0.0, 1.0),
        "1.0"
      );
      parameters.AddInitializer
      (
        "step size",
        "step size for enrichment cutoff, smaller step sizes improve accuracy at minor cost in speed",
        io::Serialization::GetAgentWithRange( &m_EnrichmentCutOffStepSize, 0.0, 1.0),
        "0.1"
      );
      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_information_gain_ratio.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionInformationGainRatio::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionInformationGainRatio()
      )
    );

    //! copy constructor
    ObjectiveFunctionInformationGainRatio *ObjectiveFunctionInformationGainRatio::Clone() const
    {
      return new ObjectiveFunctionInformationGainRatio( *this);
    }

    //! @brief evaluate for a given dataset with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionInformationGainRatio::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes, experimental size: "
        + util::Format()( EXPERIMENTAL.GetFeatureSize()) + ", predicted: " + util::Format()( PREDICTED.GetFeatureSize())
      );

      // sum the enrichments for each result
      math::RunningAverage< float> avg_best_contingency_matrix_measure;

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>( PREDICTED( counter)( result_number), EXPERIMENTAL( counter)( result_number))
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);

        storage::Vector< math::ROCCurve::Point>::const_iterator itr_sorted_counts( roc_curve.GetSortedCounts().Begin());
        storage::Vector< math::ROCCurve::Point>::const_iterator itr_sorted_counts_end( roc_curve.GetSortedCounts().End());

        // maximum information gain ratio seen so far
        float max_information_gain_ratio( 0.0);

        // keep track of best performing contingency matrix
        math::ContingencyMatrix best_contingency_matrix;

        // iterate over all contingency matrix cutoffs
        for( ; itr_sorted_counts != itr_sorted_counts_end; ++itr_sorted_counts)
        {
          if( itr_sorted_counts->GetNumberPredictedPositives() <= roc_curve.GetNumberActualPositives())
          {
            continue;
          }

          const math::ContingencyMatrix contingency_matrix
          (
            itr_sorted_counts->GetNumberTruePositives(),
            itr_sorted_counts->GetNumberFalsePositives(),
            roc_curve.GetNumberActualPositives() - itr_sorted_counts->GetNumberTruePositives(),
            roc_curve.GetNumberActualNegatives() - itr_sorted_counts->GetNumberFalsePositives()
          );

          const float information_gain_ratio( contingency_matrix.GetInformationGainRatio());

          // check if the current max information gain ratio is reached
          if( max_information_gain_ratio < information_gain_ratio)
          {
            // save the best information gain ratio seen so far
            max_information_gain_ratio = information_gain_ratio;
            // save the best performing contingency matrix
            best_contingency_matrix = contingency_matrix;
          }
        }

        BCL_MessageStd
        (
          "best PPV: " + util::Format()( m_ContingencyMatrixMeasure( best_contingency_matrix))
          + " @infogainratio " + util::Format()( max_information_gain_ratio)
        );

        // add the averaged information gain ratio as objective function score
        avg_best_contingency_matrix_measure += m_ContingencyMatrixMeasure( best_contingency_matrix);
      }

      // return averaged information gain ratio as objective function score
      return avg_best_contingency_matrix_measure.GetAverage();
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionInformationGainRatio::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the Information Gain Ratio as an objective function value"
      );
      parameters.AddInitializer
      (
        "measure",
        "contingency matrix measure that will be returned",
        io::Serialization::GetAgent( &m_ContingencyMatrixMeasure),
        "InformationGainRatio"
      );
      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates correct from incorrect results",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );
      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_integral_precision_fraction_predicted.h"

// includes from bcl - sorted alphabetically
#include "command/bcl_command_parameter_check_allowed.h"
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionIntegralPrecisionFractionPredicted::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionIntegralPrecisionFractionPredicted()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionIntegralPrecisionFractionPredicted::ObjectiveFunctionIntegralPrecisionFractionPredicted() :
      m_Cutoff(),
      m_FPPCutoffRange(),
      m_PositivesAboveThreshold(),
      m_DatasetSize( 0)
    {
    }

    //! @brief constructor from monitor data
    //! @param CUTOFF cutoff to determining the area under the curve
    //! @param FRACTION_PRED_POS_CUTOFF cuoff for fraction predicted positives
    //! @param POSITIVES_ABOVE_THRESHOLD flag for sorting list of predicted and experimental values in roc curve
    ObjectiveFunctionIntegralPrecisionFractionPredicted::ObjectiveFunctionIntegralPrecisionFractionPredicted
    (
      const float CUTOFF,
      const math::Range< double> &FRACTION_PRED_POS_CUTOFF,
      const bool POSITIVES_ABOVE_THRESHOLD
    ) :
      m_Cutoff( CUTOFF),
      m_FPPCutoffRange( FRACTION_PRED_POS_CUTOFF),
      m_PositivesAboveThreshold( POSITIVES_ABOVE_THRESHOLD),
      m_DatasetSize( 0)
    {
    }

    //! @brief Clone function
    //! @return pointer to new ObjectiveFunctionIntegralPrecisionFractionPredicted
    ObjectiveFunctionIntegralPrecisionFractionPredicted *
    ObjectiveFunctionIntegralPrecisionFractionPredicted::Clone() const
    {
      return new ObjectiveFunctionIntegralPrecisionFractionPredicted( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief get the desired hit rate (e.g. fraction of positive predictions)
    //! @return the desired hit rate, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; 0.01 means, for example, that only the top 1%
    //!       of values will be considered
    float ObjectiveFunctionIntegralPrecisionFractionPredicted::GetDesiredHitRate() const
    {
      // compute average exponent:
      const double average_exponent
      (
        m_FPPCutoffRange.GetMin() == double( 0)
        ? std::log10( m_FPPCutoffRange.GetMax()) + std::log10( double( m_DatasetSize - 1))
        : std::log10( m_FPPCutoffRange.GetMax()) - std::log10( m_FPPCutoffRange.GetMin())
      );
      // compute average value
      return pow( double( 10), average_exponent);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionIntegralPrecisionFractionPredicted::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      m_DatasetSize = DATA.GetNumberFeatures();

      if( m_FPPCutoffType == "compound")
      {
        m_FPPCutoffRange = math::Range< double>
        (
          m_FPPCompoundCutoffRange.GetMin() / m_DatasetSize,
          m_FPPCompoundCutoffRange.GetMax() / m_DatasetSize
        );

        BCL_MessageDbg
        (
          "Converting compound range into x axis pct range. New range: "
          + util::Format()( m_FPPCutoffRange)
        );
      }
    }

    //! @brief calculate integral of ideal PPV vs FPP curve. Necessary for normalization purposes of a calculated curve.
    //! @param ROC_CURVE calculated roc curve of interest
    //! @param X_AXIS_RANGE integration range on x axis (FPP)
    //! @return integral of ideal PPV vs FPP curve
    float ObjectiveFunctionIntegralPrecisionFractionPredicted::CalculateIdealIntegral
    (
      const math::ROCCurve ROC_CURVE,
      const math::Range< double> X_AXIS_RANGE
    ) const
    {
      const size_t total_results( ROC_CURVE.GetNumberResults());
      const double num_actual_positives( ROC_CURVE.GetNumberActualPositives());
      const double step_size( 1.0 / double( total_results - 1));
      double prev_progress( 0);

      bool initialize( true);

      // sum the precisions and precision idel for each result
      float sum_precision_ideal( 0);

      // iterate over sorted counts of roc curve and plot ideal curve for PPV vs FractionPositivePredicted
      for
      (
        storage::Vector< math::ROCCurve::Point>::const_iterator
          itr( ROC_CURVE.GetSortedCounts().Begin()), itr_end( ROC_CURVE.GetSortedCounts().End());
        itr != itr_end;
        ++itr
      )
      {
        const size_t counts( itr->GetNumberPredictedPositives());
        const double progress( step_size * counts);

        if( X_AXIS_RANGE.GetMin() < progress && X_AXIS_RANGE.GetMax() >= progress)
        {
          if( initialize)
          {
            prev_progress = progress;
            initialize = false;
            continue;
          }
          sum_precision_ideal += std::log10( progress / prev_progress) * std::min( 1.0, num_actual_positives / ( counts + 1));
          prev_progress = progress;
        }
      }

      sum_precision_ideal /=
          ( X_AXIS_RANGE.GetMin() == double( 0))
          ? std::log10( X_AXIS_RANGE.GetMax()) - std::log10( step_size)
          : std::log10( X_AXIS_RANGE.GetMax()) - std::log10( X_AXIS_RANGE.GetMin());

      return sum_precision_ideal;
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionIntegralPrecisionFractionPredicted::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );
      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values have different result sizes"
      );

      // sum the precisions and precision idel for each result
      float sum_precision( 0), sum_precision_ideal( 0);

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>
            (
              PREDICTED( counter)( result_number),
              EXPERIMENTAL( counter)( result_number)
            )
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);

        sum_precision +=
          roc_curve.Integral
          (
            &math::ContingencyMatrix::GetFractionPredictedPositives, // x-coordinate
            &math::ContingencyMatrix::GetPrecision,                  // y-coordinate
            m_FPPCutoffRange,                                        // x-coordinate cutoff range
            true                                                     // x-axis log10 scaling on
          );

        // calculate integral of corresponding ideal ppv vs fpp curve
        sum_precision_ideal = CalculateIdealIntegral( roc_curve, m_FPPCutoffRange);

        if( sum_precision_ideal > 0)
        {
          sum_precision /= sum_precision_ideal;
        }
        else
        {
          BCL_MessageStd( "Ideal FPPvsPPV integral is 0!");
          sum_precision = float( 0);
        }
      }

      // return the average precision
      return sum_precision / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionIntegralPrecisionFractionPredicted::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates integral of plot Precision vs Fraction Predicted Positives (FPP); "
        "Precision = #True-Positives / (#True-Positives + #False-Positives), "
        "FPP = ( #True-Positives + #False-Positives) / (#Positives + #Negatives)"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "potency cutoff that separates actives from inactives",
        io::Serialization::GetAgent( &m_Cutoff)
      );
      parameters.AddInitializer
      (
        "cutoff_type",
        "x_axis_range determined by a compound range (compound) or a percent range on x axis (fpp_percent)",
        io::Serialization::GetAgentWithCheck
        (
          &m_FPPCutoffType,
          command::ParameterCheckAllowed( storage::Vector< std::string>::Create( "compound", "fpp_percent"))
        )
      );
      parameters.AddInitializer
      (
        "x_axis_range",
        "Fraction Predicted Positives (FPP) cutoff range on x-axis for which integral should be calculated",
        io::Serialization::GetAgent( &m_FPPCutoffRange),
        "x_axis_range=\"[0.0 , 1.0]\""
      );
      parameters.AddInitializer
      (
        "x_axis_compound_range",
        "Fraction Predicted Positives (FPP) cutoff compound range on x-axis for which integral should be calculated"
        "the compound range translates into a x_axis_range between 0 and 1, it super-seeds x_axis_range",
        io::Serialization::GetAgent( &m_FPPCompoundCutoffRange),
        "x_axis_compound_range=\"[25 , 250]\""
      );
      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_integral_tnr_tpr.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_contingency_matrix.h"
#include "math/bcl_math_roc_curve.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionIntegralTnrTpr::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionIntegralTnrTpr()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionIntegralTnrTpr::ObjectiveFunctionIntegralTnrTpr() :
      m_Cutoff(),
      m_PositivesAboveThreshold()
    {
    }

    //! @brief constructor from monitor data
    //! @param CUTOFF cutoff to determining the area under the curve
    //! @param FRACTION_PRED_POS_CUTOFF cuoff for fraction predicted positives
    //! @param POSITIVES_ABOVE_THRESHOLD flag for sorting list of predicted and experimental values in roc curve
    ObjectiveFunctionIntegralTnrTpr::ObjectiveFunctionIntegralTnrTpr
    (
      const float CUTOFF,
      const bool POSITIVES_ABOVE_THRESHOLD
    ) :
      m_Cutoff( CUTOFF),
      m_PositivesAboveThreshold( POSITIVES_ABOVE_THRESHOLD)
    {
    }

    //! @brief Clone function
    //! @return pointer to new ObjectiveFunctionIntegralTnrTpr
    ObjectiveFunctionIntegralTnrTpr *
    ObjectiveFunctionIntegralTnrTpr::Clone() const
    {
      return new ObjectiveFunctionIntegralTnrTpr( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionIntegralTnrTpr::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionIntegralTnrTpr::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // number of data points in dataset
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());

      // sum the precisions and precision idel for each result
      float sum_precision( 0);

      for( size_t result_number( 0); result_number < result_size; ++result_number)
      {
        // list of pairs with pred and exp values for ROC Curve
        storage::List< storage::Pair< double, double> > values_predicted_experimental;

        // iterate over all experimental and predicted values and check for accuracy
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          values_predicted_experimental.PushBack
          (
            storage::Pair< double, double>
            (
              PREDICTED( counter)( result_number),
              EXPERIMENTAL( counter)( result_number)
            )
          );
        }

        // create roc curve according to cutoff
        math::ROCCurve roc_curve( values_predicted_experimental, m_Cutoff, m_PositivesAboveThreshold);

        sum_precision += roc_curve.Integral
          (
            &math::ContingencyMatrix::GetTrueNegativeRate,           // x-coordinate
            &math::ContingencyMatrix::GetTruePositiveRate,           // y-coordinate
            math::Range< double>(0,1),                               // x-coordinate cutoff range
            false                                                    // x-axis log10 scaling on
          );
      }

      // return the average precision
      return sum_precision / float( result_size);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionIntegralTnrTpr::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates integral of plotting true negative rate (TNR) vs true positive rate (TPR); "
        "TNR =  #True-Negatives / (#False-Positives + #True-Negatives), "
        "TPR =  #True-Positives / (#True-Positives + #False-Negatives)"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "potency cutoff that separates actives from inactives",
        io::Serialization::GetAgent( &m_Cutoff)
      );

      parameters.AddInitializer
      (
        "parity",
        "specifies actives are above or below cutoff, 0 - below, 1 - above",
        io::Serialization::GetAgent( &m_PositivesAboveThreshold),
        "0"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_interface.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_feature_data_reference.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    //! @brief Goal as string
    //! @param TYPE the goal
    //! @return the string for the goal
    const std::string &ObjectiveFunctionInterface::GetGoalName( const Goal &TYPE)
    {
      static const std::string s_names[] =
      {
        "Classification",     // Objective is based on ranking or a threshold
        "RankClassification", // Objective is based on ranking
        "Regression",         // Objective is based on minimizing error
        "Other",              // Objective is based on some other criteria
        GetStaticClassName< Goal>()
      };
      return s_names[ TYPE];
    }

    //! @brief get the threshold, for classification type objectives
    //! @return the threshold, for classification type objectives
    float ObjectiveFunctionInterface::GetThreshold() const
    {
      return util::GetUndefined< float>();
    }

    //! @brief get the parity, for rank classification type objectives
    //! @return the parity, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; true means the objective is most interested
    //!       in prediction of values higher than the threshold, false means below threshold
    bool ObjectiveFunctionInterface::GetRankingParity() const
    {
      return true;
    }

    //! @brief get the desired hit rate (e.g. fraction of positive predictions)
    //! @return the desired hit rate, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; 0.01 means, for example, that only the top 1%
    //!       of values will be considered
    float ObjectiveFunctionInterface::GetDesiredHitRate() const
    {
      return util::GetUndefined< float>();
    }

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionInterface::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      const size_t dataset_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());
      linal::Matrix< char> prediction_classification( dataset_size, result_size, '\0');
      const float threshold( GetThreshold());
      const float desired_hit_rate( GetDesiredHitRate());
      const bool ranking_parity( GetRankingParity());
      if( GetGoalType() == e_RankClassification)
      {
        storage::Vector< float> column( dataset_size, 0.0);
        for( size_t j( 0); j < result_size; ++j)
        {
          float prediction_cutoff( 0.0);
          if( util::IsDefined( desired_hit_rate))
          {
            for( size_t i( 0); i < dataset_size; ++i)
            {
              column( i) = PREDICTED( i)( j);
            }
            if( ranking_parity)
            {
              column.Sort( std::greater< float>());
            }
            else
            {
              column.Sort( std::less< float>());
            }

            // get the prediction cutoff
            prediction_cutoff =
              column
              (
                std::min
                (
                  size_t( column.GetSize() - 1),
                  size_t( column.GetSize() * desired_hit_rate)
                )
              );
          }
          else
          {
            column.Resize( 0);
            // auc or similar ranking.  Just choose the cutoff at which precision = 50%
            if( ranking_parity)
            {
              for( size_t i( 0); i < dataset_size; ++i)
              {
                if( EXPERIMENTAL( i)( j) >= threshold)
                {
                  column.PushBack( PREDICTED( i)( j));
                }
              }
            }
            else
            {
              for( size_t i( 0); i < dataset_size; ++i)
              {
                if( EXPERIMENTAL( i)( j) <= threshold)
                {
                  column.PushBack( PREDICTED( i)( j));
                }
              }
            }
            column.Sort( std::less< float>());

            // get the prediction cutoff
            prediction_cutoff = column( size_t( column.GetSize() * 0.5));
          }

          if( ranking_parity)
          {
            for( size_t i( 0); i < dataset_size; ++i)
            {
              const bool experimental_above_cutoff( EXPERIMENTAL( i)( j) >= threshold);
              if( ( PREDICTED( i)( j) >= prediction_cutoff) == experimental_above_cutoff)
              {
                // true
                prediction_classification( i, j) = experimental_above_cutoff ? 'P' : 'N';
              }
              else
              {
                // false
                prediction_classification( i, j) = experimental_above_cutoff ? 'n' : 'p';
              }
            }
          }
          else
          {
            for( size_t i( 0); i < dataset_size; ++i)
            {
              const bool experimental_below_cutoff( EXPERIMENTAL( i)( j) <= threshold);
              if( ( PREDICTED( i)( j) <= prediction_cutoff) == experimental_below_cutoff)
              {
                prediction_classification( i, j) = experimental_below_cutoff ? 'P' : 'N';
              }
              else
              {
                prediction_classification( i, j) = experimental_below_cutoff ? 'n' : 'p';
              }
            }
          }
        }
      }
      else if( GetGoalType() == e_Classification)
      {
        // for classification purposes, only use Pn
        for( size_t i( 0); i < dataset_size; ++i)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            prediction_classification( i, j)
              = char( ( EXPERIMENTAL( i)( j) >= threshold) == ( PREDICTED( i)( j) >= threshold) ? 'P' : 'n');
          }
        }
      }
      else
      {
        // for each column, compute the standard deviation of the deviations from experimental values
        // declare predictions incorrect that are more than 1 standard deviations from the mean deviation for this model
        for( size_t j( 0); j < result_size; ++j)
        {
          math::RunningAverageSD< float> result_ave_deviation;
          for( size_t i( 0); i < dataset_size; ++i)
          {
            if( util::IsDefined( EXPERIMENTAL( i)( j)))
            {
              result_ave_deviation += math::Absolute( PREDICTED( i)( j) - EXPERIMENTAL( i)( j));
            }
          }
          const float threshold_deviation( result_ave_deviation.GetAverage() + result_ave_deviation.GetStandardDeviation());
          for( size_t i( 0); i < dataset_size; ++i)
          {
            prediction_classification( i, j)
              = char( !util::IsDefined( EXPERIMENTAL( i)( j)) || math::Absolute( PREDICTED( i)( j) - EXPERIMENTAL( i)( j)) <= threshold_deviation ? 'P' : 'n');
          }
        }
      }
      return prediction_classification;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_mae.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionMae::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionMae( e_None)
      )
    );
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionMae::s_FractionExplainedInstance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionMae( e_FractionExplained)
      )
    );
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionMae::s_MADInstance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionMae( e_MeanAbsoluteDeviation)
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionMae::ObjectiveFunctionMae( const Normalization &NORM) :
      m_Normalization( NORM)
    {
    }

    //! copy constructor
    ObjectiveFunctionMae *ObjectiveFunctionMae::Clone() const
    {
      return new ObjectiveFunctionMae( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ObjectiveFunctionMae::GetAlias() const
    {
      static const std::string s_name( "MAE"), s_fe_mae( "MAE_FractionExplained"), s_mad( "MAE_NMAD");
      return m_Normalization == e_None ? s_name : m_Normalization == e_FractionExplained ? s_fe_mae : s_mad;
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionMae::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      const size_t n_results( DATA.GetFeatureSize());
      storage::Vector< float> averages( n_results);
      if( m_Normalization == e_MeanAbsoluteDeviation || m_Normalization == e_FractionExplained)
      {
        storage::Vector< math::RunningAverage< float> > average_result_vals( n_results);
        for( size_t row_id( 0), n_rows( DATA.GetNumberFeatures()); row_id < n_rows; ++row_id)
        {
          for( size_t res_id( 0); res_id < n_results; ++res_id)
          {
            if( util::IsDefined( DATA( row_id)( res_id)))
            {
              average_result_vals( res_id) += DATA( row_id)( res_id);
            }
          }
        }
        for( size_t res_id( 0); res_id < n_results; ++res_id)
        {
          averages( res_id) = average_result_vals( res_id).GetAverage();
          average_result_vals( res_id).Reset();
        }
        for( size_t row_id( 0), n_rows( DATA.GetNumberFeatures()); row_id < n_rows; ++row_id)
        {
          for( size_t res_id( 0); res_id < n_results; ++res_id)
          {
            if( util::IsDefined( DATA( row_id)( res_id)))
            {
              average_result_vals( res_id) += math::Absolute( DATA( row_id)( res_id) - averages( res_id));
            }
          }
        }
        m_AverageDeviations.Resize( n_results);
        for( size_t res_id( 0); res_id < n_results; ++res_id)
        {
          m_AverageDeviations( res_id) = average_result_vals( res_id).GetAverage();
        }
      }
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionMae::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );

      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values do not have the same number of elements!"
      );

      // number of experimental values
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());

      // number of predicted result columns
      const size_t result_size( PREDICTED.GetFeatureSize());

      if( data_set_size == 0 || result_size == 0)
      {
        // no data available, no result column given
        return float( 0);
      }

      // runnung average rmsd value
      storage::Vector< math::RunningAverage< float> > avg_compare( result_size);

      // iterate over all rows in dataset
      for( size_t counter( 0); counter < data_set_size; ++counter)
      {
        // iterate over all rows in dataset
        for( size_t result_index( 0); result_index < result_size; ++result_index)
        {
          if( util::IsDefined( EXPERIMENTAL( counter)( result_index)))
          {
            // get difference between experimental value and predicted values
            avg_compare( result_index)
              += math::Absolute( EXPERIMENTAL( counter)( result_index) - PREDICTED( counter)( result_index));
          }
        }
      }
      math::RunningAverage< float> avg_sum;
      for( size_t result_index( 0); result_index < result_size; ++result_index)
      {
        if( m_Normalization == e_None)
        {
          avg_sum += avg_compare( result_index).GetAverage();
          BCL_MessageStd
          (
            "MAE Detail Result Index: " + util::Format()( result_index)
            + " MAE: " + util::Format()( avg_compare( result_index).GetAverage())
          );
        }
        else if( m_Normalization == e_MeanAbsoluteDeviation)
        {
          avg_sum += avg_compare( result_index).GetAverage() / m_AverageDeviations( result_index);
          BCL_MessageStd
          (
            "MAE Detail Result Index: " + util::Format()( result_index)
            + " MAE: " + util::Format()( avg_compare( result_index).GetAverage())
            + " MAD: " + util::Format()( m_AverageDeviations( result_index))
            + " MAE_NormByMAD: " +
            util::Format()
            (
              avg_compare( result_index).GetAverage() / m_AverageDeviations( result_index)
            )
          );
        }
        else if( m_Normalization == e_FractionExplained)
        {
          avg_sum += 1.0 - avg_compare( result_index).GetAverage() / m_AverageDeviations( result_index);
          BCL_MessageStd
          (
            "MAE Detail Result Index: " + util::Format()( result_index)
            + " MAE: " + util::Format()( avg_compare( result_index).GetAverage())
            + " MAD: " + util::Format()( m_AverageDeviations( result_index))
            + " MAD_FractionExplained: " +
            util::Format()
            (
              1.0 - avg_compare( result_index).GetAverage() / m_AverageDeviations( result_index)
            )
          );
        }
      }

      return avg_sum.GetAverage();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionMae::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        m_Normalization == e_None
        ? "Calculates the mean-absolute-deviation between predicted and actual results"
        : (
             m_Normalization == e_FractionExplained
             ? "Calculates the fraction of absolute deviation explained"
             : "Mean absolute error divided by mean absolute deviation of each result column in the dataset"
           )
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_partial.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionPartial::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionPartial()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    ObjectiveFunctionPartial *ObjectiveFunctionPartial::Clone() const
    {
      return new ObjectiveFunctionPartial( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ObjectiveFunctionPartial::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionPartial::GetImprovementType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Weight >= 0.0
               ? m_Objective->GetImprovementType()
               : m_Objective->GetImprovementType() == opti::e_LargerEqualIsBetter
                 ? opti::e_SmallerEqualIsBetter
                 : opti::e_LargerEqualIsBetter;
      }
      return opti::e_LargerEqualIsBetter;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionPartial::GetGoalType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetGoalType();
      }
      return e_Other;
    }

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionPartial::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      const size_t dataset_size( EXPERIMENTAL.GetNumberFeatures());
      const size_t result_size( EXPERIMENTAL.GetFeatureSize());
      linal::Matrix< char> prediction_classification( dataset_size, result_size, '\0');

      // filter the undesired outputs from the ranges
      const FeatureDataSet< float> experimental_reduced( EXPERIMENTAL.GetMatrix(), m_Ranges);
      const FeatureDataSet< float> predicted_reduced( PREDICTED.GetMatrix(), m_Ranges);

      // get the classifications from the internal objective function
      linal::Matrix< char> internal_prediction_classification
      (
        m_Objective->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED)
      );

      // determine the corresponding index of each value in the range
      storage::Vector< size_t> mapping;
      for( size_t j( 0); j < result_size; ++j)
      {
        if( m_Ranges.IsWithin( j))
        {
          mapping.PushBack( j);
        }
      }

      BCL_Assert( mapping.GetSize() == internal_prediction_classification.GetNumberCols(), "Mapping is broken");

      const size_t mapping_size( mapping.GetSize());

      // copy the values from the internal objective function into the returned matrix at the appropriate positions
      for( size_t i( 0); i < dataset_size; ++i)
      {
        for( size_t j( 0); j < mapping_size; ++j)
        {
          prediction_classification( i, mapping( j)) = internal_prediction_classification( i, j);
        }
      }

      return prediction_classification;
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionPartial::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      const FeatureDataSet< float> experimental_reduced( DATA.GetMatrix(), m_Ranges);

      m_Objective->SetData( experimental_reduced, IDS);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionPartial::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // filter the undesired outputs from the ranges
      const FeatureDataSet< float> experimental_reduced( EXPERIMENTAL.GetMatrix(), m_Ranges);
      const FeatureDataSet< float> predicted_reduced( PREDICTED.GetMatrix(), m_Ranges);

      return m_Weight * m_Objective->operator ()( experimental_reduced, predicted_reduced);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionPartial::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "A weighted meta-objective function that uses only a subset of the result data"
      );

      parameters.AddInitializer
      (
        "weight",
        "amount by which to weight the objective function's output",
        io::Serialization::GetAgent( &m_Weight),
        "1.0"
      );

      parameters.AddInitializer
      (
        "function",
        "core objective function to use",
        io::Serialization::GetAgent( &m_Objective)
      );
      parameters.AddInitializer
      (
        "outputs",
        "ranges of chunks to load, e.g. outputs=\"[ 0, 5)+(7,10)\"",
        io::Serialization::GetAgent( &m_Ranges),
        "[0]"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_rmsd.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_running_average_sd.h"
#include "math/bcl_math_running_min_max.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionRmsd::s_RawInstance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionRmsd( ObjectiveFunctionRmsd::e_None)
      )
    );
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionRmsd::s_MinMaxInstance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionRmsd( ObjectiveFunctionRmsd::e_MinMax)
      )
    );
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionRmsd::s_StdInstance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionRmsd( ObjectiveFunctionRmsd::e_Std)
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor from type of normalization
    ObjectiveFunctionRmsd::ObjectiveFunctionRmsd( const Normalization &NORM) :
      m_Normalization( NORM)
    {
    }

    //! copy constructor
    ObjectiveFunctionRmsd *ObjectiveFunctionRmsd::Clone() const
    {
      return new ObjectiveFunctionRmsd( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ObjectiveFunctionRmsd::GetAlias() const
    {
      static const std::string s_name( "RMSD"), s_normalized_name( "NRMSD"), s_normalized_std_name( "RMSD_NSTD");
      return m_Normalization == e_None ? s_name
             : m_Normalization == e_MinMax ? s_normalized_name
             : s_normalized_std_name;
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionRmsd::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == PREDICTED.GetNumberFeatures(),
        "Experimental and predicted values do not have the same number of elements!"
      );

      BCL_Assert
      (
        EXPERIMENTAL.GetFeatureSize() == PREDICTED.GetFeatureSize(),
        "Experimental and predicted values do not have the same element size!" + util::Format()( EXPERIMENTAL.GetFeatureSize()) + " " + util::Format()( PREDICTED.GetFeatureSize())
      );

      // number of experimental values
      const size_t data_set_size( EXPERIMENTAL.GetNumberFeatures());

      // number of predicted result columns
      const size_t result_size( PREDICTED.GetFeatureSize());

      if( data_set_size == 0 || result_size == 0)
      {
        // no data available, no result column given
        return float( 0);
      }

      // handle simple normalization
      if( m_Normalization == e_None)
      {
        // runnung average rmsd value
        math::RunningAverage< float> avg_compare;

        // iterate over all rows in dataset
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          // runnung rmsd value
          float compare( 0);

          // iterate over all rows in dataset
          for( size_t result_index( 0); result_index < result_size; ++result_index)
          {
            if( util::IsDefined( EXPERIMENTAL( counter)( result_index)))
            {
              // get difference between experimental value and predicted values
              float tmp( EXPERIMENTAL( counter)( result_index) - PREDICTED( counter)( result_index));
              compare += tmp * tmp;
            }
          }

          // add to avg rmsd accumulation
          avg_compare += compare;
        }

        return math::Sqrt( avg_compare.GetAverage() / float( result_size));
      }
      else if( m_Normalization == e_MinMax)
      {
        // track the average difference in each column
        storage::Vector< math::RunningAverage< float> > avg_difference_sq( result_size);

        // track the mins and maxes for each column
        storage::Vector< math::RunningMinMax< float> > min_max( result_size);

        // iterate over all rows in dataset
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          // iterate over all rows in dataset
          for( size_t result_index( 0); result_index < result_size; ++result_index)
          {
            if( util::IsDefined( EXPERIMENTAL( counter)( result_index)))
            {
              // get difference between experimental value and predicted values
              float tmp( EXPERIMENTAL( counter)( result_index) - PREDICTED( counter)( result_index));
              avg_difference_sq( result_index) += tmp * tmp;
              min_max( result_index) += EXPERIMENTAL( counter)( result_index);
            }
          }
        }
        // get the average difference, normalized by max-min of each input column
        math::RunningAverage< float> ave_difference_norm;
        for( size_t result_index( 0); result_index < result_size; ++result_index)
        {
          // get this column's range
          const float range( min_max( result_index).GetMax() - min_max( result_index).GetMin());
          if( range > float( 0.0))
          {
            ave_difference_norm += avg_difference_sq( result_index) / math::Sqr( range);
          }
        }
        return math::Sqrt( ave_difference_norm.GetAverage());
      }
      else if( m_Normalization == e_Std)
      {
        // track the average difference in each column
        storage::Vector< math::RunningAverage< float> > avg_difference_sq( result_size);

        // track the mins and maxes for each column
        storage::Vector< math::RunningAverageSD< float> > variances( result_size);

        // iterate over all rows in dataset
        for( size_t counter( 0); counter < data_set_size; ++counter)
        {
          // iterate over all rows in dataset
          for( size_t result_index( 0); result_index < result_size; ++result_index)
          {
            if( util::IsDefined( EXPERIMENTAL( counter)( result_index)))
            {
              // get difference between experimental value and predicted values
              float tmp( EXPERIMENTAL( counter)( result_index) - PREDICTED( counter)( result_index));
              avg_difference_sq( result_index) += tmp * tmp;
              variances( result_index) += EXPERIMENTAL( counter)( result_index);
            }
          }
        }
        // get the average difference, normalized by max-min of each input column
        math::RunningAverage< float> ave_difference_norm;
        std::stringstream oss;
        for( size_t result_index( 0); result_index < result_size; ++result_index)
        {
          // get this column's range
          const float range( variances( result_index).GetVariance());
          if( range > float( 0.0))
          {
            ave_difference_norm += avg_difference_sq( result_index) / range;
            oss << "Result " << result_index << " RMSD_NSTD " << math::Sqrt( avg_difference_sq( result_index) / range)
                << " Variance explained " << 1.0 - avg_difference_sq( result_index) / range << '\n';
          }
        }
        BCL_MessageVrb( "RMSD detail: " + oss.str());
        return math::Sqrt( ave_difference_norm.GetAverage());
      }
      return 0.0;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionRmsd::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates the root-mean-squared-deviation between predicted and actual results"
        "normalized by # of outputs "
        + std::string
          (
            m_Normalization == e_None ? "" :
            m_Normalization == e_MinMax ? "and each result column's range (max-min) " :
            m_Normalization == e_Std ? "and each result column's standard deviation " : ""
          )
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_segment_overlap.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_const_reference.h"
#include "math/bcl_math_running_average.h"
#include "model/bcl_model_data_set_select_columns.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionSegmentOverlap::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionSegmentOverlap()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionSegmentOverlap::ObjectiveFunctionSegmentOverlap() :
      ObjectiveFunctionCategoricalMax(),
      m_PerIdOutput( false),
      m_OutputSubclassOverlaps( false),
      m_WeightPerElement( false),
      m_SequenceIdLabel( "ProteinId"),
      m_ElementIdLabel( "AASeqID"),
      m_MissingIds(),
      m_BoundaryIds(),
      m_ClassCounts(),
      m_SequenceNames(),
      m_SequenceSizes(),
      m_ActualSegments()
    {
    }

    //! @brief Clone function
    //! @return pointer to new Evaluator
    ObjectiveFunctionSegmentOverlap *ObjectiveFunctionSegmentOverlap::Clone() const
    {
      return new ObjectiveFunctionSegmentOverlap( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionSegmentOverlap::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionSegmentOverlap::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      // reset all data members
      m_MissingIds.Reset();
      m_BoundaryIds.Reset();
      m_SequenceNames.Reset();
      m_SequenceSizes.Reset();
      m_ActualSegments.Reset();

      // set data on the base class
      ObjectiveFunctionCategoricalMax::SetData( DATA, IDS);

      // get the feature labels so that sequences can be identified
      const FeatureLabelSet model_feature_labels( *IDS.GetFeatureLabelSet());
      DataSetSelectColumns sequence_id_selector
      (
        model_feature_labels.GetSize(),
        IDS.GetFeatureLabelSet()->GetPropertyIndices( m_SequenceIdLabel)
      );
      DataSetSelectColumns element_id_selector
      (
        model_feature_labels.GetSize(),
        IDS.GetFeatureLabelSet()->GetPropertyIndices( m_ElementIdLabel)
      );
      BCL_Assert( sequence_id_selector.GetOutputFeatureSize(), "Could not find " + m_SequenceIdLabel.ToString() + " in IDs!");
      BCL_Assert( element_id_selector.GetOutputFeatureSize(), "Could not find " + m_ElementIdLabel.ToString() + " in IDs!");
      const size_t dataset_size( DATA.GetNumberFeatures());
      BCL_Assert( dataset_size == IDS.GetNumberFeatures(), "IDs and Data have different size!");

      // add boundary for the first sequence, unless the dataset is empty
      if( dataset_size)
      {
        m_BoundaryIds.PushBack( 0);
      }
      {
        // locate all the sequence boundaries and ids
        std::string last_sequence_id( sequence_id_selector.GetOutputFeatureSize(), ' ');
        std::string this_sequence_id( last_sequence_id);
        for( size_t i( 0); i < dataset_size; ++i)
        {
          sequence_id_selector( IDS( i), &this_sequence_id[ 0]);
          if( this_sequence_id != last_sequence_id)
          {
            if( i)
            {
              m_SequenceSizes.PushBack( i - m_BoundaryIds.LastElement());
              m_BoundaryIds.PushBack( i);
            }
            last_sequence_id = this_sequence_id;
            m_SequenceNames.PushBack( this_sequence_id);
          }
        }
      }

      // add the final sequence's size and boundary id information
      if( dataset_size)
      {
        m_SequenceSizes.PushBack( dataset_size - m_BoundaryIds.LastElement());
        m_BoundaryIds.PushBack( dataset_size);
      }

      // get the number of sequences
      const size_t number_sequences( m_SequenceSizes.GetSize());
      const size_t number_class_groups( this->GetClassBoundaries().GetSize());
      const size_t number_results( DATA.GetFeatureSize());

      // create the other data members

      // missing ids for each sequence
      m_MissingIds.Reset();
      m_MissingIds.Resize( number_sequences);

      // class counts
      m_ClassCounts = linal::Matrix< size_t>( number_sequences, number_results);

      // base-classes matrix containing the class for each row
      const linal::Matrix< size_t> &actual_classes( this->GetActualClasses());

      // segments
      m_ActualSegments.Reset();
      m_ActualSegments.Resize( number_sequences);
      m_ActualSegments.SetAllElements
      (
        storage::Vector< storage::Vector< storage::Triplet< size_t, size_t, size_t> > >( number_class_groups)
      );

      for( auto itr_perm( m_AllowedPermutations.Begin()), itr_perm_end( m_AllowedPermutations.End()); itr_perm != itr_perm_end; ++itr_perm)
      {
        BCL_Assert( itr_perm->GetSize() == number_results, "All permutations should have the same size as the result output!");
      }
      m_AllowedPermutations.InsertElements( 0, storage::CreateIndexVector( number_results));

      size_t sequence_number( 0);
      // members for missing ids in the sequence
      std::string this_element_id_str( element_id_selector.GetOutputFeatureSize(), ' ');
      for
      (
        storage::Vector< size_t>::const_iterator
          itr_bounds( m_BoundaryIds.Begin()),
          itr_bounds_next( m_BoundaryIds.Begin() + 1),
          itr_bounds_end( m_BoundaryIds.End());
        itr_bounds_next != itr_bounds_end;
        ++itr_bounds, ++itr_bounds_next, ++sequence_number
      )
      {
        const size_t protein_start_row( *itr_bounds);
        const size_t protein_end_row( *itr_bounds_next);
        size_t last_element_id( util::GetUndefinedSize_t());

        // get a reference to the relevant row in the class-bounds matrix
        linal::VectorReference< size_t> class_counts( m_ClassCounts.GetRow( sequence_number));

        // get a reference to the vector where missing ids will be placed
        storage::Vector< size_t> &missing_id_vec( m_MissingIds( sequence_number));

        for( size_t row( protein_start_row); row < protein_end_row; ++row)
        {
          // accumulate class counts
          for( size_t subclass( 0); subclass < number_class_groups; ++subclass)
          {
            ++class_counts( actual_classes( row, subclass));
          }

          // get this row's ids
          element_id_selector( IDS( row), &this_element_id_str[ 0]);

          // get the id as an integer
          const size_t this_id( util::ConvertStringToNumericalValue< size_t>( this_element_id_str));

          // compare the id to the previous id
          if( row == protein_start_row)
          {
            // first row, just accept this id as the previous id
            last_element_id = this_id;
          }
          else
          {
            // after the first row
            if( this_id != last_element_id + 1 && actual_classes.GetRow( row) != actual_classes.GetRow( row - 1))
            {
              missing_id_vec.PushBack( row);
            }
            last_element_id = this_id;
          }
        }
        missing_id_vec.PushBack( protein_end_row);

        // get the segments for each subclass of this one
        for( size_t subclass( 0); subclass < number_class_groups; ++subclass)
        {
          m_ActualSegments( sequence_number)( subclass) = GetSegments( subclass, sequence_number, actual_classes);
        }
      }
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    float ObjectiveFunctionSegmentOverlap::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      // compute QX
      const float basic_state_accuracy( ObjectiveFunctionCategoricalMax::operator()( EXPERIMENTAL, PREDICTED));

      BCL_Assert
      (
        EXPERIMENTAL.GetNumberFeatures() == size_t( 0) || !m_SequenceSizes.IsEmpty(),
        "SetData was not called before operator()!"
      );

      // compute average SOVs, one for each subclass
      const size_t number_subclasses( EXPERIMENTAL.GetFeatureSize());
      storage::Vector< math::RunningAverage< float> > segment_overlap_aves( number_subclasses);

      // compute average SOVs, one for each class
      const size_t number_classifications( this->GetClassBoundaries().GetSize());
      storage::Vector< math::RunningAverage< float> > segment_overlap_overall_aves( number_classifications);
      math::RunningAverage< float> combined_ave;

      // create a vector with the class boundaries and then the number of results
      storage::Vector< size_t> all_class_boundaries( size_t( 1), size_t( 0));
      all_class_boundaries.Append
      (
        storage::Vector< size_t>( this->GetClassBoundaries().Begin(), this->GetClassBoundaries().End())
      );
      const bool consider_perms( m_PerIdOutput && m_AllowedPermutations.GetSize() > size_t( 1));

      std::ostringstream output;
      output << "\nQ-overall: " << basic_state_accuracy << '\n';
      if( m_PerIdOutput)
      {
        output << "Sequence\t";
      }
      for( size_t classification_number( 0); classification_number < number_classifications; ++classification_number)
      {
        output << "SOV Overall Class " << classification_number << '\t';
        if( m_OutputSubclassOverlaps)
        {
          const size_t subclass_start( all_class_boundaries( classification_number));
          const size_t subclass_end( all_class_boundaries( classification_number + 1));

          for( size_t subclass_id( subclass_start); subclass_id < subclass_end; ++subclass_id)
          {
            output << "SOV Subclass " << subclass_id << '\t';
          }
        }
        if( consider_perms)
        {
          output << "Permutation #";
        }
      }
      output << '\n';
      // compute SOV for each sequence and subclass type, if desired
      for
      (
        size_t sequence_number( 0), number_sequences( m_SequenceSizes.GetSize());
        sequence_number < number_sequences;
        ++sequence_number
      )
      {
        if( m_PerIdOutput)
        {
          output << m_SequenceNames( sequence_number) << '\t';
        }
        const float seq_size( m_SequenceSizes( sequence_number));

        // for each classification
        for( size_t classification_number( 0); classification_number < number_classifications; ++classification_number)
        {
          // compute overall segment overlaps
          const storage::Pair< float, size_t> sov_this_sequence_classification
          (
            GetSegmentOverlap( classification_number, sequence_number)
          );

          if( !m_WeightPerElement)
          {
            segment_overlap_overall_aves( classification_number) += sov_this_sequence_classification.First();
            combined_ave += sov_this_sequence_classification.First();
          }
          else
          {
            segment_overlap_overall_aves( classification_number).AddWeightedObservation
            (
              sov_this_sequence_classification.First(),
              double( seq_size)
            );
            combined_ave.AddWeightedObservation( sov_this_sequence_classification.First(), double( seq_size));
          }
          if( m_PerIdOutput)
          {
            output << sov_this_sequence_classification.First() << '\t';
          }

          // compute segment overlaps for every subclass of this sequence, if desired
          if( m_OutputSubclassOverlaps)
          {
            const size_t subclass_start( all_class_boundaries( classification_number));
            const size_t subclass_end( all_class_boundaries( classification_number + 1));
            for( size_t subclass_id( subclass_start); subclass_id < subclass_end; ++subclass_id)
            {
              const float sov_this_sequence_subclassification
              (
                GetSegmentOverlap( classification_number, sequence_number, subclass_id, sov_this_sequence_classification.Second()).First()
              );

              if( !m_WeightPerElement)
              {
                segment_overlap_aves( subclass_id) += sov_this_sequence_subclassification;
              }
              else
              {
                segment_overlap_aves( subclass_id).AddWeightedObservation
                (
                  sov_this_sequence_subclassification,
                  seq_size
                );
              }

              if( m_PerIdOutput)
              {
                output << sov_this_sequence_subclassification << '\t';
              }
            }
          }
          if( consider_perms)
          {
            output << sov_this_sequence_classification.Second() << '\t';
          }
        }
        if( m_PerIdOutput)
        {
          output << '\n';
        }
      }
      if( m_PerIdOutput)
      {
        output << "Overall\t";
      }
      for( size_t classification_number( 0); classification_number < number_classifications; ++classification_number)
      {
        output << segment_overlap_overall_aves( classification_number).GetAverage() << '\t';
        if( m_OutputSubclassOverlaps)
        {
          const size_t subclass_start( all_class_boundaries( classification_number));
          const size_t subclass_end( all_class_boundaries( classification_number + 1));

          for( size_t subclass_id( subclass_start); subclass_id < subclass_end; ++subclass_id)
          {
            output << segment_overlap_aves( subclass_id).GetAverage() << '\t';
          }
        }
      }
      BCL_MessageStd( output.str());
      // return accuracy
      return combined_ave.GetAverage();
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionSegmentOverlap::GetSerializer() const
    {
      io::Serializer parameters( ObjectiveFunctionCategoricalMax::GetSerializer());
      parameters.SetClassDescription
      (
        "Segment overlap is a common method for benchmarking secondary structure prediction methods. "
        "The version implemented here is the updated definition from Zemla et al. - PROTEINS: Structure, Function, and "
        "Genetics, 34, 1999, pp. 220-223"
      );
      parameters.AddInitializer
      (
        "output sequence info",
        "whether to output information for every sequence member",
        io::Serialization::GetAgent( &m_PerIdOutput),
        "False"
      );
      parameters.AddInitializer
      (
        "output subclass overlaps",
        "Whether to output SOV for every subclass; for secondary structure predictions, these are usually termed "
        "SOV-Helix, SOV-Strand, SOV-Coil",
        io::Serialization::GetAgent( &m_OutputSubclassOverlaps),
        "False"
      );
      parameters.AddInitializer
      (
        "element weight",
        "Whether to weight each sequence's contribution to the SOV by its number of elements (e.g. residues)",
        io::Serialization::GetAgent( &m_WeightPerElement),
        "False"
      );
      parameters.AddInitializer
      (
        "sequence id",
        "label from the ids that indicates the sequence",
        io::Serialization::GetAgent( &m_SequenceIdLabel),
        "ProteinId"
      );
      parameters.AddInitializer
      (
        "element id",
        "label from the ids that indicates the element's position within the sequence",
        io::Serialization::GetAgent( &m_ElementIdLabel),
        "AASeqID"
      );
      parameters.AddOptionalInitializer
      (
        "equivalences",
        "If, for a given sequence, a permutation of the labels is equally valid, it can be indicated here. For example, "
        "for membrane proteins if we have the labels Inside/Outside/Membrane, for most purposes it would be fine to flip "
        "the labels for inside and outside, so long as it is done consistently across the protein. Which sequences used "
        "a different permutation will be indicated in the output. For multiple class groups, the indexing will not start over in "
        "each, so a permutation label like (0,1,2,3,4,5) would be the identity permutation regardless of how many internal "
        "class groups there happen to be",
        io::Serialization::GetAgent( &m_AllowedPermutations)
      );
      return parameters;
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool ObjectiveFunctionSegmentOverlap::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERROR_STREAM
    )
    {
      m_ElementIdLabel.SetName( "", true);
      m_SequenceIdLabel.SetName( "", true);
      return true;
    }

    //! @brief get the segments for a given subclass and sequence
    //! @param SUBCLASS subclass index; if just computing secondary structure using on method, this is always 0; but
    //!        when using multiple methods, it may range from 0-number classifications
    //! @param SEQUENCE sequence index
    //! @param CHOSEN_SUBCLASS_MATRIX matrix of chosen sub-classes
    //! @return vector of segments for the given sequence for the given subclass
    ObjectiveFunctionSegmentOverlap::SegmentContainer
      ObjectiveFunctionSegmentOverlap::GetSegments
      (
        const size_t &SUBCLASS,
        const size_t &SEQUENCE,
        const linal::MatrixConstInterface< size_t> &CHOSEN_CLASSES,
        const size_t &PERMUTATION
      ) const
    {
      SegmentContainer segments;
      // get the bounds for the sequence
      const size_t start_row( m_BoundaryIds( SEQUENCE)), end_row( m_BoundaryIds( SEQUENCE + 1));

      // detect empty sequences
      if( start_row == end_row)
      {
        return segments;
      }

      // get the chosen class at the first position
      size_t prev_class( CHOSEN_CLASSES( start_row, SUBCLASS));

      // insert it into the segments
      segments.PushBack( storage::Triplet< size_t, size_t, size_t>( prev_class, start_row, start_row));

      // get an iterator to the missing ids for this sequence
      storage::Vector< size_t>::const_iterator itr_missing( m_MissingIds( SEQUENCE).Begin());

      // get the permutation
      const storage::Vector< size_t> &perm( m_AllowedPermutations( PERMUTATION));

      for( size_t pos( start_row + 1); pos < end_row; ++pos)
      {
        // get the chosen class at this position
        const size_t pos_class( perm( CHOSEN_CLASSES( pos, SUBCLASS)));

        // test whether there was a gap between the previous row and this one
        if( *itr_missing == pos)
        {
          // end the previous segment, start a new one
          segments.PushBack( storage::Triplet< size_t, size_t, size_t>( pos_class, pos, pos));
          ++itr_missing;
          prev_class = pos_class;
        }
        else if( pos_class != prev_class)
        {
          // start a new segment
          segments.PushBack( storage::Triplet< size_t, size_t, size_t>( pos_class, pos, pos));
          prev_class = pos_class;
        }
        else
        {
          // extension to the previous segment
          ++segments.LastElement().Third();
        }
      }
      return segments;
    }

    //! @brief compute segment overlap from predicted to experimental
    //! @param SUBCLASS subclass index; if just computing secondary structure using on method, this is always 0; but
    //!        when using multiple methods, it may range from 0-number classifications
    //! @param SEQUENCE_ID sequence index
    //! @param RESTRICT_CLASS optional value; if set, only consider sub
    //! @param RESTRICT_PERMUTATION optional value; if set, only consider the indicated permutation
    //! @return the segment overlap and the permutation index used
    storage::Pair< float, size_t> ObjectiveFunctionSegmentOverlap::GetSegmentOverlap
    (
      const size_t &SUBCLASS,
      const size_t &SEQUENCE,
      const size_t &RESTRICT_CLASS,
      const size_t &RESTRICT_PERMUTATION
    ) const
    {
      // handle differential permutations
      if( RESTRICT_PERMUTATION == util::GetUndefined< size_t>())
      {
        storage::Pair< float, size_t> res_perm
        (
          GetSegmentOverlap( SUBCLASS, SEQUENCE, util::GetUndefined< size_t>(), size_t( 0))
        );
        for( size_t perm_id( 1), n_perms( m_AllowedPermutations.GetSize()); perm_id < n_perms; ++perm_id)
        {
          storage::Pair< float, size_t> res_perm_tmp
          (
            GetSegmentOverlap( SUBCLASS, SEQUENCE, util::GetUndefined< size_t>(), perm_id)
          );
          if( res_perm_tmp.First() > res_perm.First())
          {
            res_perm = res_perm_tmp;
          }
        }
        if( !util::IsDefined( RESTRICT_CLASS))
        {
          return res_perm;
        }
        return GetSegmentOverlap( SUBCLASS, SEQUENCE, RESTRICT_CLASS, res_perm.Second());
      }

      // get the predicted segments for this sequence
      SegmentContainer predicted_segments( GetSegments( SUBCLASS, SEQUENCE, this->GetPredictedClasses(), RESTRICT_PERMUTATION));

      // get the actual segments for the sequence
      const SegmentContainer &actual_segments( m_ActualSegments( SEQUENCE)( SUBCLASS));

      // test whether a single-class segment overlap was requested
      const bool single_class( util::IsDefined( RESTRICT_CLASS));

      // normalization factor; == sum of actual counts in all classes considered
      double normalization( 0.0);

      double segment_overlap( 0.0);

      // iterate over the actual segments
      for
      (
        SegmentContainer::const_iterator
          itr_actual_seg( actual_segments.Begin()), itr_actual_seg_end( actual_segments.End());
        itr_actual_seg != itr_actual_seg_end;
        ++itr_actual_seg
      )
      {
        bool found_one( false);
        const size_t actual_state( itr_actual_seg->First());

        // get the start and end of this segment
        const size_t &actual_start( itr_actual_seg->Second()), &actual_end( itr_actual_seg->Third());

        // compute the real length of this segment
        const size_t actual_length( actual_end - actual_start + 1);

        // iterate over the predicted segments
        for
        (
          SegmentContainer::const_iterator
            itr_pred_seg( predicted_segments.Begin()), itr_pred_seg_end( predicted_segments.End());
          itr_pred_seg != itr_pred_seg_end;
          ++itr_pred_seg
        )
        {
          // get the predicted segments state
          const size_t pred_state( itr_pred_seg->First());

          // continue if only considering one state and this is not the correct state
          if( single_class && pred_state != RESTRICT_CLASS && actual_state != RESTRICT_CLASS)
          {
            continue;
          }

          // get the start and end of this predicted segment
          const size_t &pred_start( itr_pred_seg->Second()), &pred_end( itr_pred_seg->Third());

          // skip non-overlapping ranges
          if( pred_end < actual_start || pred_start > actual_end)
          {
            continue;
          }

          // compute the predicted length of this segment
          const size_t pred_length( pred_end - pred_start + 1);

          // only consider matching states
          if( actual_state != pred_state)
          {
            continue;
          }

          found_one = true;

          // update normalization factor
          normalization += actual_length;

          // compute the intersection and union of the predicted and actual ranges
          size_t intersection_start( 0), intersection_end( 0), union_start( 0), union_end( 0);
          if( actual_end > pred_end)
          {
            union_end = actual_end;
            intersection_end = pred_end;
          }
          else
          {
            union_end = pred_end;
            intersection_end = actual_end;
          }
          if( actual_start < pred_start)
          {
            union_start = actual_start;
            intersection_start = pred_start;
          }
          else
          {
            union_start = pred_start;
            intersection_start = actual_start;
          }

          // determine the actual length of the intersection and the union
          const size_t intersection_length( intersection_end - intersection_start + 1);
          const size_t union_length( union_end - union_start + 1);

          // compute the delta, which is the number of residues that could be wrong due to trivial factors like an extra
          // residue or two at the ends of an SSE.  The delta length is restricted such that no segment overlap will be
          // > 1
          const size_t delta_length
          (
            std::min
            (
              size_t( union_length - intersection_length), // prevents segment overlap from going larger than 1
              std::min
              (
                intersection_length,
                size_t( std::min( pred_length, actual_length) / 2) // half of the predicted/actual segment length
              )
            )
          );

          segment_overlap += float( intersection_length + delta_length) * float( actual_length) / float( union_length);
//          if( !SEQUENCE)
//          {
//            BCL_Debug( pred_start);
//            BCL_Debug( pred_end);
//            BCL_Debug( actual_start);
//            BCL_Debug( actual_end);
//
//            BCL_Debug( intersection_length);
//            BCL_Debug( delta_length);
//            BCL_Debug( actual_length);
//            BCL_Debug( union_length);
//            BCL_Debug( float( intersection_length + delta_length) * float( actual_length) / float( union_length));
//            BCL_Debug( segment_overlap);
//          }
        }
        if( !found_one && ( actual_state == RESTRICT_CLASS || RESTRICT_CLASS == util::GetUndefined< size_t>()))
        {
          normalization += actual_length;
        }
      }

      if( normalization)
      {
        segment_overlap /= normalization;
      }
      else if( !m_ClassCounts( SEQUENCE, SUBCLASS) && predicted_segments.IsEmpty())
      {
        segment_overlap = 1.0;
      }
      else
      {
        segment_overlap = 0.0;
      }
      //if( !SEQUENCE)
      //{
      //  BCL_Debug( m_SequenceNames( SEQUENCE));
      //  BCL_Debug( SUBCLASS);
      //  BCL_Debug( this->GetSegmentString( SUBCLASS, SEQUENCE));
      //  BCL_Debug( predicted_segments);
      //  BCL_Debug( actual_segments);
      //  BCL_Debug( RESTRICT_CLASS);
      //  BCL_Debug( segment_overlap);
      //  BCL_Debug( normalization);
      //}
      return storage::Pair< float, size_t>( segment_overlap, RESTRICT_PERMUTATION);
    }

    //! @brief create the segment string, useful for checking results
    //! @param SUBCLASS subclass index; if just computing secondary structure using on method, this is always 0; but
    //!        when using multiple methods, it may range from 0-number classifications
    //! @param SEQUENCE_ID sequence index
    //! @return string with format: XXXXX ID,  PSEC,  OSEC
    std::string ObjectiveFunctionSegmentOverlap::GetSegmentString( const size_t &SUBCLASS, const size_t &SEQUENCE) const
    {
      std::ostringstream output;

      // get the actual segments for the sequence
      const SegmentContainer &actual_segments( m_ActualSegments( SEQUENCE)( SUBCLASS));
      SegmentContainer::const_iterator itr_actual_segments( actual_segments.Begin());

      // get the bounds for the sequence
      const size_t start_row( m_BoundaryIds( SEQUENCE)), end_row( m_BoundaryIds( SEQUENCE + 1));

      // get an iterator to the missing ids for this sequence
      storage::Vector< size_t>::const_iterator itr_missing( m_MissingIds( SEQUENCE).Begin());

      size_t missing_so_far( 0);
      for( size_t pos( start_row); pos < end_row; ++pos)
      {
        // test whether there was a gap between the previous row and this one
        if( *itr_missing == pos)
        {
          output << "XXXXX " << pos + missing_so_far << ",-1,-1\n";
          ++missing_so_far;
          ++itr_missing;
          ++itr_actual_segments;
        }
        if( itr_actual_segments->Third() < pos)
        {
          ++itr_actual_segments;
        }
        output << "XXXXX " << pos + missing_so_far << ','
               << this->GetPredictedClasses()( pos, SUBCLASS) << ','
               << itr_actual_segments->First() << '\n';
      }
      return output.str();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_selective.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average.h"
#include "model/bcl_model_data_set_select_columns.h"
#include "model/bcl_model_feature_label_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionSelective::s_Instance
    (
      util::Enumerated< ObjectiveFunctionInterface>::AddInstance
      (
        new ObjectiveFunctionSelective()
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! copy constructor
    ObjectiveFunctionSelective *ObjectiveFunctionSelective::Clone() const
    {
      return new ObjectiveFunctionSelective( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ObjectiveFunctionSelective::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionSelective::GetImprovementType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetImprovementType();
      }
      return opti::e_LargerIsBetter;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionSelective::GetGoalType() const
    {
      if( m_Objective.IsDefined())
      {
        return m_Objective->GetGoalType();
      }
      return e_Other;
    }

    //! @brief classify. Obtain a matrix of with N|n for all predicted negatives, P|p for all predicted positives, \0 for all non-predicted values
    //!        case indicates whether the prediction was true (upper) or false (lower)
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return matrix with PpNn\0 values indicating TP,FP,TN,FN,NA
    linal::Matrix< char> ObjectiveFunctionSelective::GetFeaturePredictionClassifications
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      return m_Objective->GetFeaturePredictionClassifications( EXPERIMENTAL, PREDICTED);
    }

    //! @brief set dataset; some objective functions may need to setup internal data structures based on this function
    //! @param DATA monitoring dataset results, non-scaled
    //! @param IDS ids; can be used by the objective function
    void ObjectiveFunctionSelective::SetData
    (
      const FeatureDataSet< float> &DATA,
      const FeatureDataSet< char> &IDS
    )
    {
      m_MappingIds.Reset();

      // get the feature labels so that sequences can be identified
      DataSetSelectColumns sequence_id_selector;
      if( !m_DbIdLabel.IsEmpty())
      {
        const FeatureLabelSet model_feature_labels( *IDS.GetFeatureLabelSet());
        sequence_id_selector =
          DataSetSelectColumns
          (
            model_feature_labels.GetSize(),
            model_feature_labels.GetPropertyIndices
            (
              util::ObjectDataLabel( m_DbIdLabel.GetValue(), m_DbIdLabel.GetArguments())
            )
          );
      }
      else
      {
        // no labels were specified, use them all
        sequence_id_selector =
          DataSetSelectColumns( IDS.GetFeatureSize(), storage::CreateIndexVector( IDS.GetFeatureSize()));
      }

      BCL_Assert( sequence_id_selector.GetOutputFeatureSize(), "Could not find " + m_DbIdLabel.ToString() + " in IDs!");

      const size_t dataset_size( DATA.GetNumberFeatures());
      BCL_Assert( dataset_size == IDS.GetNumberFeatures(), "IDs and Data have different size!");

      {
        std::string this_sequence_id( sequence_id_selector.GetOutputFeatureSize(), ' ');
        for( size_t i( 0); i < dataset_size; ++i)
        {
          sequence_id_selector( IDS( i), &this_sequence_id[ 0]);
          m_MappingIds[ this_sequence_id].PushBack( i);
        }
      }

      m_ExperimentalPruned = FeatureDataSet< float>( m_MappingIds.GetSize(), DATA.GetFeatureSize(), float( 0.0));
      FeatureDataSet< char> pruned_ids( m_MappingIds.GetSize(), IDS.GetFeatureSize(), ' ');
      size_t molecule_index( 0);
      for
      (
        storage::Map< std::string, storage::Vector< size_t> >::const_iterator
          itr( m_MappingIds.Begin()), itr_end( m_MappingIds.End());
          itr != itr_end;
        ++itr, ++molecule_index
      )
      {
        m_ExperimentalPruned.GetRawMatrix().ReplaceRow( molecule_index, DATA( itr->second.FirstElement()));
        pruned_ids.GetRawMatrix().ReplaceRow( molecule_index, IDS( itr->second.FirstElement()));
      }
      m_Objective->SetData( m_ExperimentalPruned, pruned_ids);
    }

    //! @brief evaluate for a given datasets with experimental and predicted values the implemented objective function
    //! @param EXPERIMENTAL feature dataset with experimental values
    //! @param PREDICTED feature dataset with predicted values
    //! @return objective function value based on the given data of experimental and predicted values
    float ObjectiveFunctionSelective::operator()
    (
      const FeatureDataSetInterface< float> &EXPERIMENTAL,
      const FeatureDataSetInterface< float> &PREDICTED
    ) const
    {
      FeatureDataSet< float> predicted( m_MappingIds.GetSize(), EXPERIMENTAL.GetFeatureSize(), float( 0.0));

      size_t molecule_index( 0);
      math::RunningAverage< linal::Vector< float> > minmax;
      for
      (
        storage::Map< std::string, storage::Vector< size_t> >::const_iterator
          itr( m_MappingIds.Begin()), itr_end( m_MappingIds.End());
          itr != itr_end;
        ++itr, ++molecule_index
      )
      {
        minmax.Reset();
        const storage::Vector< size_t> &values( itr->second);
        for
        (
          storage::Vector< size_t>::const_iterator itr_vec( values.Begin()), itr_vec_end( values.End());
          itr_vec != itr_vec_end;
          ++itr_vec
        )
        {
          minmax += PREDICTED( *itr_vec);
        }
        predicted.GetRawMatrix().ReplaceRow( molecule_index, minmax.GetAverage());
      }

      return m_Weight * m_Objective->operator ()( m_ExperimentalPruned, predicted);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ObjectiveFunctionSelective::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Filters results; selecting only the maximum value among results with the same ID"
      );

      parameters.AddInitializer
      (
        "weight",
        "amount by which to weight the objective function's output",
        io::Serialization::GetAgent( &m_Weight),
        "1.0"
      );

      parameters.AddInitializer
      (
        "function",
        "core objective function to use",
        io::Serialization::GetAgent( &m_Objective)
      );

      parameters.AddInitializer
      (
        "label",
        "id label. If not specified, uses the full id label from the dataset",
        io::Serialization::GetAgent( &m_DbIdLabel),
        ""
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_objective_function_wrapper.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_interface.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! single instance of that class
    const util::SiPtr< const util::ObjectInterface> ObjectiveFunctionWrapper::s_Instance
    (
      GetObjectInstances().AddInstance( new ObjectiveFunctionWrapper())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    ObjectiveFunctionWrapper::ObjectiveFunctionWrapper() :
      m_Data(),
      m_Specialization()
    {
    }

    //! @brief constructor with parameters
    //! @param SPECIALIZATION specify objective function implementation
    ObjectiveFunctionWrapper::ObjectiveFunctionWrapper
    (
      const util::Implementation< ObjectiveFunctionInterface> &SPECIALIZATION
    ) :
      m_Data(),
      m_Specialization( SPECIALIZATION)
    {
    }

    //! @brief constructor with parameters
    //! @param DATA data set that is used for testing the progress of the model::Interface
    //! @param SPECIALIZATION specify objective function implementation
    ObjectiveFunctionWrapper::ObjectiveFunctionWrapper
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::Implementation< ObjectiveFunctionInterface> &SPECIALIZATION
    ) :
      m_Data( DATA),
      m_Specialization( SPECIALIZATION)
    {
    }

    //! @brief Clone function
    //! @return pointer to new ObjectiveFunctionWrapper
    ObjectiveFunctionWrapper *ObjectiveFunctionWrapper::Clone() const
    {
      return new ObjectiveFunctionWrapper( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ObjectiveFunctionWrapper::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief set data and rescaling for support vector model
    //! @param DATA dataset of interest
    //! @param RESCALE_INPUT, RESCALE_OUTPUT the rescale functions
    void ObjectiveFunctionWrapper::SetData
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_INPUT
    )
    {
      BCL_Assert( DATA.IsDefined(), "Data set undefined!");
      m_Data = DATA;

      if( RESCALE_INPUT.IsDefined())
      {
        m_Data->GetFeatures().Rescale( *RESCALE_INPUT);
      }
      if( m_Specialization.IsDefined())
      {
        if( DATA->GetIdsPtr().IsDefined())
        {
          m_Specialization->SetData( *DATA->GetResultsPtr(), *DATA->GetIdsPtr());
        }
        else
        {
          m_Specialization->SetData( *DATA->GetResultsPtr());
        }
      }
    }

    //! @brief get monitoring data for model::Interface
    const util::ShPtr< descriptor::Dataset> &ObjectiveFunctionWrapper::GetData() const
    {
      return m_Data;
    }

    //! @brief get the overall goal of the objective function
    //! @return the goal of the objective function
    ObjectiveFunctionInterface::Goal ObjectiveFunctionWrapper::GetGoalType() const
    {
      // return the improvement type; regression by default if no objective function is defined
      return m_Specialization.IsDefined() ? m_Specialization->GetGoalType() : ObjectiveFunctionInterface::e_Regression;
    }

    //! @brief determine what sign of the derivative of this objective function indicates improvement
    //! @return the sign of the derivative of this objective function indicates improvement
    opti::ImprovementType ObjectiveFunctionWrapper::GetImprovementType() const
    {
      // return improvement types; undefined if none is available
      return
        m_Specialization.IsDefined()
        ? m_Specialization->GetImprovementType()
        : opti::s_NumberImprovementTypes;
    }

    //! @brief get implementation used by the objective function
    const util::Implementation< ObjectiveFunctionInterface> &
    ObjectiveFunctionWrapper::GetImplementation() const
    {
      return m_Specialization;
    }

    //! @brief get the threshold, for classification type objectives
    //! @return the threshold, for classification type objectives
    float ObjectiveFunctionWrapper::GetThreshold() const
    {
      return m_Specialization->GetThreshold();
    }

    //! @brief get the parity, for rank classification type objectives
    //! @return the parity, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; true means the objective is most interested
    //!       in prediction of values higher than the threshold, false means below threshold
    bool ObjectiveFunctionWrapper::GetRankingParity() const
    {
      return m_Specialization->GetRankingParity();
    }

    //! @brief get the desired hit rate (e.g. fraction of positive predictions)
    //! @return the desired hit rate, for rank classification type objectives
    //! @note this has meaning only for rank-classification objectives; 0.01 means, for example, that only the top 1%
    //!       of values will be considered
    float ObjectiveFunctionWrapper::GetDesiredHitRate() const
    {
      return m_Specialization->GetDesiredHitRate();
    }

    //! @brief test whether one objective function value is better than another
    //! @param NEW_RESULT, OLD_RESULT two objective function results
    //! @return true if NEW_RESULT is better than OLD_RESULT
    bool ObjectiveFunctionWrapper::TestWhetherResultsImproved( const float &NEW_RESULT, const float &OLD_RESULT)
    {
      if( GetImprovementType() == opti::e_LargerEqualIsBetter)
      {
        return NEW_RESULT > OLD_RESULT;
      }

      return NEW_RESULT < OLD_RESULT;
    }

    //! @brief alter an output descaling function based on the objective function
    //! @param RESCALING current rescaling function
    //! @return the optimized rescaling function
    util::ShPtr< RescaleFeatureDataSet> ObjectiveFunctionWrapper::OptimizeRescalingFunction
    (
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE,
      const FeatureDataSet< float> &PREDICTIONS
    ) const
    {
      // this function
      // computes a rescaling function as f(x) = (x-adjusted cutoff)*slope+EXPERIMENTAL_CUTOFF
      // where adjusted cutoff is the predicted value at which FRACTION have been predicted
      // determine slope

      // well defined hit rate, determine rescaling functions for every cutoff
      storage::Vector< math::Range< float> > old_ranges( RESCALE->GetRescaleRanges());

      const size_t dataset_size( PREDICTIONS.GetNumberFeatures());

      // compute the desired number of predicted positives

      // get a matrix reference
      linal::MatrixConstReference< float> predictions( PREDICTIONS.GetMatrix());
      util::ShPtrVector< math::FunctionInterfaceSerializable< float, float> > rescalers( PREDICTIONS.GetFeatureSize());
      for( size_t result( 0), result_size( PREDICTIONS.GetFeatureSize()); result < result_size; ++result)
      {
        math::RunningAverageSD< float> average_sd;
        for( size_t row( 0); row < dataset_size; ++row)
        {
          average_sd += predictions( row, result);
        }

        // get the original range width
        const float original_range_width( old_ranges( result).GetWidth());

        // get the predicted range width
        const float predicted_range_width( 2.0 * math::Absolute( average_sd.GetStandardDeviation()));

        // compute the slope
        float slope( 1.0);
        if( original_range_width && predicted_range_width)
        {
          slope = original_range_width / predicted_range_width;
        }

        // compute the offset
        const float offset( -slope * average_sd.GetAverage());

        // create the linear function to map values in the old range onto the new range
        rescalers( result) = util::CloneToShPtr( math::LinearFunction( slope, offset));
      }

      // create a new rescaler with the updated ranges
      return util::ShPtr< RescaleFeatureDataSet>( new RescaleFeatureDataSet( *RESCALE, rescalers));
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief compute error between predicted values from FEATURE and RESULT
    //! @param MODEL NeuralNetwork to be monitored
    //! @return error between predicted values from FEATURE and RESULT
    float ObjectiveFunctionWrapper::operator()( const util::PtrInterface< Interface> &MODEL) const
    {
      // predict a list of experimental and predicted values and evaluate objective function
      if( m_Specialization.IsDefined() && m_Data.IsDefined() && m_Data->GetResultsPtr().IsDefined())
      {
        return m_Specialization->operator()( *m_Data->GetResultsPtr(), Predict( MODEL));
      }
      // no objective function: just return 0
      return 0.0;
    }

    //! @brief compute feature dataset with predicted values
    //! @param MODEL shptr on model::Interface that will be monitored
    //! @return predicted values from FEATURE and RESULT based on given model
    FeatureDataSet< float> ObjectiveFunctionWrapper::Predict( const util::PtrInterface< Interface> &MODEL) const
    {
      // return list with predicted and experimental values
      return MODEL->PredictWithoutRescaling( m_Data->GetFeatures()).DeScale();
    }

    //! @brief compute feature dataset with predicted values
    //! @param PREDICTIONS predictions for which to evaluate this measure
    //!        PREDICTIONS may be rescaled, as necessary, for this operation, but it will be returned
    //!        with the same scaling it was given during input
    //! @return predicted values from FEATURE and RESULT based on given model
    float ObjectiveFunctionWrapper::Evaluate( FeatureDataSet< float> &PREDICTIONS) const
    {
      // return list with predicted and experimental values
      if( m_Specialization.IsDefined())
      {
        // handle rank-classification tasks, in which scaling does not matter
        if
        (
          GetGoalType() == ObjectiveFunctionInterface::e_RankClassification
          || PREDICTIONS.HasSameScaling( *m_Data->GetResultsPtr())
        )
        {
          return m_Specialization->operator()( *m_Data->GetResultsPtr(), PREDICTIONS);
        }
        // If predictions is scaled, it must be descaled first
        if( PREDICTIONS.IsRescaled())
        {
          util::ShPtr< RescaleFeatureDataSet> rescaling( PREDICTIONS.GetScaling());
          PREDICTIONS.DeScale();
          float result( Evaluate( PREDICTIONS));
          PREDICTIONS.Rescale( *rescaling);
          return result;
        }
        if( m_Data->GetResultsPtr()->IsRescaled())
        {
          util::ShPtr< RescaleFeatureDataSet> rescaling( m_Data->GetResultsPtr()->GetScaling());
          m_Data->GetResults().DeScale();
          float result( Evaluate( PREDICTIONS));
          m_Data->GetResults().Rescale( *rescaling);
          return result;
        }
      }
      return 0.0;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &ObjectiveFunctionWrapper::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_Specialization, ISTREAM);

      // return the stream
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return outputstream which was written to
    std::ostream &ObjectiveFunctionWrapper::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write members
      io::Serialize::Write( m_Specialization, OSTREAM, INDENT);

      // return the stream
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

  } // namespace model

} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_pretrain_neural_network_from_file.h"

// includes from bcl - sorted alphabetically
#include "command/bcl_command_parameter_check_file_existence.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> PretrainNeuralNetworkFromFile::s_Instance
    (
      util::Enumerated< PretrainNeuralNetworkInterface>::AddInstance( new PretrainNeuralNetworkFromFile())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone function
    //! @return pointer to new PretrainNeuralNetworkFromFile
    PretrainNeuralNetworkFromFile *PretrainNeuralNetworkFromFile::Clone() const
    {
      return new PretrainNeuralNetworkFromFile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &PretrainNeuralNetworkFromFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &PretrainNeuralNetworkFromFile::GetAlias() const
    {
      static const std::string s_name( "File");
      return s_name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief the main operation, pretrains a neural network
    //! @param DATA the data for use in pretraining
    //! @param OBJECTIVE ShPtr to the objective function for the network
    util::ShPtr< NeuralNetwork> PretrainNeuralNetworkFromFile::PretrainNetwork
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE
    )
    {
      BCL_Assert
      (
        m_NeuralNetwork->GetNumberInputs() == DATA->GetFeatureSize(),
        "Read in network had the wrong input size!"
      );
      BCL_Assert
      (
        m_NeuralNetwork->GetNumberOutputs() == DATA->GetResultSize(),
        "Read in network had the wrong output size!"
      );
      return m_NeuralNetwork;
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERROR_STREAM the stream to write errors to
    bool PretrainNeuralNetworkFromFile::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERROR_STREAM
    )
    {
      // read in the model
      io::IFStream input_network;
      io::File::MustOpenIFStream( input_network, m_FileName);
      util::ShPtr< Interface> model;
      io::Serialize::Read( model, input_network);

      // ensure that a model was read in
      if( !model.IsDefined())
      {
        ERROR_STREAM << "Network file had an undefined model";
        return false;
      }
      io::File::CloseClearFStream( input_network);

      // ensure that the model is a neural network
      m_NeuralNetwork = util::ShPtr< NeuralNetwork>( model);
      if( !m_NeuralNetwork.IsDefined())
      {
        ERROR_STREAM << "PretrainNeuralNetworkFromFile cannot load a model of type " << model->GetClassIdentifier();
        return false;
      }
      return true;
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer PretrainNeuralNetworkFromFile::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "load a neural network written out as a util::ShPtr<model::Interface>"
      );

      parameters.AddInitializer
      (
        "",
        "file name of an util::ShPtr<model::Interface> to a model::NeuralNetwork",
        io::Serialization::GetAgentWithCheck
        (
          &m_FileName,
          command::ParameterCheckFileExistence()
        )
      );

      return parameters;
    }

  //////////////////////
  // helper functions //
  //////////////////////

  } // namespace model

} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_pretrain_stacked_auto_encoder.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_interface_store_in_file.h"
#include "model/bcl_model_neural_network_selective_backpropagation_default.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    const util::SiPtr< const util::ObjectInterface> PretrainStackedAutoEncoder::s_PretrainInstance
    (
      util::Enumerated< PretrainNeuralNetworkInterface>::AddInstance( new PretrainStackedAutoEncoder())
    );

    //! @brief default constructor
    PretrainStackedAutoEncoder::PretrainStackedAutoEncoder() :
      m_Approximator( true),
      m_TrainingData(),
      m_EncodedFeatures(),
      m_CurrentAutoEncoderModel(),
      m_AutoEncoderArchitecture()
    {
    }

    //! @brief constructor from training data, transfer, rescale, and objective functions
    //! @param TRAINING_DATA data to train the NeuralNetwork on
    //! @param UPDATE_EVERY_NTH_FEATURE how often the weights get updated
    //! @param ARCHITECTURE the # of neurons in each hidden layer of the network
    //! @param TRANSFER_FUNCTION ShPtr to the transfer function between input and output of each neuron
    //! @param OBJECTIVE_FUNCTION ShPtr to objective function
    //! @param WEIGHT_UPDATE_FUNCTION method by which to update the weights
    //! @param ITERATIONS_PER_RMSD_REPORT # iterations per report of the rmsd
    PretrainStackedAutoEncoder::PretrainStackedAutoEncoder
    (
      util::ShPtr< descriptor::Dataset> &TRAINING_DATA
    ) :
      m_Approximator( true),
      m_TrainingData(),
      m_EncodedFeatures(),
      m_CurrentAutoEncoderModel(),
      m_AutoEncoderArchitecture()
    {
      // set and rescale training data set
      SetTrainingData( TRAINING_DATA);
    }

    //! @brief copy constructor
    //! @return a new PretrainStackedAutoEncoder copied from this instance
    PretrainStackedAutoEncoder *PretrainStackedAutoEncoder::Clone() const
    {
      return new PretrainStackedAutoEncoder( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &PretrainStackedAutoEncoder::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &PretrainStackedAutoEncoder::GetAlias() const
    {
      static const std::string s_Name( "AutoEncoder");
      return s_Name;
    }

    //! @brief set training data set for a specific iterate in approximater framework
    //! @param DATA training data set
    void PretrainStackedAutoEncoder::SetTrainingData
    (
      util::ShPtr< descriptor::Dataset> &DATA
    )
    {
      DATA->GetFeatures().DeScale();
      DATA->GetFeatures().Rescale( m_Approximator.GetTransferFunction()->GetDynamicOutputRange(), RescaleFeatureDataSet::e_MinMax);

      // setup dataset for autoencoding, features and results are the same
      m_TrainingData = util::ShPtr< descriptor::Dataset>
      (
        new descriptor::Dataset( DATA->GetFeaturesPtr(), DATA->GetFeaturesPtr())
      );

    } // SetTrainingData

    //! @brief construct a model from the current iterate
    //! @return shptr to the new model interface
    util::ShPtr< Interface> PretrainStackedAutoEncoder::GetCurrentModel() const
    {
      return m_CurrentAutoEncoderModel;
    }

    //! @brief the main operation, pretrains a neural network
    //! @param DATA the data for use in pretraining
    //! @param OBJECTIVE ShPtr to the objective function for the network
    util::ShPtr< NeuralNetwork> PretrainStackedAutoEncoder::PretrainNetwork
    (
      util::ShPtr< descriptor::Dataset> &DATA,
      const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE
    )
    {
      m_AutoEncoderArchitecture = m_Approximator.GetHiddenArchitecture();

      SetTrainingData( DATA);
      while( !m_AutoEncoderArchitecture.IsEmpty())
      {
        TrainNextLayer( OBJECTIVE);
      }

      if( !m_AutoEncoderStoragePath.empty())
      {
        // serialize encoder
        InterfaceStoreInFile model_storage( m_AutoEncoderStoragePath);

        std::string result_label( "Combined(0");
        for( size_t cnt( 1), num_input( m_CurrentAutoEncoderModel->GetNumberOutputs()); cnt < num_input; ++cnt)
        {
          result_label += ",";
          result_label += util::Format()( cnt);
        }
        result_label += ")";

        model_storage.StoreResultDescriptor( util::ObjectDataLabel( result_label));

        model_storage.Store( util::ShPtr< Interface>( m_CurrentAutoEncoderModel), this->GetFeatureCode().GetLabel());
      }

      // train output for approximator
      BCL_MessageStd( "Train output layer for usage as pretrainer");

      // make it compatible with approximator that surrounds this pretrainer
      m_AutoEncoderArchitecture.PushBack( DATA->GetResultSize());
      while( !m_AutoEncoderArchitecture.IsEmpty())
      {
        TrainNextLayer( OBJECTIVE);
      }

      return GetCurrentModel();
    }

    //! @brief conducts the next approximation step and stores the approximation
    void PretrainStackedAutoEncoder::TrainNextLayer( const util::ShPtr< ObjectiveFunctionWrapper> &OBJECTIVE)
    {
      BCL_MessageStd( "Construct autoencoder layer ...");

      // set up AutoEncoder (AE) architecture
      storage::Vector< size_t> ae_architecture;
      ae_architecture.PushBack( m_AutoEncoderArchitecture.FirstElement());

      // remove current layer from considering future ae layers
      m_AutoEncoderArchitecture.RemoveElements( 0, 1);

      util::ShPtr< descriptor::Dataset> encoded_train_data;
      if( m_CurrentAutoEncoderModel.IsDefined())
      {

        encoded_train_data = util::ShPtr< descriptor::Dataset>
        (
          new descriptor::Dataset( m_EncodedFeatures, m_EncodedFeatures)
        );
      }
      else
      {
        BCL_MessageStd( "No encode of training data!");
      }

      // in round 1 take training data, for subsequent rounds take encoded data to train
      util::ShPtr< descriptor::Dataset> encoded_training_data
      (
        encoded_train_data.IsDefined() ? encoded_train_data : m_TrainingData
      );

      // set up NN to train current AE layer
      ApproximatorNeuralNetwork autoencoder( m_Approximator);
      // set AE architecture for current layer
      autoencoder.SetHiddenArchitecture( ae_architecture);

      // train current AE layer
      util::ShPtr< NeuralNetwork> current_ae_layer( autoencoder.PretrainNetwork( encoded_training_data, OBJECTIVE));

      // construct encoder portion
      util::ShPtr< NeuralNetwork> current_ae_encoder( new NeuralNetwork( *current_ae_layer));
      current_ae_encoder->RemoveOutputLayer();

      m_EncodedFeatures = util::ShPtr< FeatureDataSet< float> >
      (
        new FeatureDataSet< float>( current_ae_encoder->PredictWithoutRescaling( *encoded_training_data->GetFeaturesPtr()))
      );

      // construct all layers to resamble current encoder
      if( m_CurrentAutoEncoderModel.IsDefined())
      {
        // encoder
        util::ShPtr< NeuralNetwork> previous_model( new NeuralNetwork( *m_CurrentAutoEncoderModel));
        previous_model->Append( current_ae_encoder);
        current_ae_encoder = previous_model;
      }

      // construct decoder portion
      util::ShPtr< NeuralNetwork> current_ae_decoder( new NeuralNetwork( *current_ae_layer));
      current_ae_decoder->RemoveInputLayer();

      // construct all layers to resamble current encoder
      if( m_CurrentAutoDecoderModel.IsDefined())
      {
        // decoder
        current_ae_decoder->Append( m_CurrentAutoDecoderModel.HardCopy());
      }

      // update cuurent encoder and decoder
      m_CurrentAutoEncoderModel = current_ae_encoder;
      m_CurrentAutoDecoderModel = current_ae_decoder;

      BCL_MessageStd( "Current Encoder: " + util::Format()( current_ae_encoder->GetArchitecture()));
      BCL_MessageStd( "Current Decoder: " + util::Format()( current_ae_decoder->GetArchitecture()));

      // test the ability to auto encode of the current model

//      util::ShPtr< NeuralNetwork> ae_model( new NeuralNetwork( *m_CurrentAutoEncoderModel));
//      ae_model->Append( m_CurrentAutoDecoderModel);
//
//      BCL_Debug( ae_model->GetArchitecture());
//
//      util::ShPtr< FeatureDataSet< float> > test_ae
//      (
//        new FeatureDataSet< float>( ae_model->PredictWithoutRescaling( *( m_TrainingData->GetFeaturesPtr())))
//      );
//
//      BCL_MessageStd( util::Format()( m_TrainingData->GetFeaturesPtr()->GetRawMatrix().GetRow( 0)).substr(0,300));
//      BCL_MessageStd( util::Format()( test_ae->GetRawMatrix().GetRow( 0)).substr(0,300));

      BCL_MessageStd( "Construct autoencoder layer ... done");
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer PretrainStackedAutoEncoder::GetSerializer() const
    {
      io::Serializer parameters( m_Approximator.GetSerializer());
      parameters.SetClassDescription
      (
        "trains a stacked auto encoder based on neural networks (see http://en.wikipedia.org/wiki/Artificial_neural_network)"
      );

      parameters.AddInitializer
      (
        "model storage path",
        "output path that specifies the directory name where the autoencoder model is stored",
        io::Serialization::GetAgent( &m_AutoEncoderStoragePath),
        ""
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_rescale_feature_data_set.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "math/bcl_math_running_average_sd.h"
#include "model/bcl_model_feature_data_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> RescaleFeatureDataSet::s_Instance
    (
      GetObjectInstances().AddInstance( new RescaleFeatureDataSet())
    );

    //! @brief Type as string
    //! @param TYPE the type
    //! @return the string for the kernel
    const std::string &RescaleFeatureDataSet::GetTypeName( const Type &TYPE)
    {
      static const std::string s_names[] =
      {
        "None",
        "MinMax",
        "AveStd",
        GetStaticClassName< Type>()
      };
      return s_names[ TYPE];
    }

    //! global static function that allows altering the default null value; note that undefined is interpreted as the
    //! middle of the output range
    float &RescaleFeatureDataSet::GetDefaultValueForEmptyRangedColumns()
    {
      static float s_default_value( util::GetUndefined< float>());
      return s_default_value;
    }

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    RescaleFeatureDataSet::RescaleFeatureDataSet() :
      m_RescaleRanges(),
      m_Range()
    {
    }

    //! @brief construct from feature result data set
    //! @param DATA the feature data set from which to obtain the ranges
    //! @param RANGE features will be rescaled to this range
    //! @param TYPE type of rescaling to perform
    RescaleFeatureDataSet::RescaleFeatureDataSet
    (
      const FeatureDataSetInterface< float> &DATA,
      const math::Range< float> &RANGE,
      const Type &TYPE
    ) :
      m_RescaleRanges(),
      m_Range( RANGE)
    {
      // get the ranges from the underlying matrix
      if( TYPE == e_AveStd)
      {
        GetInputRangesFromMatrixAveStd( DATA.GetMatrix());
      }
      else if( TYPE == e_MinMax)
      {
        GetInputRangesFromMatrix( DATA.GetMatrix());
      }
      else
      {
        // non-rescaler
        m_RescaleFunction.Resize( DATA.GetFeatureSize(), math::LinearFunction( 1.0, 0.0));
        m_DescaleFunction.Resize( DATA.GetFeatureSize(), math::LinearFunction( 1.0, 0.0));
      }
    }

    //! @brief construct from a matrix const interface
    //! @param DATA the matrix from which to obtain the ranges
    //! @param RANGE features will be rescaled to this range
    //! @param TYPE type of rescaling to perform
    RescaleFeatureDataSet::RescaleFeatureDataSet
    (
      const linal::MatrixConstInterface< float> &DATA,
      const math::Range< float> &RANGE,
      const Type &TYPE
    ) :
      m_RescaleRanges(),
      m_Range( RANGE)
    {
      if( TYPE == e_AveStd)
      {
        GetInputRangesFromMatrixAveStd( DATA);
      }
      else if( TYPE == e_MinMax)
      {
        GetInputRangesFromMatrix( DATA);
      }
      else
      {
        m_RescaleFunction.Resize( DATA.GetNumberCols(), math::LinearFunction( 1.0, 0.0));
        m_DescaleFunction.Resize( DATA.GetNumberCols(), math::LinearFunction( 1.0, 0.0));
      }
    }

    //! @brief construct from from ranges to to range
    //! @param FROM_RANGES the original ranges
    //! @param TO_RANGE the rescaled range
    RescaleFeatureDataSet::RescaleFeatureDataSet
    (
      const RescaleFeatureDataSet &RESCALING,
      const util::ShPtrVector< math::FunctionInterfaceSerializable< double, double> > &MODIFIERS
    ) :
      m_RescaleRanges( RESCALING.m_RescaleRanges),
      m_Range( RESCALING.m_Range)
    {
      BCL_Assert( MODIFIERS.GetSize() == m_RescaleRanges.GetSize(), "Require as many modifiers as rescaling ranges");
      for( size_t col( 0), n_cols( MODIFIERS.GetSize()); col < n_cols; ++col)
      {
        m_RescaleRanges( col)
          = math::Range< float>
            (
              ( *MODIFIERS( col))( m_RescaleRanges( col).GetMin()),
              ( *MODIFIERS( col))( m_RescaleRanges( col).GetMax())
            );
      }
      ConstructScalingFunctions();
    }

    //! @brief Clone function
    //! @return pointer to new RescaleFeatureDataSet
    RescaleFeatureDataSet *RescaleFeatureDataSet::Clone() const
    {
      return new RescaleFeatureDataSet( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RescaleFeatureDataSet::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief rescale a value for a particular column using the appropriate rescale function
    //! @param COLUMN column that the value logically belongs to
    //! @param VALUE value to rescale
    //! @return the rescaled value
    float RescaleFeatureDataSet::RescaleValue( const size_t &COLUMN, const float &VALUE) const
    {
      return m_RescaleFunction( COLUMN)( VALUE);
    }

    //! @brief descale a value for a particular column using the appropriate descale function
    //! @param COLUMN column that the value logically belongs to
    //! @param VALUE value to descale
    //! @return the descaled value
    float RescaleFeatureDataSet::DescaleValue( const size_t &COLUMN, const float &VALUE) const
    {
      return m_DescaleFunction( COLUMN)( VALUE);
    }

    //! @brief scales back to pre-scaled values
    //! @param FEATURE the feature data set interface to scale back to original values
    //! @return FeatureDataSet of de-scaled values
    FeatureDataSet< float> RescaleFeatureDataSet::DeScale( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // create a matrix copy
      linal::Matrix< float> mat( FEATURE.GetMatrix());

      // descale it
      DeScaleMatrix( mat);

      // create a new data set out of it
      return FeatureDataSet< float>( mat);
    }

    //! @brief scales back to pre-rescaled values
    //! @param MATRIX the matrix to descale
    void RescaleFeatureDataSet::DeScaleMatrix( linal::MatrixInterface< float> &MATRIX) const
    {
      // check for identity rescaling
      if( m_DescaleFunction.IsEmpty() || m_RescaleRanges.IsEmpty())
      {
        return;
      }

      // make sure the matrix has the right number of columns, if it is not empty
      BCL_Assert
      (
        MATRIX.GetNumberRows() == size_t( 0) || MATRIX.GetNumberCols() == m_RescaleRanges.GetSize(),
        "DeScaleMatrix given matrix with incorrect # columns!"
      );

      // get matrix and dimensionality info
      const size_t number_features( MATRIX.GetNumberCols());
      const size_t number_data_pts( MATRIX.GetNumberRows());

      // scale back to original values
      for( size_t row_id( 0); row_id < number_data_pts; ++row_id)
      {
        storage::Vector< math::LinearFunction>::const_iterator itr_func( m_DescaleFunction.Begin());
        for( float *itr( MATRIX[ row_id]), *itr_end( itr + number_features); itr != itr_end; ++itr, ++itr_func)
        {
          *itr = ( *itr_func)( *itr);
        }
      }
    }

    //! @brief scale a MatrixInterface
    //! @param MATRIX the matrix to rescale
    void RescaleFeatureDataSet::RescaleMatrix( linal::MatrixInterface< float> &MATRIX) const
    {
      // check for identity rescaling
      if( m_RescaleFunction.IsEmpty())
      {
        return;
      }
      BCL_Assert
      (
        m_RescaleFunction.GetSize() == MATRIX.GetNumberCols(),
        "Wrong number of rescaling columns! " + util::Format()( m_RescaleFunction.GetSize())
        + " vs " + util::Format()( MATRIX.GetNumberCols())
      );

      // get matrix and dimensionality info
      const size_t number_features( MATRIX.GetNumberCols());
      const size_t number_data_pts( MATRIX.GetNumberRows());

      // scale back to original values
      for( size_t row_id( 0); row_id < number_data_pts; ++row_id)
      {
        storage::Vector< math::LinearFunction>::const_iterator itr_func( m_RescaleFunction.Begin());
        for( float *itr( MATRIX[ row_id]), *itr_end( itr + number_features); itr != itr_end; ++itr, ++itr_func)
        {
          *itr = ( *itr_func)( *itr);
        }
      }
    }

  ///////////////
  // operators //
  ///////////////

    //! @brief scale a FeatureDataSet
    //! @param FEATURE the FeatureDataSetInterface to rescale
    //! @return rescaled FeatureDataSet to one range
    FeatureDataSet< float> RescaleFeatureDataSet::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // get matrix and dimensionality info
      linal::Matrix< float> mat( FEATURE.GetMatrix());
      RescaleMatrix( mat);

      // return scaled feature data set
      return FeatureDataSet< float>( mat);
    }

    //! @brief test for inequality
    //! @param OTHER other rescaling function
    //! @return true if the rescaling functions are inequal
    bool RescaleFeatureDataSet::operator !=( const RescaleFeatureDataSet &OTHER) const
    {
      return !( m_Range == OTHER.m_Range) || !( m_RescaleRanges == OTHER.m_RescaleRanges);
    }

    //! @brief test for equality
    //! @param OTHER other rescaling function
    //! @return true if the rescaling functions are equal
    bool RescaleFeatureDataSet::operator ==( const RescaleFeatureDataSet &OTHER) const
    {
      return m_Range == OTHER.m_Range && m_RescaleRanges == OTHER.m_RescaleRanges;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read from std::istream
    //! @param ISTREAM input stream
    //! @return istream which was read from
    std::istream &RescaleFeatureDataSet::Read( std::istream &ISTREAM)
    {
      // read members
      io::Serialize::Read( m_RescaleRanges, ISTREAM);
      io::Serialize::Read( m_Range, ISTREAM);
      ConstructScalingFunctions();

      // end
      return ISTREAM;
    }

    //! @brief write to std::ostream
    //! @param OSTREAM output stream to write to
    //! @param INDENT number of indentations
    //! @return output stream which was written to
    std::ostream &RescaleFeatureDataSet::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write
      io::Serialize::Write( m_RescaleRanges, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Range, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

    //! @brief determine m_RescaleRanges from the given input matrix
    //! @param DATA the matrix from which to obtain the ranges
    void RescaleFeatureDataSet::GetInputRangesFromMatrix( const linal::MatrixConstInterface< float> &DATA)
    {
      m_RescaleRanges.Reset();

      // pre-allocating vector memory
      const size_t number_data_pts( DATA.GetNumberRows());
      m_RescaleRanges.AllocateMemory( DATA.GetNumberCols());

      // find ranges of each column
      float tmp( 0);
      for( size_t col( 0), number_features( DATA.GetNumberCols()); col < number_features; ++col)
      {
        // set initial min and max
        float min( std::numeric_limits< float>::infinity());
        float max( -std::numeric_limits< float>::infinity());

        // iterate through all values in that feature col
        // no need to worry about nans here, b/c they will always fail both the < and > comparison
        for( size_t row( 0); row < number_data_pts; ++row)
        {
          // set as min or max if qualifies
          tmp = DATA( row, col);
          if( util::IsDefined( tmp))
          {
            if( tmp < min)
            {
              min = tmp;
            }
            else if( tmp > max)
            {
              max = tmp;
            }
          }
        }
        // add to range vector
        if( min < max)
        {
          m_RescaleRanges.PushBack( math::Range< float>( min, max));
        }
        else
        {
          m_RescaleRanges.PushBack( math::Range< float>( 0.0, 0.0));
        }
      }
      ConstructScalingFunctions();
    }

    //! @brief determine m_RescaleRanges from the given input matrix
    //! @param DATA the matrix from which to obtain the ranges
    void RescaleFeatureDataSet::GetInputRangesFromMatrixAveStd( const linal::MatrixConstInterface< float> &DATA)
    {
      m_RescaleRanges.Reset();

      // pre-allocating vector memory
      m_RescaleRanges.Resize( DATA.GetNumberCols());

      math::RunningAverageSD< linal::Vector< float> > ave_std;

      // compute averages and std for whole dataset
      for( size_t row( 0), n_rows( DATA.GetNumberRows()); row < n_rows; ++row)
      {
        ave_std += DATA.GetRow( row);
      }

      // find ranges of each column
      const linal::Vector< float> &ave( ave_std.GetAverage());
      const linal::Vector< float> &std( ave_std.GetStandardDeviation());

      for( size_t col( 0), number_features( DATA.GetNumberCols()); col < number_features; ++col)
      {
        // THIS LINE OF CODE IS CRITICAL; otherwise floating point numbers smaller than float min may be written out,
        // but they cannot be read back in; at least on gcc 4.7.  This is likely a compiler/library bug
        const float average( math::Absolute( ave( col)) < std::numeric_limits< float>::min() ? 0.0 : ave( col));

        // determine whether the column has any variance
        if( !util::IsDefined( std( col)))
        {
          m_RescaleRanges( col) = math::Range< float>( average, average);
        }
        else
        {
          m_RescaleRanges( col)
            = math::Range< float>
              (
                average - float( 2.0) * std( col),
                average + float( 2.0) * std( col)
              );
        }
      }
      ConstructScalingFunctions();
    }

    //! @brief determine m_RescaleRanges from the given input matrix
    //! @param DATA the matrix from which to obtain the ranges
    //! @param FRACTION (0,1) the minimum fraction of each column's values that should be covered by the chosen range
    void RescaleFeatureDataSet::GetInputRangesFromMatrixRange
    (
      const linal::MatrixConstInterface< float> &DATA,
      const float &FRACTION
    )
    {
      m_RescaleRanges.Reset();

      // pre-allocating vector memory
      m_RescaleRanges.Resize( DATA.GetNumberCols());

      // create a temporary vector for holding each column
      linal::Vector< float> column_storage( DATA.GetNumberRows());

      // determine the sorted feature range desired to be between -1 and 1 for each column
      const size_t sorted_feature_range_min( DATA.GetNumberRows() * ( 1.0 - FRACTION) / 2.0);
      const size_t sorted_feature_range_max( ( DATA.GetNumberRows() - 1) * FRACTION / 2.0);

      // for each column
      for( size_t col( 0), number_features( DATA.GetNumberCols()); col < number_features; ++col)
      {
        // load all data from the column into the vector
        for( size_t row( 0), n_rows( DATA.GetNumberRows()); row < n_rows; ++row)
        {
          column_storage( row) = DATA( row, col);
        }

        // use the nth-element algorithm to discover the actual value range that would cover 80% of the data
        linal::Vector< float>::iterator min_range_itr( column_storage.Begin() + sorted_feature_range_min);
        std::nth_element( column_storage.Begin(), min_range_itr, column_storage.End());
        float min_range_value( *min_range_itr);
        linal::Vector< float>::iterator max_range_itr( column_storage.Begin() + sorted_feature_range_max);
        std::nth_element( min_range_itr, max_range_itr, column_storage.End());
        float max_range_value( *max_range_itr);
        if( min_range_value == max_range_value)
        {
          // probably a binary or nearly binary variable; take the end values instead
          linal::Vector< float>::iterator itr_min( column_storage.Begin()), itr_max( column_storage.End() - 1);
          std::nth_element( column_storage.Begin(), itr_min, min_range_itr);
          std::nth_element( max_range_itr, itr_max, column_storage.End());
          min_range_value = *itr_min;
          max_range_value = *itr_max;
          BCL_MessageCrt( "Reverting to default scaling on descriptor: " + util::Format()( col));
        }

        m_RescaleRanges( col) = math::Range< float>( min_range_value, max_range_value);
      }

      ConstructScalingFunctions();
    }

    //! @brief construct rescaling and descaling functions
    void RescaleFeatureDataSet::ConstructScalingFunctions()
    {
      const float midpoint
      (
        util::IsDefined( GetDefaultValueForEmptyRangedColumns())
        ? GetDefaultValueForEmptyRangedColumns()
        : m_Range.GetMiddle()
      );
      const size_t number_columns( m_RescaleRanges.GetSize());
      m_RescaleFunction.Resize( number_columns);
      m_DescaleFunction.Resize( number_columns);
      for( size_t column( 0); column < number_columns; ++column)
      {
        // check range, if ~0, set all values to midpoint
        if( m_RescaleRanges( column).GetWidth() < std::numeric_limits< float>::epsilon())
        {
          m_RescaleFunction( column) = math::LinearFunction( 0.0, midpoint);
          m_DescaleFunction( column) = math::LinearFunction( 0.0, m_RescaleRanges( column).GetMiddle());
        }
        else
        {
          m_RescaleFunction( column) = math::LinearFunction( m_RescaleRanges( column), m_Range);
          m_DescaleFunction( column) = math::LinearFunction( m_Range, m_RescaleRanges( column));
        }
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_restricted_boltzmann_machine_layer.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_matrix_reference.h"
#include "math/bcl_math_running_average.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range for neural network transfer functions
    const math::Range< float> RestrictedBoltzmannMachineLayer::s_DefaultInputRange( 0, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RestrictedBoltzmannMachineLayer::s_Instance
    (
      GetObjectInstances().AddInstance( new RestrictedBoltzmannMachineLayer())
    );

    //! @brief Type as string
    //! @param TYPE the type
    //! @return the string for the kernel
    const std::string &RestrictedBoltzmannMachineLayer::GetTypeName( const Type &TYPE)
    {
      static const std::string s_names[] =
      {
        "StochasticSigmoid",
        "Binomial",
        GetStaticClassName< Type>()
      };
      return s_names[ TYPE];
    }

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    RestrictedBoltzmannMachineLayer::RestrictedBoltzmannMachineLayer() :
      m_NumberVisibleNodes( 0),
      m_NumberHiddenNodes( 0),
      m_NetworkType( e_StochasticSigmoid)
    {
    }

    //! @brief construct an untrained network
    //! @param NUMBER_VISIBLE number of visible neurons
    //! @param NUMBER_HIDDEN number of hidden neurons
    //! @param NETWORK_TYPE type of network in terms of transfer functions
    RestrictedBoltzmannMachineLayer::RestrictedBoltzmannMachineLayer
    (
      const size_t &NUMBER_VISIBLE,
      const size_t &NUMBER_HIDDEN,
      const Type   &NETWORK_TYPE
    ) :
      m_NumberVisibleNodes( 0),
      m_NumberHiddenNodes( 0),
      m_NetworkType( NETWORK_TYPE)
    {
      SetArchitecture( NUMBER_VISIBLE, NUMBER_HIDDEN);
    }

    //! @brief construct from all necessary parameters
    //! @param BIAS_VISIBLE the bias for each neuron of the visible layer
    //! @param BIAS_HIDDEN the bias for each neuron of the hidden layer
    //! @param WEIGHT the weight for each neuron of a hidden layer
    //! @param TRANSFER_FUNCTION_HIDDEN transfer function for the hidden layer neurons
    //! @param TRANSFER_FUNCTION_VISIBLE transfer function for the hidden layer neurons
    //! @param HIDDEN_NOISE_VARIANCE variance of the noise to add to the hidden layer when propagating to the visible
    RestrictedBoltzmannMachineLayer::RestrictedBoltzmannMachineLayer
    (
      const linal::Vector< float> &BIAS_VISIBLE,
      const linal::Vector< float> &BIAS_HIDDEN,
      const linal::Matrix< float> &WEIGHT,
      const Type                  &NETWORK_TYPE
    ) :
      m_BiasHidden( BIAS_HIDDEN),
      m_BiasVisible( BIAS_VISIBLE),
      m_Weight( WEIGHT),
      m_NumberVisibleNodes( BIAS_VISIBLE.GetSize()),
      m_NumberHiddenNodes( BIAS_HIDDEN.GetSize()),
      m_NetworkType( NETWORK_TYPE)
    {
      m_SlopeHidden = linal::Vector< float>( m_NumberHiddenNodes, float( 1.0));
      m_SlopeVisible = linal::Vector< float>( m_NumberVisibleNodes, float( 1.0));
      BCL_Assert
      (
        m_NumberHiddenNodes == m_Weight.GetNumberCols(),
        "Incorrectly sized weight matrix or bias vector for hidden neurons"
      );
      BCL_Assert
      (
        m_NumberVisibleNodes == m_Weight.GetNumberRows(),
        "Incorrectly sized weight matrix or bias vector for hidden neurons"
      );
    }

    //! copy constructor
    RestrictedBoltzmannMachineLayer *RestrictedBoltzmannMachineLayer::Clone() const
    {
      return new RestrictedBoltzmannMachineLayer( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RestrictedBoltzmannMachineLayer::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief set the architecture of this RBM
    //! @param NUMBER_INPUTS number of input/visible neurons
    //! @param NUMBER_OUTPUTS number of output/hidden neurons
    void RestrictedBoltzmannMachineLayer::SetArchitecture( const size_t &NUMBER_INPUTS, const size_t &NUMBER_OUTPUTS)
    {
      if( m_NumberHiddenNodes == NUMBER_OUTPUTS || m_NumberVisibleNodes == NUMBER_INPUTS)
      {
        return;
      }
      m_NumberHiddenNodes = NUMBER_OUTPUTS;
      m_NumberVisibleNodes = NUMBER_INPUTS;
      m_BiasHidden = linal::Vector< float>( m_NumberHiddenNodes, float( 0.0));
      m_BiasVisible = linal::Vector< float>( m_NumberVisibleNodes, float( 0.0));
      m_SlopeHidden = linal::Vector< float>( m_NumberHiddenNodes, float( 1.0));
      m_SlopeVisible = linal::Vector< float>( m_NumberVisibleNodes, float( 1.0));
      m_Weight = linal::Matrix< float>( m_NumberVisibleNodes, m_NumberHiddenNodes, float( 0.0));
      for( float *itr( m_Weight.Begin()), *itr_end( m_Weight.End()); itr != itr_end; ++itr)
      {
        *itr = random::GetGlobalRandom().RandomGaussian( 0.0, 0.1);
      }
    }

    //! @brief stochastic activation/ sampling function for continuous distributions
    //! @see @link http://www.ee.nthu.edu.tw/~hchen/pubs/iee2003.pdf @endlink
    //! @param X the activation variable
    //! @param A the noise sensitivity parameter
    float StochasticActivation( const float &X, const float &A)
    {
      // Note: This function differs slightly from that given in http://www.ee.nthu.edu.tw/~hchen/pubs/iee2003.pdf
      // (in particular, the A also multiplies X in that paper), however, this version is both more stable and
      // mathematically justified, since in the paper's equation, A is effectively modifying all the weights and biases
      // by a multiplicative factor, which is never accounted for in the update equations. In practice, all we need is a
      // noise tuning parameter that prevents the RBM from over-fitting
      return 1.0 / ( 1.0 + exp( -random::GetGlobalRandom().RandomGaussian( X, 0.25 * A)));
    }

    //! @brief activate a hidden or visible set of neurons
    //! @param NEURONS the neurons to update
    //! @param SLOPES activation slopes for the neurons
    void Sample
    (
      linal::VectorInterface< float> &NEURONS,
      const linal::VectorConstInterface< float> &NEURON_ACTIVITY
    )
    {
      for( size_t i( 0), number_neurons( NEURONS.GetSize()); i < number_neurons; ++i)
      {
        NEURONS( i) = random::GetGlobalRandom().Double() < NEURON_ACTIVITY( i);
      }
    }

    //! @brief A simple sigmoid function
    //! @param X the activation variable
    float Sigmoid( const float &X)
    {
      return 1.0 / ( 1.0 + exp( -X));
    }

    //! @brief activate a hidden or visible set of neurons
    //! @param NEURONS the neurons to update
    //! @param SLOPES activation slopes for the neurons
    //! @param NETWORK_TYPE actual type of the network
    //! @param STOCHASTIC whether to apply stochastic weighting onto the hidden layer
    void ActivateAndSample
    (
      linal::VectorInterface< float> &NEURONS,
      const linal::Vector< float> &SLOPES,
      const RestrictedBoltzmannMachineLayer::Type TYPE,
      const bool STOCHASTIC
    )
    {
      const size_t number_neurons( NEURONS.GetSize());
      if( STOCHASTIC && RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
      {
        for( size_t i( 0); i < number_neurons; ++i)
        {
          NEURONS( i) = StochasticActivation( NEURONS( i), SLOPES( i));
        }
      }
      else
      {
        for( size_t i( 0); i < number_neurons; ++i)
        {
          NEURONS( i) = Sigmoid( NEURONS( i));
        }
      }
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief perform a complete gibbs sampling step
    //! @param INPUT visible input
    //! @param HIDDEN ref to vector that will hold hidden neuron activations (not sampled) at completion
    //! @param INPUT_RECONSTRUCTED ref to vector to store reconstructed input layer
    //! @param HIDDEN_RECONSTRUCTED ref to vector to store reconstructed hidden layer
    //! @param STOCHASTIC_STEP_COUNT Number of propagations that should incorporate randomness (max = 3)
    //! @note STOCHASTIC_STEP_COUNT, 0 gives fast convergence but poor generalizability, 3 gives the most generality
    //!       but is very slow for binomial networks.
    //!       Hinton uses 1 for binomial networks to speed convergence; stochastic sigmoid should usually use 3,
    //!       Truncated exponential requires slow learning rate and/or setting STOCHASTIC_STEP_COUNT to 0
    //! @param HIDDEN_SAMPLE vector to hold sampling from hidden layer, used to avoid temporary memory allocation
    //! @param VISIBLE_SAMPLE vector to hold sampling from input layer, used to avoid temporary memory allocation
    void RestrictedBoltzmannMachineLayer::GibbsSample
    (
      const linal::VectorConstInterface< float> &INPUT,
      linal::VectorInterface< float> &HIDDEN,
      linal::VectorInterface< float> &INPUT_RECONSTRUCTED,
      linal::VectorInterface< float> &HIDDEN_RECONSTRUCTED,
      const size_t &STOCHASTIC_STEP_COUNT,
      linal::VectorInterface< float> &HIDDEN_SAMPLE,
      linal::VectorInterface< float> &VISIBLE_SAMPLE
    ) const
    {
      if( !STOCHASTIC_STEP_COUNT)
      {
        // simple non-stochastic case
        ForwardPropagate( INPUT, HIDDEN, false);
        BackPropagate( INPUT_RECONSTRUCTED, HIDDEN, false);
        ForwardPropagate( INPUT_RECONSTRUCTED, HIDDEN_RECONSTRUCTED, false);
      }
      else if( m_NetworkType == e_StochasticSigmoid)
      {
        // stochastic sigmoid does not use the sampling vectors since sampling and activation are performed in a single
        // step
        ForwardPropagate( INPUT, HIDDEN, true);
        BackPropagate( INPUT_RECONSTRUCTED, HIDDEN, STOCHASTIC_STEP_COUNT > 1);
        ForwardPropagate( INPUT_RECONSTRUCTED, HIDDEN_RECONSTRUCTED, STOCHASTIC_STEP_COUNT > 2);
      }
      else
      {
        // binomial activation; sampling must occur separately from activation to compute the proper derivatives
        ForwardPropagate( INPUT, HIDDEN, false);
        Sample( HIDDEN_SAMPLE, HIDDEN);
        BackPropagate( INPUT_RECONSTRUCTED, HIDDEN_SAMPLE, false);
        if( STOCHASTIC_STEP_COUNT > 1)
        {
          Sample( VISIBLE_SAMPLE, INPUT_RECONSTRUCTED);
          ForwardPropagate( VISIBLE_SAMPLE, HIDDEN_RECONSTRUCTED, false);
        }
        else
        {
          ForwardPropagate( INPUT_RECONSTRUCTED, HIDDEN_RECONSTRUCTED, false);
        }
      }
    }

    //! @brief Forward propagate a given feature from the input to the hidden layer
    //! @param INPUT input feature of interest
    //! @param HIDDEN reference to vector that will be overwritten by the hidden layer
    //! @param STOCHASTIC whether to apply stochastic weighting onto the hidden layer
    void RestrictedBoltzmannMachineLayer::ForwardPropagate
    (
      const linal::VectorConstInterface< float> &INPUT,
      linal::VectorInterface< float> &HIDDEN,
      const bool STOCHASTIC
    ) const
    {
      // Unrolled version of
      // HIDDEN = m_Weight * INPUT + m_BiasHidden
      HIDDEN = m_BiasHidden;
      const float *itr_weight( m_Weight.Begin()), *input( INPUT.Begin()), *itr_hidden_end( HIDDEN.End());
      for( const float *itr_input_end( INPUT.End()); input != itr_input_end; ++input)
      {
        const float input_val( *input);
        for( float *itr_hidden( HIDDEN.Begin()); itr_hidden != itr_hidden_end; ++itr_hidden, ++itr_weight)
        {
          *itr_hidden += *itr_weight * input_val;
        }
      }
      ActivateAndSample( HIDDEN, m_SlopeHidden, m_NetworkType, STOCHASTIC);
    }

    //! @brief Back propagate a given feature from the hidden to the input layer
    //! @param INPUT input feature of interest
    //! @param HIDDEN hidden vector
    //! @param STOCHASTIC whether to apply stochastic weighting onto the input layer
    void RestrictedBoltzmannMachineLayer::BackPropagate
    (
      linal::VectorInterface< float> &INPUT,
      const linal::VectorInterface< float> &HIDDEN,
      const bool STOCHASTIC
    ) const
    {
      INPUT = m_BiasVisible;

      const float *itr_hidden_end( HIDDEN.End());
      const float *itr_weight( m_Weight.Begin());

      for( float *itr_input( INPUT.Begin()), *itr_end( INPUT.End()); itr_input != itr_end; ++itr_input)
      {
        float sum( 0);
        for( const float *itr_hidden( HIDDEN.Begin()); itr_hidden != itr_hidden_end; ++itr_hidden, ++itr_weight)
        {
          sum += *itr_hidden * *itr_weight;
        }
        *itr_input += sum;
      }
      ActivateAndSample( INPUT, m_SlopeVisible, m_NetworkType, STOCHASTIC);
    }

    //! @brief Invert the boltzmann machine (swaps visible with hidden neurons)
    void RestrictedBoltzmannMachineLayer::Invert()
    {
      std::swap( m_SlopeHidden, m_SlopeVisible);
      std::swap( m_BiasHidden, m_BiasVisible);
      std::swap( m_NumberVisibleNodes, m_NumberHiddenNodes);
      m_Weight = m_Weight.Transposed();
    }

    //! @brief compute the reconstruction error RMSD across a dataset
    float RestrictedBoltzmannMachineLayer::ComputeReconstructionError( const FeatureDataSetInterface< float> &DATA) const
    {
      math::RunningAverage< float> ave_deviation;
      // initialize vectors to store the reconstructed values at the visible layer
      linal::Vector< float> visible_reconstructed( m_NumberVisibleNodes), hidden( m_NumberHiddenNodes);
      const float *const itr_vis_recon_end( visible_reconstructed.End());
      for( size_t feature_id( 0), dataset_size( DATA.GetNumberFeatures()); feature_id < dataset_size; ++feature_id)
      {
        // get a reference to the actual feature
        const linal::VectorConstInterface< float> &feature( DATA( feature_id));

        // Determine non-stochastic reconstruction error
        ForwardPropagate( feature, hidden, false);
        BackPropagate( visible_reconstructed, hidden, false);

        // Update gradients
        // Update Visible bias gradient with FEATURE - visible_reconstructed
        for
        (
          const float *itr_vis_recon( visible_reconstructed.Begin()), *itr_vis( feature.Begin());
          itr_vis_recon != itr_vis_recon_end;
          ++itr_vis, ++itr_vis_recon
        )
        {
          ave_deviation += math::Sqr( *itr_vis - *itr_vis_recon);
        }
      }
      return math::Sqrt( ave_deviation.GetAverage());
    }

    //! @brief compute the reconstruction error RMSD across a single data point
    float RestrictedBoltzmannMachineLayer::ComputeReconstructionError
    (
      const linal::VectorConstInterface< float> &INPUT,
      linal::VectorInterface< float> &HIDDEN,
      linal::VectorInterface< float> &INPUT_RECONSTRUCTED
    ) const
    {
      // Determine non-stochastic reconstruction error
      ForwardPropagate( INPUT, HIDDEN, false);
      BackPropagate( INPUT_RECONSTRUCTED, HIDDEN, false);

      float deviation( 0);
      for
      (
        const float *itr_vis_recon( INPUT_RECONSTRUCTED.Begin()), *itr_vis( INPUT.Begin()), *itr_vis_end( INPUT.End());
        itr_vis != itr_vis_end;
        ++itr_vis, ++itr_vis_recon
      )
      {
        deviation += math::Sqr( *itr_vis - *itr_vis_recon);
      }
      deviation /= float( INPUT.GetSize());
      return math::Sqrt( deviation);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read RestrictedBoltzmannMachineLayer from std::istream
    std::istream &RestrictedBoltzmannMachineLayer::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( m_SlopeHidden, ISTREAM);
      io::Serialize::Read( m_SlopeVisible, ISTREAM);
      io::Serialize::Read( m_BiasHidden, ISTREAM);
      io::Serialize::Read( m_BiasVisible, ISTREAM);
      io::Serialize::Read( m_Weight, ISTREAM);
      io::Serialize::Read( m_NetworkType, ISTREAM);
      m_NumberVisibleNodes = m_BiasVisible.GetSize();
      m_NumberHiddenNodes = m_BiasHidden.GetSize();
      return ISTREAM;
    }

    //! write RestrictedBoltzmannMachineLayer into std::ostream
    std::ostream &RestrictedBoltzmannMachineLayer::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( m_SlopeHidden, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SlopeVisible, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasHidden, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasVisible, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Weight, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NetworkType, OSTREAM, INDENT);
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_balanced.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetBalanced::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetBalanced())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetBalanced *RetrieveDataSetBalanced::Clone() const
    {
      return new RetrieveDataSetBalanced( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetBalanced::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetBalanced::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the feature code for all the implementations
        ( *itr)->SelectFeatures( CODE);
      }
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetBalanced::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the result code for all the implementations
        ( *itr)->SelectResults( CODE);
      }
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetBalanced::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the id code for all the implementations
        ( *itr)->SelectIds( CODE);
      }
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetBalanced::GetAlias() const
    {
      static const std::string s_Name( "Balanced");
      return s_Name;
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBalanced::GetFeatureLabelsWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBalanced::GetResultCodeWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBalanced::GetIdCodeWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetIdCodeWithSizes();
    }

    //! @brief get whether dataset generation requires labels
    //! @return true if dataset generation requires labels
    bool RetrieveDataSetBalanced::RequiresFeatureLabels() const
    {
      // iterate through the retrievers
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires feature labels
        if( ( *itr)->RequiresFeatureLabels())
        {
          // one retriever requires the feature labels, so return true
          return true;
        }
      }

      return false;
    }

    //! @brief get whether dataset generation requires result labels
    //! @return true if dataset generation requires result labels
    bool RetrieveDataSetBalanced::RequiresResultLabels() const
    {
      // iterate through the retrievers
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires result labels
        if( ( *itr)->RequiresResultLabels())
        {
          // one retriever requires the result labels, so return true
          return true;
        }
      }

      return false;
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetBalanced::GetNumberPartitionsAndIds() const
    {
      if( m_Retrievers.IsEmpty())
      {
        return
          storage::Pair< size_t, math::RangeSet< size_t> >( 1, math::RangeSet< size_t>( math::Range< size_t>( 0, 0)));
      }
      storage::Pair< size_t, math::RangeSet< size_t> > initial
      (
        m_Retrievers.FirstElement()->GetNumberPartitionsAndIds()
      );
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin() + 1), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires result labels
        if( ( *itr)->GetNumberPartitionsAndIds() != initial)
        {
          // different partitions, return undefined
          return
            storage::Pair< size_t, math::RangeSet< size_t> >( util::GetUndefined< size_t>(), math::RangeSet< size_t>());
        }
      }
      // all partitions are the same, return it
      return initial;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetBalanced::GenerateDataSet()
    {
      // just return if there were no datasets
      if( m_Retrievers.IsEmpty())
      {
        return util::ShPtr< descriptor::Dataset>( new descriptor::Dataset());
      }
      else if( m_Retrievers.GetSize() == 1) // if there was only 1 retriever, just return its results
      {
        return m_Retrievers.FirstElement()->GenerateDataSet();
      }

      // initialize a new vector of data sets, beginning with the first element
      util::ShPtrVector< descriptor::Dataset> datasets
      (
        1, m_Retrievers.FirstElement()->GenerateDataSet()
      );

      // determine feature / result size to ensure that all features and results are of the same size
      const size_t feature_size( datasets.FirstElement()->GetFeatureSize());
      const size_t result_size( datasets.FirstElement()->GetResultSize());
      const size_t id_size( datasets.FirstElement()->GetIdSize());

      // keep track of the size of the largest data set
      size_t largest_dataset_size( datasets.FirstElement()->GetSize());

      // generate the rest of the datasets
      for
      (
        size_t dataset_count( 1), number_datasets( m_Retrievers.GetSize());
        dataset_count < number_datasets;
        ++dataset_count
      )
      {
        // create a data set using the next retriever
        datasets.PushBack( m_Retrievers( dataset_count)->GenerateDataSet());

        // ensure that the feature size and result size were correct
        BCL_Assert
        (
          feature_size == datasets.LastElement()->GetFeatureSize(),
          " cannot balance datasets with differing feature sizes"
        );
        BCL_Assert
        (
          result_size == datasets.LastElement()->GetResultSize(),
          " cannot balance datasets with differing result sizes"
        );
        BCL_Assert
        (
          id_size == datasets.LastElement()->GetIdSize(),
          " cannot balance datasets with differing id sizes"
        );

        // update the largest dataset size
        largest_dataset_size = std::max( largest_dataset_size, datasets.LastElement()->GetSize());
      }

      const size_t dataset_size( largest_dataset_size * datasets.GetSize());

      // create a new data set memory that balances the output of the old data sets
      // initialize a new data set
      util::ShPtr< descriptor::Dataset> sp_dataset_balanced
      (
        new descriptor::Dataset
        (
          dataset_size,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );
      storage::Vector< linal::MatrixConstReference< float> > unbalanced_features( datasets.GetSize());
      storage::Vector< linal::MatrixConstReference< float> > unbalanced_results( datasets.GetSize());
      storage::Vector< linal::MatrixConstReference< char> > unbalanced_ids( datasets.GetSize());
      for
      (
        size_t dataset_number( 0), number_datasets( datasets.GetSize());
        dataset_number < number_datasets;
        ++dataset_number
      )
      {
        unbalanced_features( dataset_number) = linal::MatrixConstReference< float>( datasets( dataset_number)->GetFeaturesReference());
        unbalanced_results( dataset_number)  = linal::MatrixConstReference< float>( datasets( dataset_number)->GetResultsReference() );
        unbalanced_ids( dataset_number)      = linal::MatrixConstReference< char>( datasets( dataset_number)->GetIdsReference()     );
      }
      for( size_t data_number( 0), balanced_row_number( 0); data_number < largest_dataset_size; ++data_number)
      {
        for
        (
          size_t dataset_number( 0), number_datasets( datasets.GetSize());
          dataset_number < number_datasets;
          ++dataset_number, ++balanced_row_number
        )
        {
          // get a reference to the dataset that this feature/result will be pulled from
          const descriptor::Dataset &dataset( *datasets( dataset_number));

          // determine the feature number to take from this dataset
          const size_t feature_number( data_number % dataset.GetSize());

          sp_dataset_balanced->AddData
          (
            balanced_row_number,
            unbalanced_features( dataset_number).GetRow( feature_number),
            unbalanced_results( dataset_number).GetRow( feature_number),
            unbalanced_ids( dataset_number).GetRow( feature_number)
          );
        }
      }

      return sp_dataset_balanced;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetBalanced::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "balances features/results from several data sets such that a randomly chosen element in the data set "
        "has a roughly equal probability of coming from each data set"
      );
      member_data.AddInitializer( "", "datasets to balance", io::Serialization::GetAgent( &m_Retrievers));

      return member_data;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetBalanced::GetNominalSize() const
    {
      size_t max_size( 0);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the result code for all the implementations
        max_size = std::max( max_size, ( *itr)->GetNominalSize());
      }
      return max_size * m_Retrievers.GetSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_base.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "linal/bcl_linal_vector_reference.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetBase::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      m_FeatureCodeLabel = CODE;
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetBase::SelectResults( const util::ObjectDataLabel &CODE)
    {
      m_ResultCodeLabel = CODE;
    }

    //! @brief Set the code / label for the ids (3rd part) of the data set
    //! @param CODE the new code
    void RetrieveDataSetBase::SelectIds( const util::ObjectDataLabel &CODE)
    {
      m_IdCodeLabel = CODE;
    }

    //! @brief Convenience function for apps that use implementations of this interface
    //! This function checks loads the feature labels and/or columns from the given filename flag
    //! and asserts if the filename flag was not given but was needed
    //! @param FLAG flag which can contain filename of the features and/or feature indices
    void RetrieveDataSetBase::SelectFeaturesGivenFilenameFlag( const command::FlagInterface &FLAG)
    {
      if( FLAG.GetFlag() && !FLAG.GetParameterList().IsEmpty())
      {
        const std::string label_or_filename( FLAG.GetFirstParameter()->GetValue());
        if( io::DirectoryEntry( label_or_filename).DoesExist())
        {
          io::IFStream input;
          io::File::MustOpenIFStream( input, FLAG.GetFirstParameter()->GetValue());
          util::ObjectDataLabel label( input);
          this->SelectFeatures( label);
          io::File::CloseClearFStream( input);

          // data set retrievers for primary sources (like SdfFile or SdfDb)
          util::ObjectDataLabel canonicalized_label( this->GetFeatureLabelsWithSizes().GetLabel());

          // handle the case that this is a bin file generated by an older bcl, in which case the name on the label will
          // be wrong
          if( canonicalized_label.GetValue() == "Features" || label.GetValue() == "Features")
          {
            canonicalized_label = util::ObjectDataLabel( label.GetValue(), canonicalized_label.GetArguments());
          }
          // handle the case that the user enters a descriptor that is not surrounded by a Combine.  This is commonly
          // the case when the user asks for help, or where the result label is really trivial and does not need its own
          // file
          else if
          (
            canonicalized_label.GetValue() != label.GetValue()
            && canonicalized_label.GetNumberArguments() == 1
            && canonicalized_label.GetArgument( 0).GetValue() == label.GetValue()
          )
          {
            canonicalized_label = canonicalized_label.GetArgument( 0);
          }

          // check whether the feature labels are different; usually due to internal canonicalization
          if( canonicalized_label != label)
          {
            if( canonicalized_label.GetNumberArguments() == label.GetNumberArguments())
            {
              BCL_MessageCrt
              (
                "The given feature descriptors file uses default values for some descriptors. This isn't a problem for "
                "most common workflows, however, there is one exception. "
                "If you're creating a bin file, and manually selecting descriptors (e.g. deleting descriptors from the "
                "file to test their relevance when training models), then you will need to use the version of this file"
                " with the .updated postfix, which was written out for your convenience"
              );
              io::OFStream output;
              io::File::MustOpenOFStream( output, FLAG.GetFirstParameter()->GetValue() + ".updated");
              output << canonicalized_label.ToStringDefaultWidth();
              io::File::CloseClearFStream( output);
              this->SelectFeatures( canonicalized_label);
            }
            else
            {
              BCL_MessageCrt
              (
                "Bad label, canonicalized: " + canonicalized_label.ToStringDefaultWidth()
                + "; given: " + label.ToStringDefaultWidth()
              );
            }
          }
        }
        else
        {
          // look for an extension
          const size_t extension_pos( label_or_filename.rfind( '.'));
          size_t close_parens_pos( label_or_filename.rfind( ')'));
          if( close_parens_pos == std::string::npos)
          {
            close_parens_pos = 0;
          }
          // if the label looks more like an extension than a descriptor, just issue the filename error message
          if( extension_pos != std::string::npos && extension_pos >= close_parens_pos)
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " requires a valid file containing descriptors or a valid descriptor",
              -1
            );
          }
          std::ostringstream output_err_stream;
          util::ObjectDataLabel label;
          if( label.TryAssign( label_or_filename, output_err_stream))
          {
            this->SelectFeatures( label);
          }
          else
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " requires feature descriptors but was given neither a file with labels nor a parseable descriptor, error "
              "from parsing: " + output_err_stream.str(),
              -1
            );
          }
        }
      }
      else
      {
        BCL_Assert( !this->RequiresFeatureLabels(), this->GetClassIdentifier() + " requires feature labels!");
      }
    }

    //! @brief Convenience function for apps that use implementations of this interface
    //! This function checks loads the result labels and/or columns from the given filename flag
    //! and asserts if the filename flag was not given but was needed
    //! @param LABELS_ flag which can contain filename of the feature labels, if they are needed
    void RetrieveDataSetBase::SelectResultsGivenFilenameFlag( const command::FlagInterface &FLAG)
    {
      if( FLAG.GetFlag() && !FLAG.GetParameterList().IsEmpty())
      {
        const std::string label_or_filename( FLAG.GetFirstParameter()->GetValue());
        if( io::DirectoryEntry( label_or_filename).DoesExist())
        {
          io::IFStream input;
          io::File::MustOpenIFStream( input, FLAG.GetFirstParameter()->GetValue());
          util::ObjectDataLabel label( input);
          this->SelectResults( label);
          io::File::CloseClearFStream( input);

          // data set retrievers for primary sources (like SdfFile or SdfDb)
          util::ObjectDataLabel canonicalized_label( this->GetResultCodeWithSizes().GetLabel());

          // handle the case that this is a bin file generated by an older bcl, in which case the name on the label will
          // be wrong
          if( canonicalized_label.GetValue() == "Features" || label.GetValue() == "Features")
          {
            canonicalized_label = util::ObjectDataLabel( label.GetValue(), canonicalized_label.GetArguments());
          }
          // handle the case that the user enters a descriptor that is not surrounded by a Combine.  This is commonly
          // the case when the user asks for help, or where the result label is really trivial and does not need its own
          // file
          else if
          (
            canonicalized_label.GetValue() != label.GetValue()
            && canonicalized_label.GetNumberArguments() == 1
            && canonicalized_label.GetArgument( 0).GetValue() == label.GetValue()
          )
          {
            label = canonicalized_label;
          }

          // check whether the feature labels are different; usually due to internal canonicalization
          if( canonicalized_label != label)
          {
            if( canonicalized_label.GetNumberArguments() == label.GetNumberArguments())
            {
              BCL_MessageCrt
              (
                "The given results descriptors file uses default values for some descriptors. This isn't a problem for "
                "most common workflows, however, there is one exception. "
                "If you're creating a bin file, and will be manually selecting which result descriptors to train on for "
                "a model, then you will need to use the version of this file with the .updated postfix, which was "
                "written out for your convenience"
              );
              io::OFStream output;
              io::File::MustOpenOFStream( output, FLAG.GetFirstParameter()->GetValue() + ".updated");
              output << canonicalized_label.ToStringDefaultWidth();
              io::File::CloseClearFStream( output);
              this->SelectResults( canonicalized_label);
            }
            else
            {
              BCL_MessageCrt
              (
                "Bad label, canonicalized: " + canonicalized_label.ToStringDefaultWidth()
                + "; given: " + label.ToStringDefaultWidth()
              );
            }
          }
        }
        else
        {
          // look for an extension
          const size_t extension_pos( label_or_filename.rfind( '.'));
          size_t close_parens_pos( label_or_filename.rfind( ')'));
          if( close_parens_pos == std::string::npos)
          {
            close_parens_pos = 0;
          }
          // if the label looks more like an extension than a descriptor, just issue the filename error message
          if( extension_pos != std::string::npos && extension_pos >= close_parens_pos)
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " requires a valid file containing descriptors or a valid descriptor",
              -1
            );
          }
          std::ostringstream output_err_stream;
          util::ObjectDataLabel label;
          if( label.TryAssign( label_or_filename, output_err_stream))
          {
            this->SelectResults( label);
          }
          else
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " was given result descriptors flag with neither a file with labels nor a parseable descriptor, error "
              "from parsing: " + output_err_stream.str(),
              -1
            );
          }
        }
      }
      else
      {
        BCL_Assert( !this->RequiresResultLabels(), this->GetClassIdentifier() + " requires result labels!");
      }
    }

    //! @brief Convenience function for apps that use implementations of this interface
    //! This function checks loads the ids labels and/or columns from the given filename flag
    //! and asserts if the filename flag was not given but was needed
    //! @param FLAG which can contain filename of the id labels, if they are needed
    void RetrieveDataSetBase::SelectIdsGivenFilenameFlag( const command::FlagInterface &FLAG)
    {
      if( FLAG.GetFlag() && !FLAG.GetParameterList().IsEmpty())
      {
        const std::string label_or_filename( FLAG.GetFirstParameter()->GetValue());
        if( io::DirectoryEntry( label_or_filename).DoesExist())
        {
          io::IFStream input;
          io::File::MustOpenIFStream( input, FLAG.GetFirstParameter()->GetValue());
          util::ObjectDataLabel label( input);
          this->SelectIds( label);
          io::File::CloseClearFStream( input);

          // data set retrievers for primary sources (like SdfFile or SdfDb)
          util::ObjectDataLabel canonicalized_label( this->GetIdCodeWithSizes().GetLabel());

          // handle the case that this is a bin file generated by an older bcl, in which case the name on the label will
          // be wrong
          if( canonicalized_label.GetValue() == "Features" || label.GetValue() == "Features")
          {
            canonicalized_label = util::ObjectDataLabel( label.GetValue(), canonicalized_label.GetArguments());
          }
          // handle the case that the user enters a descriptor that is not surrounded by a Combine.  This is commonly
          // the case when the user asks for help, or where the result label is really trivial and does not need its own
          // file
          else if
          (
            canonicalized_label.GetValue() != label.GetValue()
            && canonicalized_label.GetNumberArguments() == 1
            && canonicalized_label.GetArgument( 0).GetValue() == label.GetValue()
          )
          {
            canonicalized_label = canonicalized_label.GetArgument( 0);
          }

          // check whether the feature labels are different; usually due to internal canonicalization
          if( canonicalized_label != label)
          {
            if( canonicalized_label.GetNumberArguments() == label.GetNumberArguments())
            {
              BCL_MessageCrt
              (
                "The given ids descriptor file used default values for some descriptors. This isn't a problem for "
                "most common workflows, however, there is one exception. "
                "If you're creating a bin file, and will be manually changing the id descriptors for use in the "
                "-id_labels flag on any application, then you will need to use the version of this file with the "
                ".updated postfix, which was written out for your convenience. Applications will generally load all ids"
                " given in the bin file unless told otherwise, so this should not be necessary except when some of the "
                "ids are not desired"
              );
              io::OFStream output;
              io::File::MustOpenOFStream( output, FLAG.GetFirstParameter()->GetValue() + ".updated");
              output << canonicalized_label.ToStringDefaultWidth();
              io::File::CloseClearFStream( output);
              this->SelectIds( canonicalized_label);
            }
            else
            {
              BCL_MessageCrt
              (
                "Bad label, canonicalized: " + canonicalized_label.ToStringDefaultWidth()
                + "; given: " + label.ToStringDefaultWidth()
              );
            }
          }
        }
        else
        {
          // look for an extension
          const size_t extension_pos( label_or_filename.rfind( '.'));
          size_t close_parens_pos( label_or_filename.rfind( ')'));
          if( close_parens_pos == std::string::npos)
          {
            close_parens_pos = 0;
          }
          // if the label looks more like an extension than a descriptor, just issue the filename error message
          if( extension_pos != std::string::npos && extension_pos >= close_parens_pos)
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " was given id labels flag but was not given a valid file containing descriptors or a valid descriptor",
              -1
            );
          }
          std::ostringstream output_err_stream;
          util::ObjectDataLabel label;
          if( label.TryAssign( label_or_filename, output_err_stream))
          {
            this->SelectIds( label);
          }
          else
          {
            BCL_Exit
            (
              this->GetClassIdentifier()
              + " was given id descriptors flag with neither a file with labels nor a parseable descriptor, error "
              "from parsing: " + output_err_stream.str(),
              -1
            );
          }
        }
      }
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @return the code / label for the feature (1st part) of the data set
    const util::ObjectDataLabel &RetrieveDataSetBase::GetFeatureCode() const
    {
      return m_FeatureCodeLabel;
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    const util::ObjectDataLabel &RetrieveDataSetBase::GetResultCode() const
    {
      return m_ResultCodeLabel;
    }

    //! @brief Get the code / label for the ids (3rd part) of the data set
    //! @return the code / label for the ids (3rd part) of the data set
    const util::ObjectDataLabel &RetrieveDataSetBase::GetIdCode() const
    {
      return m_IdCodeLabel;
    }

    //! @brief determine the start and end index position given the size, the range id and the number of ranges
    //! @param RANGE_OF_IDS range of range ids
    //! @param NUMBER_RANGES number of total ranges
    //! @param TOTAL_SIZE_OF_DATASET total size of dataset
    //! @return range with start and end position
    math::Range< size_t> RetrieveDataSetBase::GetStartEndPositionOfRange
    (
      const math::Range< size_t> RANGE_OF_IDS,
      const size_t NUMBER_RANGES,
      const size_t TOTAL_SIZE_OF_DATASET
    )
    {
      math::Range< size_t> closed_range( RANGE_OF_IDS.CloseBorders());
      BCL_MessageDbg
      (
        "Param: RANGES: " + closed_range.GetString()
        + " NUMBER_RANGES: " + util::Format()( NUMBER_RANGES)
        + " TOTAL_SIZE_OF_DATASET: " + util::Format()( TOTAL_SIZE_OF_DATASET)
      );

      BCL_Assert( NUMBER_RANGES > 0, "Must have at least one range");

      BCL_Assert
      (
        closed_range.GetMax() < NUMBER_RANGES,
        "range given to GetStartEndPositionOfRange is larger or equal then the number of ranges!"
      );

      // size of range
      const size_t min_size_of_range( TOTAL_SIZE_OF_DATASET / NUMBER_RANGES);
      const size_t number_ranges_with_max_size( TOTAL_SIZE_OF_DATASET % NUMBER_RANGES);

      // range start
      const size_t range_start
      (
        closed_range.GetMin() * min_size_of_range
        + std::min( closed_range.GetMin(), number_ranges_with_max_size)
      );
      // range end
      const size_t range_end
      (
        ( closed_range.GetMax() + 1) * min_size_of_range
        + std::min( ( closed_range.GetMax() + 1), number_ranges_with_max_size)
      );

      // return start and end position of range
      return math::Range< size_t>
             (
               math::RangeBorders::e_LeftClosed,
               range_start,
               range_end,
               math::RangeBorders::e_RightOpen
             );
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded
    //! @param RESULTS_STORAGE where to store the corresponding results
    //! @param IDS_STORAGE wehre to put the corresponding ids
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDataSetBase::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      // ensure that features and results are the same size
      BCL_Assert
      (
        FEATURES_STORAGE.GetNumberRows() == RESULTS_STORAGE.GetNumberRows(),
        "Different number of features and results"
      );
      BCL_Assert
      (
        FEATURES_STORAGE.GetNumberRows() == IDS_STORAGE.GetNumberRows() || IDS_STORAGE.GetNumberRows() == size_t( 0),
        "Different number of features and ids"
      );

      // ensure that the start position is within the storage matrix
      BCL_Assert
      (
        START_FEATURE_NUMBER <= FEATURES_STORAGE.GetNumberRows(),
        "start feature was higher than storage"
      );

      // standardize the range, so the left side of it has a closed border and the right has an open border
      const math::Range< size_t> standard_subset( SUBSET.StandardizeRange());

      // ensure that the end position is within the storage matrix
      BCL_Assert
      (
        START_FEATURE_NUMBER + standard_subset.GetWidth() <= FEATURES_STORAGE.GetNumberRows(),
        "Storage was too small to contain entire subset"
      );

      // this is the naive implementation, only used if the data set retriever does not provide a better way
      util::ShPtr< descriptor::Dataset> dataset( GenerateDataSet());

      // determine the end feature # for the copy
      const size_t end_chosen_features
      (
        std::max( standard_subset.GetMin(), std::min( standard_subset.GetMax(), dataset->GetSize()))
      );

      // select the desired elements
      std::copy
      (
        dataset->GetFeaturesReference()[ standard_subset.GetMin()],
        dataset->GetFeaturesReference()[ end_chosen_features],
        FEATURES_STORAGE[ START_FEATURE_NUMBER]
      );

      std::copy
      (
        dataset->GetResultsReference()[ standard_subset.GetMin()],
        dataset->GetResultsReference()[ end_chosen_features],
        RESULTS_STORAGE[ START_FEATURE_NUMBER]
      );
      std::copy
      (
        dataset->GetIdsReference()[ standard_subset.GetMin()],
        dataset->GetIdsReference()[ end_chosen_features],
        IDS_STORAGE[ START_FEATURE_NUMBER]
      );

      return end_chosen_features - standard_subset.GetMin();
    }

    //! @brief get the size of the dataset resulting from a selected range of chunks of a dataset with a given size
    //! @param RANGES_OF_IDS range of range ids
    //! @param NUMBER_RANGES number of total ranges
    //! @param TOTAL_SIZE_OF_DATASET total size of dataset
    //! @return the size of the dataset resulting from a selected range of chunks of a dataset with a given size
    size_t RetrieveDataSetBase::GetSubsetSize
    (
      const math::RangeSet< size_t> RANGES_OF_IDS,
      const size_t NUMBER_RANGES,
      const size_t TOTAL_SIZE_OF_DATASET
    )
    {
      // if there are no ranges, then return the total size
      if( RANGES_OF_IDS.IsEmpty())
      {
        return TOTAL_SIZE_OF_DATASET;
      }

      size_t size_so_far( 0);
      // add up the sizes for each range in the rangeset
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr( RANGES_OF_IDS.GetRanges().Begin()), itr_end( RANGES_OF_IDS.GetRanges().End());
        itr != itr_end;
        ++itr
      )
      {
        size_so_far += GetStartEndPositionOfRange( *itr, NUMBER_RANGES, TOTAL_SIZE_OF_DATASET).GetWidth();
      }

      return size_so_far;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_bootstrap.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetBootstrap::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetBootstrap())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetBootstrap *RetrieveDataSetBootstrap::Clone() const
    {
      return new RetrieveDataSetBootstrap( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetBootstrap::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetBootstrap::GetAlias() const
    {
      static const std::string s_Name( "Bootstrap");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetBootstrap::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetBootstrap::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetBootstrap::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBootstrap::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBootstrap::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetBootstrap::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetBootstrap::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetBootstrap::GenerateDataSet()
    {
      util::ShPtr< descriptor::Dataset> dataset( m_Implementation->GenerateDataSet());

      random::UniformDistribution uni;
      uni.SetSeed( random::GetGlobalRandom().GetFlagRandomSeed()->GetFirstParameter()->GetNumericalValue< size_t>());
      const size_t mx( dataset->GetSize() - 1);
      util::ShPtr< descriptor::Dataset> dataset_new( dataset->HardCopy());
      for( size_t src_row_id( 0); src_row_id <= mx; ++src_row_id)
      {
        const size_t row_id( uni.Random( size_t( 0), mx));
        dataset_new->GetFeaturesReference().GetRow( src_row_id).CopyValues( dataset->GetFeaturesReference().GetRow( row_id));
        dataset_new->GetResultsReference().GetRow( src_row_id).CopyValues( dataset->GetResultsReference().GetRow( row_id));
        dataset_new->GetIdsReference().GetRow( src_row_id).CopyValues( dataset->GetIdsReference().GetRow( row_id));
      }
      return dataset_new;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetBootstrap::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Retrieves a bootstrapped dataset. This means that the data are loaded in,");
      parameters.AddInitializer
      (
        "",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      return parameters;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetBootstrap::GetNominalSize() const
    {
      return GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_retrieve_data_set_by_feature.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByFeature::s_FeaturesInstance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetByFeature( true))
    );
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByFeature::s_ResultsInstance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetByFeature( false))
    );
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByFeature::s_FeaturesFilterInstance
    (
      util::Enumerated< util::FunctionInterfaceSerializable< util::ShPtr< descriptor::Dataset>, util::ShPtr< descriptor::Dataset> > >::AddInstance
      (
        new RetrieveDataSetByFeature( true, true)
      )
    );
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByFeature::s_ResultsFilterInstance
    (
      util::Enumerated< util::FunctionInterfaceSerializable< util::ShPtr< descriptor::Dataset>, util::ShPtr< descriptor::Dataset> > >::AddInstance
      (
        new RetrieveDataSetByFeature( false, true)
      )
    );

    //! @brief constructor from parameters
    //! @param RANGES the ranges of features to consider
    //! @param RETRIEVER the dataset retriever to use
    RetrieveDataSetByFeature::RetrieveDataSetByFeature
    (
      const bool &RETRIEVE_FEATURES,
      const bool &JUST_FILTER,
      const size_t &FEATURE_INDEX,
      const math::RangeSet< float> &RANGES,
      const util::Implementation< RetrieveDataSetBase> &RETRIEVER,
      const std::string &DESCRIPTOR
    ) :
      m_FeatureRanges( RANGES),
      m_FeatureName( DESCRIPTOR),
      m_FeatureIndex( FEATURE_INDEX),
      m_Retriever( RETRIEVER),
      m_RetrieveFeature( RETRIEVE_FEATURES),
      m_FeatureInDataset( false),
      m_Invert( false),
      m_JustFilter( JUST_FILTER)
    {
    }

    //! @brief Clone function
    //! @return pointer to new RetrieveDataSetByFeature
    RetrieveDataSetByFeature *RetrieveDataSetByFeature::Clone() const
    {
      return new RetrieveDataSetByFeature( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &RetrieveDataSetByFeature::GetClassIdentifier() const
    {
      return GetStaticClassName< RetrieveDataSetByFeature>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetByFeature::GetAlias() const
    {
      static const std::string s_feature_name( "FeatureRange"), s_results_name( "ResultRange");
      return m_RetrieveFeature ? s_feature_name : s_results_name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    void RetrieveDataSetByFeature::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
      if( m_RetrieveFeature)
      {
        m_FeatureIndex = util::GetUndefined< size_t>();
      }
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetByFeature::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
      if( !m_RetrieveFeature)
      {
        m_FeatureIndex = util::GetUndefined< size_t>();
      }
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetByFeature::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByFeature::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByFeature::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByFeature::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetByFeature::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      RetrieveDataSetByFeature::GenerateDataSet()
    {
      // get the base data set
      util::ShPtr< descriptor::Dataset> dataset( m_Retriever->GenerateDataSet());

      // update the feature index if necessary
      UpdateFeatureIndex();

      storage::Vector< size_t> features_to_keep;
      if( m_FeatureInDataset)
      {
        // select the desired features
        features_to_keep =
          GetFeaturesInRange( m_RetrieveFeature ? dataset->GetFeaturesReference() : dataset->GetResultsReference(), m_FeatureIndex);
      }
      else
      {
        util::ShPtr< descriptor::Dataset> dataset_tmp( m_RetrieverHardCopy->GenerateDataSet());
        BCL_Assert
        (
          dataset_tmp->GetSize() == dataset->GetSize(),
          "Dataset filtering failed because dataset generated without the filter descriptor has a different size than "
          "the dataset generated with the descriptor"
        );
        features_to_keep =
          GetFeaturesInRange( m_RetrieveFeature ? dataset_tmp->GetFeaturesReference() : dataset_tmp->GetResultsReference(), m_FeatureIndex);
      }
      dataset->KeepRows( features_to_keep);

      // return
      return dataset;
    }

    //! @brief operator() can be used to filter a dataset
    //! @param DATASET the dataset to filter
    //! @return the filtered dataset
    util::ShPtr< descriptor::Dataset> RetrieveDataSetByFeature::operator()( const util::ShPtr< descriptor::Dataset> &DATASET) const
    {
      // update the feature index if necessary
      FeatureLabelSet feature_label_set
      (
        m_RetrieveFeature ? *DATASET->GetFeaturesPtr()->GetFeatureLabelSet() : *DATASET->GetResultsPtr()->GetFeatureLabelSet()
      );
      feature_label_set = feature_label_set.SplitFeatureLabelSet( true);
      size_t feature_position( feature_label_set.GetMemberLabels().Find( m_FeatureName));
      BCL_Assert
      (
        feature_position < feature_label_set.GetMemberLabels().GetSize(),
        "Filter descriptor must be present in the dataset for pass-through filtering to work (e.g. model::TrainMCCV)"
      );
      BCL_MessageStd( "Selecting from property index: " + util::Format()( feature_position));
      return
        DATASET->GetRows
        (
          GetFeaturesInRange
          (
            m_RetrieveFeature ? DATASET->GetFeaturesReference() : DATASET->GetResultsReference(),
            feature_position
          )
        );
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief get the feature indices in the specified range
    //! @param FEATURES the set of features to consider
    //! @return indices of features for which the desired feature is in the specified range set
    storage::Vector< size_t> RetrieveDataSetByFeature::GetFeaturesInRange
    (
      const linal::MatrixConstInterface< float> &FEATURES,
      const size_t &FEATURE_INDEX
    ) const
    {
      storage::Vector< size_t> chosen_features;
      chosen_features.AllocateMemory( FEATURES.GetNumberRows());
      BCL_Assert( FEATURE_INDEX < FEATURES.GetNumberCols(), "Requested feature not in this dataset");
      for( size_t row( 0), n_rows( FEATURES.GetNumberRows()); row < n_rows; ++row)
      {
        if( m_FeatureRanges.IsWithin( FEATURES[ row][ FEATURE_INDEX]) != m_Invert)
        {
          chosen_features.PushBack( row);
        }
      }
      return chosen_features;
    }

    //! @brief test whether this retriever can generate sub-ranges of datasets without loading the entire dataset
    //! @return true if this retriever can generate sub-ranges of datasets without loading the entire dataset
    bool RetrieveDataSetByFeature::SupportsEfficientSubsetLoading() const
    {
      return m_Retriever->SupportsEfficientSubsetLoading();
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded, must be large enough to hold the subset without resizing
    //! @param RESULTS_STORAGE where to store the corresponding results, must be large enough to hold the subset without resizing
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDataSetByFeature::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      // get the base data set
      const size_t n_rows( SUBSET.GetWidth() + 1);
      linal::Matrix< float> features( n_rows, FEATURES_STORAGE.GetNumberCols(), util::GetUndefined< float>());
      linal::Matrix< float> results( n_rows, RESULTS_STORAGE.GetNumberCols(), util::GetUndefined< float>());
      linal::Matrix< char> ids( n_rows, IDS_STORAGE.GetNumberCols(), ' ');
      size_t n_created( m_Retriever->GenerateDataSubset( SUBSET, features, results, ids, size_t( 0)));

      // update the feature index if necessary
      UpdateFeatureIndex();

      if( !n_created)
      {
        BCL_MessageStd( "No features kept in this block pre-filter");
        return 0;
      }
      features.ShrinkRows( n_created);
      results.ShrinkRows( n_created);
      ids.ShrinkRows( n_created);

      storage::Vector< size_t> features_to_keep;
      if( m_FeatureInDataset)
      {
        // select the desired features
        linal::MatrixConstReference< float> reference( m_RetrieveFeature ? features : results);
        features_to_keep = GetFeaturesInRange( reference, m_FeatureIndex);
      }
      else
      {
        linal::Matrix< float> filter( n_rows, 1, util::GetUndefined< float>()), other( filter);
        linal::Matrix< char> ids_tmp;
        size_t n_created_tmp( 0);
        if( m_RetrieveFeature)
        {
          n_created_tmp = m_RetrieverHardCopy->GenerateDataSubset( SUBSET, filter, other, ids_tmp, size_t( 0));
        }
        else
        {
          n_created_tmp = m_RetrieverHardCopy->GenerateDataSubset( SUBSET, other, filter, ids, size_t( 0));
        }
        BCL_Assert
        (
          n_created_tmp == n_created,
          "Dataset filtering failed because dataset generated without the filter descriptor has a different size than "
          "the dataset generated with the descriptor"
        );
        features_to_keep = GetFeaturesInRange( filter, m_FeatureIndex);
      }
      features.KeepRows( features_to_keep);
      results.KeepRows( features_to_keep);
      ids.KeepRows( features_to_keep);

      BCL_Assert
      (
        features_to_keep.GetSize() == features.GetNumberRows(),
        "All features in the given range should have been kept"
      );

      if( features_to_keep.IsEmpty())
      {
        BCL_MessageStd( "No features kept in this block, post-filter");
        return 0;
      }

      // copy the code vectors into the matrices
      std::copy( features.Begin(), features.End(), FEATURES_STORAGE[ START_FEATURE_NUMBER]);
      // copy the result
      std::copy( results.Begin(), results.End(), RESULTS_STORAGE[ START_FEATURE_NUMBER]);
      // copy the ids
      std::copy( ids.Begin(), ids.End(), IDS_STORAGE[ START_FEATURE_NUMBER]);

      // return
      return features_to_keep.GetSize();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetByFeature::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        m_RetrieveFeature
        ? "Select features (and corresponding results) within a particular range set"
        : "Select results (and corresponding features) within a particular range set"
      );
      if( !m_JustFilter)
      {
        member_data.AddInitializer
        (
          "dataset",
          "dataset retriever to call to get the entire data set",
          io::Serialization::GetAgent( &m_Retriever)
        );
      }
      member_data.AddInitializer
      (
        "range",
        "ranges of values to load, e.g. range=\"[ 0, 5) (7,10)\"",
        io::Serialization::GetAgent( &m_FeatureRanges),
        math::RangeSet< float>::GetCompleteRange().AsString()
      );
      member_data.AddInitializer
      (
        "invert",
        "true to load all values except those specified in range",
        io::Serialization::GetAgent( &m_Invert),
        "False"
      );

      member_data.AddInitializer
      (
        m_RetrieveFeature ? "feature" : "result",
        "Name of " + std::string( m_RetrieveFeature ? "feature" : "result") + " to consider",
        io::Serialization::GetAgent( &m_FeatureName)
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetByFeature::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

    //! @brief update the m_FeatureIndex
    void RetrieveDataSetByFeature::UpdateFeatureIndex()
    {
      // if the feature index is undefined, determine it from the feature string
      if( !util::IsDefined( m_FeatureIndex))
      {
        m_FeatureInDataset = false;
        storage::Vector< size_t> property_indices;
        FeatureLabelSet feature_label_set
        (
          m_RetrieveFeature ? m_Retriever->GetFeatureLabelsWithSizes() : m_Retriever->GetResultCodeWithSizes()
        );
        feature_label_set = feature_label_set.SplitFeatureLabelSet( true);
        size_t feature_position( feature_label_set.GetMemberLabels().Find( m_FeatureName));

        if( feature_position >= feature_label_set.GetMemberLabels().GetSize())
        {
          m_RetrieverHardCopy = m_Retriever.HardCopy();
          if( m_RetrieveFeature)
          {
            BCL_Assert
            (
              !m_Retriever->RequiresFeatureLabels(),
              "Dataset filtering requires that either the dataset have already been generated, or that the filter descriptor "
              "is included in the dataset"
            );
            m_RetrieverHardCopy->SelectFeatures( util::ObjectDataLabel( "", "Combine", m_FeatureName));
            m_RetrieverHardCopy->SelectResults
            (
              util::ObjectDataLabel
              (
                "",
                "Combine",
                m_RetrieverHardCopy->GetResultCodeWithSizes().SplitFeatureLabelSet().GetMemberLabels().FirstElement()
              )
            );
          }
          else
          {
            BCL_Assert
            (
              !m_Retriever->RequiresResultLabels(),
              "Dataset filtering requires that either the dataset have already been generated, or that the filter descriptor "
              "is included in the dataset"
            );
            m_RetrieverHardCopy->SelectResults( util::ObjectDataLabel( "", "Combine", m_FeatureName));
            m_RetrieverHardCopy->SelectFeatures
            (
              util::ObjectDataLabel
              (
                "",
                "Combine",
                m_RetrieverHardCopy->GetFeatureLabelsWithSizes().SplitFeatureLabelSet().GetMemberLabels().FirstElement()
              )
            );
          }
          m_RetrieverHardCopy->SelectIds( util::ObjectDataLabel());
          m_FeatureIndex = 0;
          m_FeatureInDataset = false;
          BCL_MessageStd
          (
            "Filter descriptor was not in dataset; finding it using "
            + m_RetrieverHardCopy->GetFeatureCode().ToString()
            + " features and " + m_RetrieverHardCopy->GetResultCode().ToString() + " results"
          );
        }
        else
        {
          BCL_MessageStd( "Selecting from property index: " + util::Format()( feature_position));
          m_FeatureIndex = feature_position;
          m_FeatureInDataset = true;
        }
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_retrieve_data_set_by_id.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetById::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetById())
    );
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetById::s_FilterInstance
    (
      util::Enumerated< util::FunctionInterfaceSerializable< util::ShPtr< descriptor::Dataset>, util::ShPtr< descriptor::Dataset> > >::AddInstance
      (
        new RetrieveDataSetById( storage::Set< std::string>(), util::Implementation< RetrieveDataSetBase>(), true)
      )
    );

    //! @brief constructor from parameters
    //! @param IDS the ids to consider
    //! @param RETRIEVER the dataset retriever to use
    //! @param JUST_FILTER set this to have this object run in pure-filter mode, in which case the dataset will not be
    //!        requested from the command line
    RetrieveDataSetById::RetrieveDataSetById
    (
      const storage::Set< std::string> &IDS,
      const util::Implementation< RetrieveDataSetBase> &RETRIEVER,
      const bool &JUST_FILTER
    ) :
      m_IdName(),
      m_Ids( IDS),
      m_Retriever( RETRIEVER),
      m_Invert( false),
      m_JustFilter( JUST_FILTER)
    {
    }

    //! @brief Clone function
    //! @return pointer to new RetrieveDataSetById
    RetrieveDataSetById *RetrieveDataSetById::Clone() const
    {
      return new RetrieveDataSetById( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &RetrieveDataSetById::GetClassIdentifier() const
    {
      return GetStaticClassName< RetrieveDataSetById>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetById::GetAlias() const
    {
      static const std::string s_name( "SelectIDs");
      return s_name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetById::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetById::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetById::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetById::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetById::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetById::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetById::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      RetrieveDataSetById::GenerateDataSet()
    {
      UpdateIDsIndex();

      // get the base data set
      util::ShPtr< descriptor::Dataset> dataset( m_Retriever->GenerateDataSet());

      if( m_IdIndices.IsEmpty())
      {
        for( size_t i( 0), n_ids( dataset->GetIdSize()); i < n_ids; ++i)
        {
          m_IdIndices.PushBack( i);
        }
      }

      // filter by the desired ids
      dataset->KeepRows( GetDesiredIDs( dataset->GetIdsReference(), m_IdIndices));

      // return
      return dataset;
    }

    //! @brief operator() can be used to filter a dataset
    //! @param DATASET the dataset to filter
    //! @return the filtered dataset
    util::ShPtr< descriptor::Dataset> RetrieveDataSetById::operator()( const util::ShPtr< descriptor::Dataset> &DATASET) const
    {
      storage::Vector< size_t> id_indices;
      const size_t expected_size( m_Ids.Begin()->size());
      if
      (
        m_IdName.empty()
        || !DATASET->GetIdsPtr()->GetFeatureLabelSet().IsDefined()
        || !DATASET->GetIdsPtr()->GetFeatureLabelSet()->GetSize()
        || DATASET->GetIdsPtr()->GetFeatureSize() == expected_size
      )
      {
        id_indices = storage::CreateIndexVector( expected_size);
      }
      else
      {
        id_indices = DATASET->GetIdsPtr()->GetFeatureLabelSet()->GetPropertyIndices( util::ObjectDataLabel( m_IdName));
      }
      return DATASET->GetRows( GetDesiredIDs( DATASET->GetIdsReference(), id_indices));
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief get indices of the desired id rows
    //! @param IDS the matrix of ids
    //! @return indices of the desired ids
    storage::Vector< size_t> RetrieveDataSetById::GetDesiredIDs
    (
      const linal::MatrixConstInterface< char> &IDS,
      const storage::Vector< size_t> &ID_INDICES
    ) const
    {
      storage::Vector< size_t> chosen_results;
      chosen_results.AllocateMemory( IDS.GetNumberRows());
      BCL_Assert( ID_INDICES.LastElement() < IDS.GetNumberCols(), "Out of bounds id column!");
      const size_t n_id_cols( ID_INDICES.GetSize());
      std::string tmp( ID_INDICES.GetSize(), ' ');
      for( size_t row( 0), n_rows( IDS.GetNumberRows()); row < n_rows; ++row)
      {
        // copy the desired character into the temp string
        const char *row_ptr( IDS[ row]);
        for( size_t col( 0); col < n_id_cols; ++col)
        {
          tmp[ col] = row_ptr[ ID_INDICES( col)];
        }
        // check whether the id was selected by the user
        if( m_Ids.Contains( util::TrimString( tmp)) != m_Invert)
        {
          chosen_results.PushBack( row);
        }
      }
      return chosen_results;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetById::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Select rows of the dataset with the given ids"
      );
      if( !m_JustFilter)
      {
        member_data.AddInitializer
        (
          "dataset",
          "dataset retriever to call to get the entire data set",
          io::Serialization::GetAgent( &m_Retriever)
        );
      }
      member_data.AddOptionalInitializer
      (
        "id type",
        "type of id to select (e.g. AASeqID, AAPdbID).  "
        "If not given, the complete set of ids selected by -id_labels is used",
        io::Serialization::GetAgent( &m_IdName)
      );
      member_data.AddInitializer
      (
        "ids",
        "actual ids to select",
        io::Serialization::GetAgent( &m_Ids)
      );
      member_data.AddInitializer
      (
        "invert",
        "select only rows that lack this id",
        io::Serialization::GetAgent( &m_Invert),
        "False"
      );
      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetById::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

    //! @brief test whether this retriever can generate sub-ranges of datasets without loading the entire dataset
    //! @return true if this retriever can generate sub-ranges of datasets without loading the entire dataset
    bool RetrieveDataSetById::SupportsEfficientSubsetLoading() const
    {
      return m_Retriever->SupportsEfficientSubsetLoading();
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded, must be large enough to hold the subset without resizing
    //! @param RESULTS_STORAGE where to store the corresponding results, must be large enough to hold the subset without resizing
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDataSetById::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      UpdateIDsIndex();
      if( m_IdIndices.IsEmpty())
      {
        for( size_t i( 0), n_ids( IDS_STORAGE.GetNumberCols()); i < n_ids; ++i)
        {
          m_IdIndices.PushBack( i);
        }
      }

      // get the base data set
      const size_t n_rows( SUBSET.GetWidth() + 1);
      linal::Matrix< float> features( n_rows, FEATURES_STORAGE.GetNumberCols(), util::GetUndefined< float>());
      linal::Matrix< float> results( n_rows, RESULTS_STORAGE.GetNumberCols(), util::GetUndefined< float>());
      linal::Matrix< char> ids( n_rows, IDS_STORAGE.GetNumberCols(), ' ');
      size_t n_created( m_Retriever->GenerateDataSubset( SUBSET, features, results, ids, size_t( 0)));

      features.ShrinkRows( n_created);
      results.ShrinkRows( n_created);
      ids.ShrinkRows( n_created);

      if( !n_created)
      {
        BCL_MessageStd( "No features kept in this block pre-filter");
        return 0;
      }

      // select the desired features
      linal::MatrixConstReference< char> reference( ids);
      storage::Vector< size_t> features_to_keep( GetDesiredIDs( reference, m_IdIndices));
      features.KeepRows( features_to_keep);
      results.KeepRows( features_to_keep);
      ids.KeepRows( features_to_keep);

      BCL_Assert
      (
        features_to_keep.GetSize() == features.GetNumberRows(),
        "All features in the given range should have been kept"
      );

      if( features_to_keep.IsEmpty())
      {
        return 0;
      }

      // copy the code vectors into the matrices
      std::copy( features.Begin(), features.End(), FEATURES_STORAGE[ START_FEATURE_NUMBER]);
      // copy the result
      std::copy( results.Begin(), results.End(), RESULTS_STORAGE[ START_FEATURE_NUMBER]);
      // copy the ids
      std::copy( ids.Begin(), ids.End(), IDS_STORAGE[ START_FEATURE_NUMBER]);

      // return
      return features_to_keep.GetSize();
    }

    //! @brief update the m_IdIndices
    void RetrieveDataSetById::UpdateIDsIndex()
    {
      if( m_IdIndices.IsEmpty())
      {
        if( !m_IdName.empty())
        {
          // create a label from the given name
          util::ObjectDataLabel label( m_IdName);
          if( RetrieveDataSetBase::GetIdCode().IsEmpty())
          {
            this->SelectIds( label);
          }
          m_IdIndices = this->GetIdCodeWithSizes().GetPropertyIndices( label);
        }
      }
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool RetrieveDataSetById::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      // trim all the input strings
      storage::Set< std::string> ids_l;
      for( auto itr( m_Ids.Begin()), itr_end( m_Ids.End()); itr != itr_end; ++itr)
      {
        ids_l.Insert( util::TrimString( *itr));
      }
      m_Ids.InternalData().swap( ids_l.InternalData());
      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_retrieve_data_set_by_result.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByResult::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetByResult())
    );
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetByResult::s_FilterInstance
    (
      util::Enumerated< util::FunctionInterfaceSerializable< util::ShPtr< descriptor::Dataset>, util::ShPtr< descriptor::Dataset> > >::AddInstance
      (
        new RetrieveDataSetByResult
        (
          math::RangeSet< float>::GetCompleteRange(),
          util::Implementation< RetrieveDataSetBase>(),
          true
        )
      )
    );

    //! @brief constructor from parameters
    //! @param RANGES the ranges of results to consider
    //! @param RETRIEVER the dataset retriever to use
    //! @param FILTER_ONLY true if the instance will only be used to filter a dataset generated elsewhere in the code
    RetrieveDataSetByResult::RetrieveDataSetByResult
    (
      const math::RangeSet< float> &RANGES,
      const util::Implementation< RetrieveDataSetBase> &RETRIEVER,
      const bool &FILTER_ONLY
    ) :
      m_ResultRanges( RANGES),
      m_Retriever( RETRIEVER),
      m_JustFilter( FILTER_ONLY)
    {
    }

    //! @brief Clone function
    //! @return pointer to new RetrieveDataSetByResult
    RetrieveDataSetByResult *RetrieveDataSetByResult::Clone() const
    {
      return new RetrieveDataSetByResult( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &RetrieveDataSetByResult::GetClassIdentifier() const
    {
      return GetStaticClassName< RetrieveDataSetByResult>();
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetByResult::GetAlias() const
    {
      static const std::string s_name( "ResultsInRange");
      return s_name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetByResult::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Retriever->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetByResult::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Retriever->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetByResult::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Retriever->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByResult::GetFeatureLabelsWithSizes() const
    {
      return m_Retriever->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByResult::GetResultCodeWithSizes() const
    {
      return m_Retriever->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetByResult::GetIdCodeWithSizes() const
    {
      return m_Retriever->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetByResult::GetNumberPartitionsAndIds() const
    {
      return m_Retriever->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

    //! @brief generate dataset, reduced to the desired # of cluster centers
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
      RetrieveDataSetByResult::GenerateDataSet()
    {
      // get the base data set and use the clustering algorithm on it
      util::ShPtr< descriptor::Dataset> dataset( m_Retriever->GenerateDataSet());

      // get the desired results
      const storage::Vector< size_t> desired_result_ids( GetResultsInRange( dataset->GetResultsReference()));

      // filter by those results
      dataset->KeepRows( desired_result_ids);

      // return
      return dataset;
    }

    //! @brief operator() can be used to filter a dataset
    //! @param DATASET the dataset to filter
    //! @return the filtered dataset
    util::ShPtr< descriptor::Dataset> RetrieveDataSetByResult::operator()( const util::ShPtr< descriptor::Dataset> &DATASET) const
    {
      return DATASET->GetRows( GetResultsInRange( DATASET->GetResultsReference()));
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief get a set of features that cover the same space as FEATURES at resolution RMSD
    //! @param FEATURES the set of features to consider
    //! @param RMSD the rmsd; vectors closer than this rmsd will be considered representative of each other
    //! @return indices of features that cover the same space as FEATURES at resolution RMSD
    storage::Vector< size_t> RetrieveDataSetByResult::GetResultsInRange( const linal::MatrixConstInterface< float> &RESULTS) const
    {
      storage::Vector< size_t> chosen_results;
      chosen_results.AllocateMemory( RESULTS.GetNumberRows());
      const size_t result_size( RESULTS.GetNumberCols());
      for( size_t row( 0), n_rows( RESULTS.GetNumberRows()); row < n_rows; ++row)
      {
        const float *row_ptr( RESULTS[ row]);
        for( size_t col( 0); col < result_size; ++col)
        {
          if( m_ResultRanges.IsWithin( row_ptr[ col]))
          {
            chosen_results.PushBack( row);
            break;
          }
        }
      }
      return chosen_results;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetByResult::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "Select results (and corresponding features) within a particular range set"
      );
      if( !m_JustFilter)
      {
        member_data.AddInitializer
        (
          "dataset",
          "dataset retriever to call to get the entire data set",
          io::Serialization::GetAgent( &m_Retriever)
        );
      }
      member_data.AddInitializer
      (
        "range",
        "ranges of result values to load, e.g. range=\"[ 0, 5)+(7,10)\"",
        io::Serialization::GetAgent( &m_ResultRanges),
        math::RangeSet< float>::GetCompleteRange().AsString()
      );

      return member_data;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetByResult::GetNominalSize() const
    {
      return m_Retriever->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_chunk.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetChunk::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetChunk())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetChunk *RetrieveDataSetChunk::Clone() const
    {
      return new RetrieveDataSetChunk( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetChunk::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetChunk::GetAlias() const
    {
      static const std::string s_Name( "Chunks");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetChunk::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetChunk::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetChunk::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetChunk::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetChunk::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetChunk::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetChunk::GetNumberPartitionsAndIds() const
    {
      return storage::Pair< size_t, math::RangeSet< size_t> >( m_NumberChunks, m_ChunkRanges);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetChunk::GenerateDataSet()
    {
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());

      math::RangeSet< size_t> fixed_chunk_ranges( m_ChunkRanges);
      // if more chunks were specified than there are points in the dataset, reduce the number to the max allowed for
      // this dataset
      const size_t number_chunks( std::min( m_NumberChunks, total_number_feature_results));
      // if range [0,0] is given assume user wants to use entire data set
      if( fixed_chunk_ranges.IsEmpty())
      {
        fixed_chunk_ranges += math::Range< size_t>( 0, number_chunks);
      }

      // eliminate range above total_number_feature_results
      if( fixed_chunk_ranges.GetMax() >= number_chunks)
      {
        fixed_chunk_ranges -= math::Range< size_t>( number_chunks, fixed_chunk_ranges.GetMax());
      }

      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( fixed_chunk_ranges, number_chunks, total_number_feature_results)
      );

      // initialize a new data set
      util::ShPtr< descriptor::Dataset> complete
      (
        new descriptor::Dataset
        (
          number_feature_results,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      linal::MatrixReference< float> features( complete->GetFeaturesReference());
      linal::MatrixReference< float> results( complete->GetResultsReference());
      linal::MatrixReference< char> ids( complete->GetIdsReference());

      // set the vector up with the given ranges
      size_t features_so_far( 0);
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr( fixed_chunk_ranges.GetRanges().Begin()), itr_end( fixed_chunk_ranges.GetRanges().End());
        itr != itr_end;
        ++itr
      )
      {
        math::Range< size_t> start_end_chunk
        (
          RetrieveDataSetBase::GetStartEndPositionOfRange( *itr, number_chunks, total_number_feature_results)
        );
        features_so_far +=
          m_Implementation->GenerateDataSubset
          (
            start_end_chunk.StandardizeRange(),
            features,
            results,
            ids,
            features_so_far
          );
      }

      complete->ShrinkRows( features_so_far);

      return complete;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetChunk::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Retrieves chunks (approximately equal-sized partitions) of a specific dataset");
      parameters.AddInitializer
      (
        "number chunks",
        "number of equal sized chunks (approximately equal-sized partitions) to consider",
        io::Serialization::GetAgentWithMin( &m_NumberChunks, 1)
      );
      parameters.AddInitializer
      (
        "chunks",
        "ranges of chunks to load, e.g. chunks=\"[ 0, 5) (7,10)\"",
        io::Serialization::GetAgent( &m_ChunkRanges)
      );
      parameters.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      return parameters;
    } // GetParameters

    //! @brief test whether this retriever can generate sub-ranges of datasets without loading the entire dataset
    //! @return true if this retriever can generate sub-ranges of datasets without loading the entire dataset
    bool RetrieveDataSetChunk::SupportsEfficientSubsetLoading() const
    {
      return m_Implementation->SupportsEfficientSubsetLoading();
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded, must be large enough to hold the subset without resizing
    //! @param RESULTS_STORAGE where to store the corresponding results, must be large enough to hold the subset without resizing
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDataSetChunk::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      // GenerateDataset in these wrapper functions just translates the external view of the outside dataset retriever
      // into the internal view of the internally held dataset
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());

      math::RangeSet< size_t> fixed_chunk_ranges( m_ChunkRanges);
      // if more chunks were specified than there are points in the dataset, reduce the number to the max allowed for
      // this dataset
      const size_t number_chunks( std::min( m_NumberChunks, total_number_feature_results));
      // if range [0,0] is given assume user wants to use entire data set
      if( fixed_chunk_ranges.IsEmpty())
      {
        fixed_chunk_ranges += math::Range< size_t>( 0, number_chunks);
      }

      // eliminate range above total_number_feature_results
      if( fixed_chunk_ranges.GetMax() >= number_chunks)
      {
        fixed_chunk_ranges -= math::Range< size_t>( number_chunks, fixed_chunk_ranges.GetMax());
      }

      // standardize the range, so the left side of it has a closed border and the right has an open border
      const math::Range< size_t> standard_subset( SUBSET.StandardizeRange());

      // ensure that the end position is within the storage matrix
      BCL_Assert
      (
        START_FEATURE_NUMBER + standard_subset.GetWidth() <= FEATURES_STORAGE.GetNumberRows(),
        "Storage was too small to contain entire subset"
      );

      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( fixed_chunk_ranges, number_chunks, total_number_feature_results)
      );
      math::Range< size_t> subset_fixed( standard_subset);
      if( standard_subset.GetMax() > number_feature_results)
      {
        subset_fixed.SetMax( number_feature_results);
      }

      // get the actual ranges that will be taken from the dataset
      math::RangeSet< size_t> ranges;

      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr_range( fixed_chunk_ranges.GetRanges().Begin()), itr_range_end( fixed_chunk_ranges.GetRanges().End());
        itr_range != itr_range_end;
        ++itr_range
      )
      {
        ranges += RetrieveDataSetBase::GetStartEndPositionOfRange( *itr_range, number_chunks, total_number_feature_results);
      }
      ranges = ranges.GetMappedSubset( subset_fixed);

      // keep track of how many rows have been loaded
      size_t features_so_far( START_FEATURE_NUMBER);

      // walk over the lines desired, loading only those columns and results that we want
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr_range( ranges.GetRanges().Begin()), itr_range_end( ranges.GetRanges().End());
        itr_range != itr_range_end;
        ++itr_range
      )
      {
        features_so_far +=
          m_Implementation->GenerateDataSubset
          (
            *itr_range,
            FEATURES_STORAGE,
            RESULTS_STORAGE,
            IDS_STORAGE,
            features_so_far
          );
      }

      return features_so_far - START_FEATURE_NUMBER;
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool RetrieveDataSetChunk::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      return true;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetChunk::GetNominalSize() const
    {
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());
      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, m_NumberChunks, total_number_feature_results)
      );

      return number_feature_results;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_combined.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetCombined::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetCombined())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetCombined *RetrieveDataSetCombined::Clone() const
    {
      return new RetrieveDataSetCombined( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetCombined::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetCombined::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the feature code for all the implementations
        ( *itr)->SelectFeatures( CODE);
      }
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetCombined::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the result code for all the implementations
        ( *itr)->SelectResults( CODE);
      }
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetCombined::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // set the id code for all the implementations
        ( *itr)->SelectIds( CODE);
      }
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetCombined::GetAlias() const
    {
      static const std::string s_Name( "Combined");
      return s_Name;
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetCombined::GetFeatureLabelsWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetCombined::GetResultCodeWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetCombined::GetIdCodeWithSizes() const
    {
      // test for null dataset
      if( m_Retrievers.IsEmpty())
      {
        return FeatureLabelSet();
      }

      // otherwise, get the feature label set from the first retriever (they should both be the same)
      return m_Retrievers.FirstElement()->GetIdCodeWithSizes();
    }

    //! @brief get whether dataset generation requires labels
    //! @return true if dataset generation requires labels
    bool RetrieveDataSetCombined::RequiresFeatureLabels() const
    {
      // iterate through the retrievers
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires feature labels
        if( ( *itr)->RequiresFeatureLabels())
        {
          // one retriever requires the feature labels, so return true
          return true;
        }
      }

      return false;
    }

    //! @brief get whether dataset generation requires result labels
    //! @return true if dataset generation requires result labels
    bool RetrieveDataSetCombined::RequiresResultLabels() const
    {
      // iterate through the retrievers
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires result labels
        if( ( *itr)->RequiresResultLabels())
        {
          // one retriever requires the result labels, so return true
          return true;
        }
      }

      return false;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetCombined::GenerateDataSet()
    {
      // determine the approximate size of the complete dataset
      const size_t full_dataset_size( GetNominalSize());

      // initialize a new data set
      util::ShPtr< descriptor::Dataset> dataset
      (
        new descriptor::Dataset
        (
          full_dataset_size,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      linal::MatrixReference< float> features( dataset->GetFeaturesReference());
      linal::MatrixReference< float> results( dataset->GetResultsReference());
      linal::MatrixReference< char> ids( dataset->GetIdsReference());

      size_t number_features_so_far( 0);
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // get the nominal size of this dataset
        const size_t nominal_dataset_size( ( *itr)->GetNominalSize());

        // put the data set from the next retriever directly into the dataset's matrices
        number_features_so_far +=
          ( *itr)->GenerateDataSubset
          (
            math::Range< size_t>
            (
              math::RangeBorders::e_LeftClosed,
              0,
              nominal_dataset_size,
              math::RangeBorders::e_RightOpen
            ),
            features,
            results,
            ids,
            number_features_so_far
          );
      }

      // remove unused rows
      dataset->ShrinkRows( number_features_so_far);

      // return the generated data set
      return dataset;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetCombined::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription( "concatenate data sets retrieved from each implementation");
      member_data.AddInitializer( "", "datasets to combine", io::Serialization::GetAgent( &m_Retrievers));

      return member_data;
    } // GetParameters

    //! @brief test whether this retriever can generate sub-ranges of datasets without loading the entire dataset
    //! @return true if this retriever can generate sub-ranges of datasets without loading the entire dataset
    bool RetrieveDataSetCombined::SupportsEfficientSubsetLoading() const
    {
      // test all internal datasets

      // test that each data set retriever supports efficient subset loading
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        if( !( *itr)->SupportsEfficientSubsetLoading())
        {
          return false;
        }
      }

      // all internal implementations support efficient subset loading and GetSize
      return true;
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetCombined::GetNumberPartitionsAndIds() const
    {
      if( m_Retrievers.IsEmpty())
      {
        return
          storage::Pair< size_t, math::RangeSet< size_t> >( 1, math::RangeSet< size_t>( math::Range< size_t>( 0, 0)));
      }
      storage::Pair< size_t, math::RangeSet< size_t> > initial
      (
        m_Retrievers.FirstElement()->GetNumberPartitionsAndIds()
      );
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin() + 1), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        // determine whether this retriever requires result labels
        if( ( *itr)->GetNumberPartitionsAndIds() != initial)
        {
          // different partitions, return undefined
          return
            storage::Pair< size_t, math::RangeSet< size_t> >( util::GetUndefined< size_t>(), math::RangeSet< size_t>());
        }
      }
      // all partitions are the same, return it
      return initial;
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded, must be large enough to hold the subset without resizing
    //! @param RESULTS_STORAGE where to store the corresponding results, must be large enough to hold the subset without resizing
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDataSetCombined::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      // ensure that features and results are the same size
      BCL_Assert
      (
        FEATURES_STORAGE.GetNumberRows() == RESULTS_STORAGE.GetNumberRows(),
        "Different number of features and results"
      );

      // ensure that the start position is within the storage matrix
      BCL_Assert
      (
        START_FEATURE_NUMBER <= FEATURES_STORAGE.GetNumberRows(),
        "start feature was higher than storage"
      );

      // standardize the range, so the left side of it has a closed border and the right has an open border
      const math::Range< size_t> standard_subset( SUBSET.StandardizeRange());

      // ensure that the end position is within the storage matrix
      BCL_Assert
      (
        START_FEATURE_NUMBER + standard_subset.GetWidth() <= FEATURES_STORAGE.GetNumberRows(),
        "Storage was too small to contain entire subset"
      );

      // standardize the range, so the left side of it has a closed border and the right has an open border
      size_t first_to_pick( standard_subset.GetMin());
      const size_t total_size( standard_subset.GetWidth());

      // keep track of the nominal and actual sizes
      // actual size may be smaller than nominal size, e.g. if some features could not be generated or were
      // of the wrong size
      size_t nominal_size_so_far( 0), actual_size_so_far( 0);

      // get the size from each of the internal datasets
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end && nominal_size_so_far < total_size;
        ++itr
      )
      {
        // use GetSize to decide whether to generate this dataset or not
        size_t size( ( *itr)->GetNominalSize());

        // if we have added any elements to the dataset yet, then continue adding them
        if( nominal_size_so_far != 0)
        {
          // just append the subset of interest
          actual_size_so_far +=
            ( *itr)->GenerateDataSubset
            (
              math::Range< size_t>
              (
                math::RangeBorders::e_LeftClosed,
                0,
                total_size - nominal_size_so_far,
                math::RangeBorders::e_RightOpen
              ),
              FEATURES_STORAGE,
              RESULTS_STORAGE,
              IDS_STORAGE,
              actual_size_so_far + START_FEATURE_NUMBER
            );
          nominal_size_so_far += size;
        }
        else
        {
          if( size <= first_to_pick)
          {
            // subtract size from first to pick and proceed to the next dataset
            first_to_pick -= size;
          }
          else
          {
            // store the new dataset directly in dataset
            actual_size_so_far =
              ( *itr)->GenerateDataSubset
              (
                math::Range< size_t>
                (
                  math::RangeBorders::e_LeftClosed,
                  first_to_pick,
                  std::min( size, total_size + first_to_pick),
                  math::RangeBorders::e_RightOpen
                ),
                FEATURES_STORAGE,
                RESULTS_STORAGE,
                IDS_STORAGE,
                START_FEATURE_NUMBER
              );
            nominal_size_so_far += size - first_to_pick;
            first_to_pick = 0;
          }
        }
      }

      return actual_size_so_far;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetCombined::GetNominalSize() const
    {
      size_t size( 0);
      // get the size from each of the internal datasets
      for
      (
        storage::Vector< util::Implementation< RetrieveDataSetBase> >::const_iterator
          itr( m_Retrievers.Begin()), itr_end( m_Retrievers.End());
        itr != itr_end;
        ++itr
      )
      {
        size += ( *itr)->GetNominalSize();
      }

      return size;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_encoded_by_model.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_reference.h"
#include "model/bcl_model_interface.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetEncodedByModel::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetEncodedByModel())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetEncodedByModel *RetrieveDataSetEncodedByModel::Clone() const
    {
      return new RetrieveDataSetEncodedByModel( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetEncodedByModel::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetEncodedByModel::GetAlias() const
    {
      static const std::string s_Name( "EncodeByModel");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetEncodedByModel::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Features.TryRead( CODE, util::GetLogger());
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetEncodedByModel::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetEncodedByModel::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetEncodedByModel::GetFeatureLabelsWithSizes() const
    {
      storage::Vector< util::ObjectDataLabel> labels;

      const size_t feature_size( m_Model->GetNumberOutputs());

      // create 1 feature label with size 1 for each feature
      storage::Vector< size_t> sizes( feature_size, 1);

      if( GetFeatureCode().GetNumberArguments() != feature_size)
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( feature_size);
        for( size_t feature_number( 0); feature_number < feature_size; ++feature_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( feature_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetFeatureCode().GetArguments();
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetEncodedByModel::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetEncodedByModel::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetEncodedByModel::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset and apply model to encode given features
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetEncodedByModel::GenerateDataSet()
    {
      util::ShPtr< descriptor::Dataset> dataset_sp( m_Implementation->GenerateDataSet());

      util::ShPtr< FeatureDataSet< float> > features( dataset_sp->GetFeaturesPtr());

      if( !m_Features.GetLabel().IsEmpty())
      {
        linal::Matrix< float> encoded_features( m_Model->operator ()( *features).GetMatrix());
        util::ShPtr< FeatureDataSet< float> > returned_features
        (
          new FeatureDataSet< float>
          (
            encoded_features.GetNumberRows(),
            m_Features.GetOutputFeatureSize(),
            float( 0)
          )
        );
        returned_features->SetFeatureLabelSet( GetFeatureLabelsWithSizes());

        linal::Matrix< float> &returned_features_ref( returned_features->GetRawMatrix());

        for( size_t row_id( 0); row_id < encoded_features.GetNumberRows(); ++row_id)
        {
          m_Features( encoded_features.GetRow( row_id), returned_features_ref.GetRow( row_id).Begin());
        }

        dataset_sp->SetFeatures( returned_features);
      }
      else
      {
        util::ShPtr< FeatureDataSet< float> > encoded_features( m_Model->operator ()( *features).Clone());
        encoded_features->SetFeatureLabelSet( GetFeatureLabelsWithSizes());
        dataset_sp->SetFeatures( encoded_features);
      }

      return dataset_sp;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetEncodedByModel::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Encode feature of dataset by given model");
      parameters.AddInitializer
      (
        "retriever",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      parameters.AddInitializer
      (
        "storage",
        "encoder storage directory name",
        io::Serialization::GetAgent( &m_ModelRetriever)
      );

      return parameters;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetEncodedByModel::GetNominalSize() const
    {
      return m_Implementation->GetNominalSize();
    }

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool RetrieveDataSetEncodedByModel::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      m_Implementation->SelectFeatures( m_ModelRetriever->RetrieveUniqueDescriptor());
      m_Model = m_ModelRetriever->RetrieveEnsemble().FirstElement();

      return true;
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_from_delimited_file.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetFromDelimitedFile::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetFromDelimitedFile())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    //! @param FILENAME the file to retrieve the data set from
    //! @param NUMBER_CHUNKS # of chunks the file should be split into (conceptually), used to divide the file into disparate datasets
    //! @param CHUNKS chunks to load
    RetrieveDataSetFromDelimitedFile::RetrieveDataSetFromDelimitedFile
    (
      const std::string &FILENAME,
      const size_t &NUMBER_RESULT_COLS,
      const size_t &NUMBER_ID_CHARS
    ) :
      m_Filename( FILENAME),
      m_Features(),
      m_Results(),
      m_Ids(),
      m_ResultsByNumberLastColumns( NUMBER_RESULT_COLS),
      m_NumberIdCharacters( NUMBER_ID_CHARS)
    {
    }

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetFromDelimitedFile *RetrieveDataSetFromDelimitedFile::Clone() const
    {
      return new RetrieveDataSetFromDelimitedFile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetFromDelimitedFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetFromDelimitedFile::GetAlias() const
    {
      static const std::string s_Name( "Csv");
      return s_Name;
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromDelimitedFile::GetFeatureLabelsWithSizes() const
    {
      storage::Vector< util::ObjectDataLabel> labels;

      const size_t feature_size( GetFeatureResultSizes().First());

      // create 1 feature label with size 1 for each feature
      storage::Vector< size_t> sizes( feature_size, 1);

      if( GetFeatureCode().GetNumberArguments() != feature_size)
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( feature_size);
        for( size_t feature_number( 0); feature_number < feature_size; ++feature_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( feature_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetFeatureCode().GetArguments();
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromDelimitedFile::GetResultCodeWithSizes() const
    {
      storage::Vector< util::ObjectDataLabel> labels;

      const size_t result_size( GetFeatureResultSizes().Second());

      // create 1 label with size 1 for each result
      storage::Vector< size_t> sizes( result_size, 1);

      if( GetResultCode().GetNumberArguments() != result_size)
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( result_size);
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( result_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetResultCode().GetArguments();
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromDelimitedFile::GetIdCodeWithSizes() const
    {
      if( !m_NumberIdCharacters)
      {
        return FeatureLabelSet();
      }
      storage::Vector< util::ObjectDataLabel> labels;

      // create 1 label with size 1 for each result
      storage::Vector< size_t> sizes( m_NumberIdCharacters, size_t( 1));

      if( GetIdCode().GetNumberArguments() != m_NumberIdCharacters)
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( m_NumberIdCharacters);
        for( size_t id_number( 0); id_number < m_NumberIdCharacters; ++id_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( id_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetIdCode().GetArguments();
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetFromDelimitedFile::GetNumberPartitionsAndIds() const
    {
      return storage::Pair< size_t, math::RangeSet< size_t> >( 1, math::RangeSet< size_t>( math::Range< size_t>( 0, 0)));
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetFromDelimitedFile::GenerateDataSet()
    {
      // determine the # of feature/results
      const size_t total_number_feature_results( GetTotalDatasetSize());

      static const std::string s_delimiters( " ,\t|");

      BCL_MessageDbg( "file: " + m_Filename + " total # features: " + util::Format()( total_number_feature_results));

      // open up the
      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      // allocate enough memory for the features that will be loaded
      util::ShPtr< descriptor::Dataset> dataset
      (
        new descriptor::Dataset
        (
          total_number_feature_results,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      linal::MatrixReference< float> features( dataset->GetFeaturesReference());
      linal::MatrixReference< float> results( dataset->GetResultsReference());
      linal::MatrixReference< char> ids( dataset->GetIdsReference());

      // read in the desired rows
      storage::VectorND< 2, linal::Vector< double> > temp_row;
      linal::Vector< float>
        temp_feature( features.GetNumberCols()), temp_result( results.GetNumberCols());

      // the results need to go through DataSetSelectColumns

      std::string temp_line;

      storage::Vector< float> temp_line_vector;

      const size_t results_position( temp_feature.GetSize());

      for( size_t row_number( 0); row_number < total_number_feature_results; ++row_number)
      {
        std::getline( input, temp_line);

        if( m_NumberIdCharacters)
        {
          // prune off the id
          std::copy( temp_line.begin(), temp_line.begin() + m_NumberIdCharacters, ids[ row_number]);
          BCL_Assert
          (
            s_delimiters.find( temp_line[ m_NumberIdCharacters]) != std::string::npos,
            "IDs should be fixed width, followed by a valid delimiter! Failed on line: " + util::Format()( row_number)
            + " with line: " + temp_line
          );
          temp_line = temp_line.substr( m_NumberIdCharacters + 1);
        }

        temp_line_vector = util::SplitStringToNumerical< float>( temp_line, s_delimiters);

        // determine
        storage::Vector< float>::iterator features_end_itr( temp_line_vector.Begin() + results_position);
        BCL_Assert
        (
          temp_line_vector.GetSize()
          ==
          m_Results.GetInputFeatureSize() + m_Features.GetInputFeatureSize()
          + features.GetNumberCols() + results.GetNumberCols(),
          "Line #" + util::Format()( row_number) + " (0-indexed) with incorrect number of values; had: "
          + util::Format()( temp_line_vector.GetSize()) + " but expected: " +
          util::Format()
          (
              m_Results.GetInputFeatureSize() + m_Features.GetInputFeatureSize()
              + features.GetNumberCols() + results.GetNumberCols()
           )
        );

        // filter the features, if only selecting some columns
        if( m_Features.GetInputFeatureSize())
        {
          // filter the result
          std::copy( temp_line_vector.Begin(), features_end_itr, temp_feature.Begin());
          m_Features( temp_feature, features[ row_number]);
        }
        else
        {
          std::copy( temp_line_vector.Begin(), features_end_itr, features[ row_number]);
        }

        // filter the result, if only selecting some columns
        if( m_Results.GetInputFeatureSize())
        {
          // filter the result
          std::copy( features_end_itr, temp_line_vector.End(), temp_result.Begin());
          m_Results( temp_result, results[ row_number]);
        }
        else
        {
          // copy all columns of the result
          std::copy( features_end_itr, temp_line_vector.End(), results[ row_number]);
        }
      }

      io::File::CloseClearFStream( input);

      // return the generated data set
      return dataset;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetFromDelimitedFile::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "retrieves a data set from a delimited value file. Possible delimiters are comma, tab, space, and pipe (|)"
      );

      member_data.AddInitializer
      (
        "filename",
        "name of the file from which to load the dataset",
        io::Serialization::GetAgentInputFilename( &m_Filename)
      );

      member_data.AddInitializer
      (
        "number result cols",
        "number of columns (to the right of the data set) that specify the results",
        io::Serialization::GetAgent( &m_ResultsByNumberLastColumns),
        "1"
      );

      member_data.AddInitializer
      (
        "number id chars",
        "number of characters (to the left of the data set) that specify the ids. "
        "There should be a delimiter (which is ignored) between the ids and features, if any ids are present",
        io::Serialization::GetAgent( &m_NumberIdCharacters),
        "0"
      );

      return member_data;
    } // GetParameters

    //! @brief get the size of the complete dataset without chunking
    //! @return the size of the complete dataset without chunking
    size_t RetrieveDataSetFromDelimitedFile::GetTotalDatasetSize() const
    {
      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      // search for search string, which will be the first line of the vector
      size_t counter( 0);
      std::string temp_string;
      while( input.good())
      {
        std::getline( input, temp_string);

        // if eof check is in while condition it will count eof as an additional line
        if( !input.eof())
        {
          ++counter;
        }
        else
        {
          break;
        }
      }

      // if there where no lines in your delimited data file
      if( counter == 0)
      {
        BCL_MessageStd( "No rows available in delimited data file!");
      }

      io::File::CloseClearFStream( input);

      return counter;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetFromDelimitedFile::GetNominalSize() const
    {
      return GetTotalDatasetSize();
    }

    //! @brief get the sizes of the features and results in the dataset without loading the whole dataset
    //! @return the sizes of the features and results in the dataset without loading the whole dataset
    storage::VectorND< 2, size_t> RetrieveDataSetFromDelimitedFile::GetFeatureResultSizes() const
    {
      // size of the dataset
      const size_t columns_size_feature( m_Features.GetOutputFeatureSize());
      const size_t columns_size_results( m_Results.GetOutputFeatureSize());

      if( m_Features.GetInputFeatureSize() && m_Results.GetInputFeatureSize())
      {
        // both the dataset select columns were enabled, return the selected sizes
        return storage::VectorND< 2, size_t>( columns_size_feature, columns_size_results);
      }

      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      storage::VectorND< 2, size_t> feature_result_sizes
      (
        util::GetUndefined< size_t>(),
        util::GetUndefined< size_t>()
      );

      // the vectors that follow contain the first features and results
      if( input.good() && !input.eof())
      {
        // extract line and tokenize
        std::string temp_vector;
        std::getline( input, temp_vector);
        if( !input.good() && util::TrimString( temp_vector).empty())
        {
          return storage::VectorND< 2, size_t>( columns_size_feature, columns_size_results);
        }
        if( m_NumberIdCharacters)
        {
          // prune off the id part, if any was present
          BCL_Assert( m_NumberIdCharacters < temp_vector.size() + 1, "More ids than characters on line: " + temp_vector);
          temp_vector = temp_vector.substr( m_NumberIdCharacters + 1);
        }
        storage::Vector< std::string> split_values( util::SplitString( temp_vector, " ,\t|"));

        BCL_Assert( m_ResultsByNumberLastColumns < split_values.GetSize(), "More results columns specified than columns available! Perhaps delimiter is wrong?");

        feature_result_sizes.First() = split_values.GetSize() - m_ResultsByNumberLastColumns;
        feature_result_sizes.Second() = m_ResultsByNumberLastColumns;
      }

      // override the given sizes by the sizes of the selects, if available
      if( m_Features.GetInputFeatureSize())
      {
        feature_result_sizes.First() = columns_size_feature;
      }

      if( m_Results.GetInputFeatureSize())
      {
        feature_result_sizes.Second() = columns_size_results;
      }

      // close file stream
      io::File::CloseClearFStream( input);

      return feature_result_sizes;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_from_file.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_statistics.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetFromFile::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetFromFile())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    //! @param FILENAME the file to retrieve the data set from
    //! @param NUMBER_CHUNKS # of chunks the file should be split into (conceptually), used to divide the file into disparate datasets
    //! @param CHUNKS chunks to load
    RetrieveDataSetFromFile::RetrieveDataSetFromFile
    (
      const std::string &FILENAME,
      const size_t &NUMBER_CHUNKS,
      const math::RangeSet< size_t> &CHUNKS
    ) :
      m_Filename( FILENAME),
      m_NumberChunks( NUMBER_CHUNKS),
      m_ChunkRanges( CHUNKS)
    {
    }

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetFromFile *RetrieveDataSetFromFile::Clone() const
    {
      return new RetrieveDataSetFromFile( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetFromFile::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetFromFile::GetAlias() const
    {
      static const std::string s_Name( "File");
      return s_Name;
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromFile::GetFeatureLabelsWithSizes() const
    {
      storage::Vector< util::ObjectDataLabel> labels;

      const size_t feature_size( GetFeatureResultSizes().First());

      // create 1 feature label with size 1 for each feature
      storage::Vector< size_t> sizes( feature_size, 1);

      if( GetFeatureCode().GetNumberArguments() == size_t( 0))
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( feature_size);
        for( size_t feature_number( 0); feature_number < feature_size; ++feature_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( feature_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetFeatureCode().GetArguments();
        sizes.Resize( labels.GetSize());
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromFile::GetResultCodeWithSizes() const
    {
      storage::Vector< util::ObjectDataLabel> labels;
      const storage::VectorND< 2, size_t> feature_result_sizes( GetFeatureResultSizes());
      const size_t original_feature_columns
      (
        m_Features.GetInputFeatureSize()
        ? m_Features.GetInputFeatureSize()
        : feature_result_sizes.First()
      );
      const size_t result_size( feature_result_sizes.Second());

      // create 1 label with size 1 for each result
      storage::Vector< size_t> sizes( result_size, 1);

      if( GetResultCode().GetNumberArguments() == size_t( 0))
      {
        // the labels will just be the index of each feature
        labels.AllocateMemory( result_size);
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          labels.PushBack( util::ObjectDataLabel( util::Format()( original_feature_columns + result_number)));
        }
      }
      else
      {
        // use the given labels
        labels = GetResultCode().GetArguments();
        sizes.Resize( labels.GetSize());
      }

      return FeatureLabelSet( "Combine", labels, sizes);
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    void RetrieveDataSetFromFile::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      const size_t original_feature_columns
      (
        m_Features.GetInputFeatureSize()
        ? m_Features.GetInputFeatureSize()
        : GetFeatureResultSizes().First()
      );
      if( CODE.GetNumberArguments() == original_feature_columns)
      {
        return;
      }
      storage::Vector< size_t>
        indices( util::SplitStringToNumerical< size_t>( CODE.ArgumentsToString( false, ','), ","));
      BCL_Assert
      (
        math::Statistics::MaximumValue( indices.Begin(), indices.End()) < original_feature_columns,
        "SelectFeatures given index beyond end of array"
      );
      m_Features = DataSetSelectColumns( original_feature_columns, indices);
      RetrieveDataSetBase::SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @param CODE the new code
    void RetrieveDataSetFromFile::SelectResults( const util::ObjectDataLabel &CODE)
    {
      const size_t original_feature_columns
      (
        m_Features.GetInputFeatureSize()
        ? m_Features.GetInputFeatureSize()
        : GetFeatureResultSizes().First()
      );
      const size_t original_result_columns
      (
        m_Results.GetInputFeatureSize()
        ? m_Results.GetInputFeatureSize()
        : GetFeatureResultSizes().Second()
      );
      if( CODE.GetNumberArguments() == original_result_columns)
      {
        return;
      }
      storage::Vector< size_t>
        indices( util::SplitStringToNumerical< size_t>( CODE.ArgumentsToString( false, ','), ","));
      BCL_Assert
      (
        math::Statistics::MaximumValue( indices.Begin(), indices.End())
          < original_feature_columns + original_result_columns,
        "SelectResults given index beyond end of array"
      );
      BCL_Assert
      (
        math::Statistics::MinimumValue( indices.Begin(), indices.End()) >= original_feature_columns,
        "SelectResults given feature index"
      );
      linal::VectorReference< size_t> indices_ref( indices.GetSize(), &*indices.Begin());
      indices_ref -= original_feature_columns;
      m_Features = DataSetSelectColumns( original_result_columns, indices);
      RetrieveDataSetBase::SelectResults( CODE);
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetFromFile::GetIdCodeWithSizes() const
    {
      // files currently have no way of storing ids
      return FeatureLabelSet();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetFromFile::GetNumberPartitionsAndIds() const
    {
      return storage::Pair< size_t, math::RangeSet< size_t> >( m_NumberChunks, m_ChunkRanges);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetFromFile::GenerateDataSet()
    {
      if( m_NumberChunks > 1)
      {
        // ensure the range set does not exceed the # of ranges
        BCL_Assert
        (
          m_ChunkRanges.GetMax() < m_NumberChunks,
          GetClassIdentifier() + " received range set with max >= # ranges! # ranges = " + util::Format()( m_NumberChunks)
          + " " + m_ChunkRanges.AsString()
        );

        // ensure the range set is not empty
        BCL_Assert
        (
          !m_ChunkRanges.IsEmpty(),
          GetClassIdentifier() + " received empty range set"
        );
      }
      else
      {
        m_NumberChunks = 1;
        m_ChunkRanges = math::RangeSet< size_t>( math::Range< size_t>( 0, 0));
      }

      // determine the # of feature/results
      const size_t total_number_feature_results( GetTotalDatasetSize());

      BCL_MessageDbg( "file: " + m_Filename + " total # features: " + util::Format()( total_number_feature_results));

      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, m_NumberChunks, total_number_feature_results)
      );

      // create a vector that will store bools that indicate whether or not to load each row
      storage::Vector< size_t> should_load_row( total_number_feature_results, size_t( 0));

      // set the vector up with the given ranges
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr( m_ChunkRanges.GetRanges().Begin()), itr_end( m_ChunkRanges.GetRanges().End());
        itr != itr_end;
        ++itr
      )
      {
        math::Range< size_t> actual_range
        (
          RetrieveDataSetBase::GetStartEndPositionOfRange( *itr, m_NumberChunks, total_number_feature_results)
        );
        for
        (
          size_t row_id( actual_range.GetMin()), max_row_id( actual_range.GetMax());
          row_id < max_row_id;
          ++row_id
        )
        {
          should_load_row( row_id) = 1;
        }
      }

      // open up the
      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      // look for this string; the number that follows is the total size
      const std::string search_string
      (
        GetStaticClassName< storage::VectorND< 2, linal::Vector< double> > >()
      );

      storage::VectorND< 2, size_t> feature_result_sizes( GetFeatureResultSizes());

      // search for search string, which will be the first line of the vector
      std::string temp_string;
      while( input.good() && !input.eof())
      {
        std::streampos old_position( input.tellg());
        std::getline( input, temp_string);
        if( util::TrimString( temp_string) == search_string)
        {
          // move back to the position right before the search string
          input.seekg( old_position);
          break;
        }
      }

      // allocate enough memory for the features that will be loaded
      util::ShPtr< descriptor::Dataset> dataset
      (
        new descriptor::Dataset
        (
          number_feature_results,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      linal::MatrixReference< float> features( dataset->GetFeaturesReference());
      linal::MatrixReference< float> results( dataset->GetResultsReference());

      // read in the desired rows
      storage::VectorND< 2, linal::Vector< double> > temp_row;
      linal::Vector< float>
        temp_feature( m_Features.GetInputFeatureSize()), temp_result( m_Results.GetInputFeatureSize());
      // the results need to go through DataSetSelectColumns
      for( size_t row_number( 0), feature_number( 0); row_number < total_number_feature_results; ++row_number)
      {
        input >> temp_row;
        if( should_load_row( row_number))
        {
          // filter the features, if only selecting some columns
          if( m_Features.GetInputFeatureSize())
          {
            // filter the result
            std::copy( temp_row.First().Begin(), temp_row.First().End(), temp_feature.Begin());
            m_Features( temp_feature, features[ feature_number]);
          }
          else
          {
            std::copy( temp_row.First().Begin(), temp_row.First().End(), features[ feature_number]);
          }

          // filter the result, if only selecting some columns
          if( m_Results.GetInputFeatureSize())
          {
            // filter the result
            std::copy( temp_row.Second().Begin(), temp_row.Second().End(), temp_result.Begin());
            m_Results( temp_result, results[ feature_number]);
          }
          else
          {
            // copy all columns of the result
            std::copy( temp_row.Second().Begin(), temp_row.Second().End(), results[ feature_number]);
          }
          ++feature_number;
        }
      }

      io::File::CloseClearFStream( input);

      // return the generated data set
      return dataset;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetFromFile::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription
      (
        "retrieves a data set from a file containing a single " +
        GetStaticClassName< storage::Vector< storage::VectorND< 2, linal::Vector< double> > > >()
      );

      member_data.AddInitializer
      (
        "filename",
        "name of the file from which to load the dataset",
        io::Serialization::GetAgentInputFilename( &m_Filename)
      );

      member_data.AddInitializer
      (
        "number chunks",
        "# of chunks the file should be split into (conceptually), used to divide the file into disparate datasets",
        io::Serialization::GetAgent( &m_NumberChunks),
        "1"
      );

      member_data.AddInitializer
      (
        "chunks",
        "ranges of chunks to load, e.g. chunks=\"[ 0, 5) (7,10)\"",
        io::Serialization::GetAgent( &m_ChunkRanges),
        "[0]"
      );

      return member_data;
    } // GetParameters

    //! @brief get the size of the complete dataset without chunking
    //! @return the size of the complete dataset without chunking
    size_t RetrieveDataSetFromFile::GetTotalDatasetSize() const
    {
      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      // look for this string; the number that follows is the total size
      const std::string search_string
      (
        GetStaticClassName< storage::Vector< storage::VectorND< 2, linal::Vector< double> > > >()
      );

      // search for search string, which will be the first line of the vector
      std::string temp_string;
      while( input.good() && !input.eof())
      {
        std::getline( input, temp_string);
        if( util::TrimString( temp_string) == search_string)
        {
          break;
        }
      }

      // the number that follows is the total size
      size_t size( 0);
      if( input.good() && !input.eof())
      {
        input >> size;
      }
      else
      {
        BCL_MessageCrt
        (
          "Dataset will be empty, could not find the expected dataset class identifier: " + search_string
        );
      }

      io::File::CloseClearFStream( input);

      return size;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetFromFile::GetNominalSize() const
    {
      const size_t total_dataset_size( GetTotalDatasetSize());
      return RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, m_NumberChunks, total_dataset_size);
    }

    //! @brief get the sizes of the features and results in the dataset without loading the whole dataset
    //! @return the sizes of the features and results in the dataset without loading the whole dataset
    storage::VectorND< 2, size_t> RetrieveDataSetFromFile::GetFeatureResultSizes() const
    {
      // first, check if only selected columns have been enabled; if so, take those sizes rather than the complete
      // size of the dataset
      const size_t columns_size_feature( m_Features.GetOutputFeatureSize());
      const size_t columns_size_results( m_Results.GetOutputFeatureSize());

      if( m_Features.GetInputFeatureSize() && m_Results.GetInputFeatureSize())
      {
        // both the dataset select columns were enabled, return the selected sizes
        return storage::VectorND< 2, size_t>( columns_size_feature, columns_size_results);
      }

      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_Filename);

      // look for this string; the number that follows is the nominal size
      const std::string search_string
      (
        GetStaticClassName< storage::VectorND< 2, linal::Vector< double> > >()
      );

      // search for search string, which will be the first line of the vector
      std::string temp_string;
      while( input.good() && !input.eof())
      {
        std::getline( input, temp_string);
        if( util::TrimString( temp_string) == search_string)
        {
          break;
        }
      }

      storage::VectorND< 2, size_t> feature_result_sizes
      (
        util::GetUndefined< size_t>(),
        util::GetUndefined< size_t>()
      );

      // the vectors that follow contain the first features and results
      if( input.good() && !input.eof())
      {
        linal::Vector< double> temp_vector;
        input >> temp_vector;
        feature_result_sizes.First() = temp_vector.GetSize();
        input >> temp_vector;
        feature_result_sizes.Second() = temp_vector.GetSize();
      }

      // override the given sizes by the sizes of the selects, if available
      if( m_Features.GetInputFeatureSize())
      {
        feature_result_sizes.First() = columns_size_feature;
      }

      if( m_Results.GetInputFeatureSize())
      {
        feature_result_sizes.Second() = columns_size_results;
      }

      io::File::CloseClearFStream( input);

      return feature_result_sizes;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_join.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_file.h"
#include "io/bcl_io_ifstream.h"
#include "io/bcl_io_serialization.h"
#include "storage/bcl_storage_triplet.h"
// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetJoin::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetJoin())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetJoin *RetrieveDataSetJoin::Clone() const
    {
      return new RetrieveDataSetJoin( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetJoin::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetJoin::GetAlias() const
    {
      static const std::string s_Name( "Join");
      return s_Name;
    }

    void RetrieveDataSetJoin::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetJoin::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetJoin::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
//      m_LHS->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetJoin::GetFeatureLabelsWithSizes() const
    {
//      FeatureLabelSet label_set( m_LHS->GetFeatureLabelsWithSizes());
//      label_set.
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetJoin::GetResultCodeWithSizes() const
    {

//      return GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetJoin::GetIdCodeWithSizes() const
    {
      return m_LHS->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetJoin::GetNumberPartitionsAndIds() const
    {
      return m_LHS->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetJoin::GenerateDataSet()
    {
      UpdateIDsIndex();
      // determine the approximate size of the complete dataset after join
      const size_t full_dataset_size( GetNominalSize());

      // we will need to map id label pairs to results
      storage::Vector< storage::Triplet< std::string, std::string, linal::Vector< float> > > id_pairs_to_result;
      std::string temp_line;
      storage::Vector< std::string> temp_line_vector;
      storage::Pair< std::string, std::string> temp_pair;

      // read id labels corresponding to datasets to be joined
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_IdPairsFileName);
      for( size_t id_row_number( 0); id_row_number < full_dataset_size; ++id_row_number)
      {
        // parse each line for datasets to be joined
        std::getline( input, temp_line);
        id_pairs_to_result.PushBack();
        temp_line_vector = util::SplitString( temp_line, ",\r\n");
        id_pairs_to_result.LastElement().First() = temp_line_vector( 0);
        id_pairs_to_result.LastElement().Second() = temp_line_vector( 1);

        linal::Vector< float> &temp_result_vector( id_pairs_to_result.LastElement().Third());
        temp_result_vector = linal::Vector< float>( temp_line_vector.GetSize() - 2);
        BCL_Assert
        (
          temp_result_vector.GetSize() == id_pairs_to_result.FirstElement().Third().GetSize(),
          "Different # of results for various id pairs on line #" + util::Format()( id_row_number + 1)
        );
        for( size_t i( 0), sz( temp_result_vector.GetSize()); i < sz; ++i)
        {
          temp_result_vector( i) = util::ConvertStringToNumericalValue< float>( temp_line_vector( i));
        }
      }
      const size_t result_size( id_pairs_to_result.LastElement().Third().GetSize());

      // close the input file stream
      io::File::CloseClearFStream( input);

      util::ShPtr< descriptor::Dataset> lhs_dataset_ptr( m_LHS->GenerateDataSet());
      util::ShPtr< descriptor::Dataset> rhs_dataset_ptr( m_RHS->GenerateDataSet());

      storage::Map< std::string, storage::Pair< std::string, size_t>> lhs_map_property_to_index, rhs_map_property_to_index;
      BCL_Assert( m_LHSIdIndices.LastElement() < lhs_dataset_ptr->GetIdsReference().GetNumberCols(), "LHS out of bounds id column!");
      BCL_Assert( m_RHSIdIndices.LastElement() < rhs_dataset_ptr->GetIdsReference().GetNumberCols(), "RHS out of bounds id column!");
      const size_t lhs_n_id_cols( m_LHSIdIndices.GetSize()), rhs_n_id_cols( m_RHSIdIndices.GetSize());
      std::string lhs_tmp( m_LHSIdIndices.GetSize(), ' '), rhs_tmp( m_RHSIdIndices.GetSize(), ' ');

      for( size_t row( 0), n_rows( lhs_dataset_ptr->GetIdsReference().GetNumberRows()); row < n_rows; ++row)
      {
        // copy the desired character into the temp string
        const char *row_ptr( lhs_dataset_ptr->GetIdsReference()[ row]);
        for( size_t col( 0); col < lhs_n_id_cols; ++col)
        {
          lhs_tmp[ col] = row_ptr[ m_LHSIdIndices( col)];
        }
        lhs_map_property_to_index[ util::TrimString( lhs_tmp)] = storage::Pair< std::string, size_t>( lhs_tmp, row);
      }
      for( size_t row( 0), n_rows( rhs_dataset_ptr->GetIdsReference().GetNumberRows()); row < n_rows; ++row)
      {
        // copy the desired character into the temp string
        const char *row_ptr( rhs_dataset_ptr->GetIdsReference()[ row]);
        for( size_t col( 0); col < rhs_n_id_cols; ++col)
        {
          rhs_tmp[ col] = row_ptr[ m_RHSIdIndices( col)];
        }
        rhs_map_property_to_index[ util::TrimString( rhs_tmp)] = storage::Pair< std::string, size_t>( rhs_tmp, row);
      }

      // initialize a new data set
      util::ShPtr< descriptor::Dataset> dataset
      (
        new descriptor::Dataset
        (
          id_pairs_to_result.GetSize(),
          lhs_dataset_ptr->GetFeatureSize() + rhs_dataset_ptr->GetFeatureSize(),
          result_size,
          m_RHSIdIndices.GetSize() + m_LHSIdIndices.GetSize()
        )
      );

      for( size_t i( 0); i < id_pairs_to_result.GetSize(); ++i)
      {
        auto row( dataset->GetFeaturesReference().GetRow( i));
        auto &lhs_index_buffstring( lhs_map_property_to_index.GetValue( id_pairs_to_result( i).First()));
        row.CreateSubVectorReference( lhs_dataset_ptr->GetFeatureSize(), 0).CopyValues( lhs_dataset_ptr->GetFeaturesReference().GetRow( lhs_index_buffstring.Second()));
        auto &rhs_index_buffstring( rhs_map_property_to_index.GetValue( id_pairs_to_result( i).Second()));
        row.CreateSubVectorReference( rhs_dataset_ptr->GetFeatureSize(), lhs_dataset_ptr->GetFeatureSize()).CopyValues
        (
          rhs_dataset_ptr->GetFeaturesReference().GetRow( rhs_index_buffstring.Second())
        );
        auto result_row( dataset->GetResultsReference().GetRow( i));
        result_row.CopyValues( id_pairs_to_result( i).Third());
      }

      // return the generated data set
      return dataset;
    } // GenerateDataSet

    //! @brief get the size of the complete dataset without chunking
    //! @return the size of the complete dataset without chunking
    size_t RetrieveDataSetJoin::GetTotalDatasetSize() const
    {
      // read the data set from the file
      io::IFStream input;
      io::File::MustOpenIFStream( input, m_IdPairsFileName);

      // search for search string, which will be the first line of the vector
      size_t counter( 0);
      std::string temp_string;
      while( input.good())
      {
        std::getline( input, temp_string);

        // if eof check is in while condition it will count eof as an additional line
        if( !input.eof())
        {
          ++counter;
        }
        else
        {
          break;
        }
      }

      // if there where no lines in your delimited data file
      if( counter == 0)
      {
        BCL_MessageStd( "No rows available in delimited data file!");
      }

      io::File::CloseClearFStream( input);

      return counter;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetJoin::GetNominalSize() const
    {
      return GetTotalDatasetSize();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetJoin::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Join two datasets with a pair-specific result value");
      parameters.AddInitializer
      (
        "id pairs filename",
        "fixed-width label for each dataset pair (e.g. for joining protein-ligand datasets this may indicate the target)",
        io::Serialization::GetAgent( &m_IdPairsFileName)
      );

      parameters.AddInitializer
      (
        "lhs",
        "dataset retriever to call to get the LHS",
        io::Serialization::GetAgent( &m_LHS)
      );

      parameters.AddInitializer
      (
        "lhs id label",
        "LHS data label",
        io::Serialization::GetAgent( &m_LHSLabel)
      );

      parameters.AddInitializer
      (
        "rhs",
        "dataset retriever to call to get the RHS",
        io::Serialization::GetAgent( &m_RHS)
      );

      parameters.AddInitializer
      (
        "rhs id label",
        "RHS data label",
        io::Serialization::GetAgent( &m_RHSLabel)
      );

      parameters.AddInitializer
      (
        "result label",
        "Result data label",
        io::Serialization::GetAgent( &m_ResultLabel)
      );

      return parameters;
    } // GetParameters

    void RetrieveDataSetJoin::UpdateIDsIndex()
    {
      if( m_LHSIdIndices.IsEmpty())
      {
        m_LHSIdIndices = m_LHS->GetIdCodeWithSizes().GetPropertyIndices( m_LHSLabel);
      }
      if( m_RHSIdIndices.IsEmpty())
      {
        m_RHSIdIndices = m_RHS->GetIdCodeWithSizes().GetPropertyIndices( m_RHSLabel);
      }

    }

    bool RetrieveDataSetJoin::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      return true;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_randomized.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetRandomized::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetRandomized())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetRandomized *RetrieveDataSetRandomized::Clone() const
    {
      return new RetrieveDataSetRandomized( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetRandomized::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetRandomized::GetAlias() const
    {
      static const std::string s_Name( "Randomize");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetRandomized::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetRandomized::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetRandomized::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRandomized::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRandomized::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRandomized::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetRandomized::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetRandomized::GenerateDataSet()
    {
      util::ShPtr< descriptor::Dataset> dataset_sp( m_Implementation->GenerateDataSet());
      dataset_sp->Shuffle();
      return dataset_sp;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetRandomized::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Randomizes feature order within the dataset");
      parameters.AddInitializer
      (
        "",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      return parameters;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetRandomized::GetNominalSize() const
    {
      return m_Implementation->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_rescaled.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetRescaled::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetRescaled())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    RetrieveDataSetRescaled::RetrieveDataSetRescaled() :
      m_RescaleInputType( RescaleFeatureDataSet::e_None),
      m_RescaleInputRange( -1, 1),
      m_RescaleOutputType( RescaleFeatureDataSet::e_None),
      m_RescaleOutputRange( -1, 1)
    {
    }

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetRescaled *RetrieveDataSetRescaled::Clone() const
    {
      return new RetrieveDataSetRescaled( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetRescaled::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetRescaled::GetAlias() const
    {
      static const std::string s_Name( "Rescale");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetRescaled::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetRescaled::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetRescaled::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRescaled::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRescaled::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRescaled::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetRescaled::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetRescaled::GenerateDataSet()
    {
      util::ShPtr< descriptor::Dataset> dataset_sp( m_Implementation->GenerateDataSet());
      dataset_sp->GetFeatures().Rescale( m_RescaleInputRange, m_RescaleInputType);
      dataset_sp->GetResults().Rescale( m_RescaleOutputRange, m_RescaleOutputType);

      return dataset_sp;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetRescaled::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Rescales a dataset. This is *only* useful for combining datasets that have different logical units or scales. "
        "For training machine learning models, set the rescaling within the model itself"
      );
      parameters.AddInitializer
      (
        "",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );
      parameters.AddInitializer
      (
        "input scaling",
        "Type of input scaling. Normally AveStd works best, but MinMax and None may also be used in some circumstances",
        io::Serialization::GetAgent( &m_RescaleInputType),
        "AveStd"
      );
      parameters.AddInitializer
      (
        "output scaling",
        "Type of input scaling. Normally AveStd works best, but MinMax and None may also be used in some circumstances",
        io::Serialization::GetAgent( &m_RescaleOutputType),
        "None"
      );
      parameters.AddInitializer
      (
        "rescale input to",
        "rescale range. All input features will be rescaled to this range unless input scaling=None",
        io::Serialization::GetAgent( &m_RescaleInputRange),
        "\"[-1,1]\""
      );
      parameters.AddInitializer
      (
        "rescale output to",
        "rescale range. All results will be rescaled to this range unless output scaling=None",
        io::Serialization::GetAgent( &m_RescaleOutputRange),
        "\"[-1,1]\""
      );

      return parameters;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetRescaled::GetNominalSize() const
    {
      return m_Implementation->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_rows.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetRows::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetRows())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetRows *RetrieveDataSetRows::Clone() const
    {
      return new RetrieveDataSetRows( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetRows::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetRows::GetAlias() const
    {
      static const std::string s_Name( "Rows");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetRows::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetRows::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetRows::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRows::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRows::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetRows::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetRows::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetRows::GenerateDataSet()
    {
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());

      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, total_number_feature_results, total_number_feature_results)
      );

      // initialize a new data set
      util::ShPtr< descriptor::Dataset> complete
      (
        new descriptor::Dataset
        (
          number_feature_results,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      linal::MatrixReference< float> features( complete->GetFeaturesReference());
      linal::MatrixReference< float> results( complete->GetResultsReference());
      linal::MatrixReference< char> ids( complete->GetIdsReference());

      // set the vector up with the given ranges
      size_t features_so_far( 0);
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr( m_ChunkRanges.GetRanges().Begin()), itr_end( m_ChunkRanges.GetRanges().End());
        itr != itr_end;
        ++itr
      )
      {
        features_so_far +=
          m_Implementation->GenerateDataSubset( itr->StandardizeRange(), features, results, ids, features_so_far);
      }

      complete->ShrinkRows( features_so_far);

      return complete;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetRows::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Retrieves row ranges of a specific dataset");
      parameters.AddInitializer
      (
        "rows",
        "ranges of rows to load, e.g. rows=\"[ 0, 5) (7,10)\"",
        io::Serialization::GetAgent( &m_ChunkRanges),
        "\"[0]\""
      );
      parameters.AddInitializer
      (
        "dataset",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      return parameters;
    } // GetParameters

    //! @brief Set the members of this object from the given LABEL
    //! @param LABEL the label containing members that should be read of this class
    //! @param ERROR_STREAM stream with which to write errors
    bool RetrieveDataSetRows::ReadInitializerSuccessHook( const util::ObjectDataLabel &LABEL, std::ostream &ERROR_STREAM)
    {
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());

      // if range [0,0] is given assume user wants to use entire data set
      if( m_ChunkRanges.IsEmpty())
      {
        m_ChunkRanges += math::Range< size_t>( 0, total_number_feature_results);
      }

      // eliminate range above total_number_feature_results
      if( m_ChunkRanges.GetMax() >= total_number_feature_results)
      {
        m_ChunkRanges -= math::Range< size_t>( total_number_feature_results, m_ChunkRanges.GetMax());
      }
      return true;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetRows::GetNominalSize() const
    {
      const size_t total_number_feature_results( m_Implementation->GetNominalSize());
      // determine the # of features/results in the set of data that will be loaded
      const size_t number_feature_results
      (
        RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, total_number_feature_results, total_number_feature_results)
      );

      return number_feature_results;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_data_set_yscramble.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDataSetYscramble::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDataSetYscramble())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDataSetYscramble *RetrieveDataSetYscramble::Clone() const
    {
      return new RetrieveDataSetYscramble( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDataSetYscramble::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDataSetYscramble::GetAlias() const
    {
      static const std::string s_Name( "YScramble");
      return s_Name;
    }

    //! @brief Set the code / label for the feature (1st part) of the data set
    //! @param CODE the new code
    //! @return the code / label for the feature (1st part) of the data set
    void RetrieveDataSetYscramble::SelectFeatures( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectFeatures( CODE);
      m_Implementation->SelectFeatures( CODE);
    }

    //! @brief Set the code / label for the result (2nd part) of the data set
    //! @return the code / label for the result (2nd part) of the data set
    void RetrieveDataSetYscramble::SelectResults( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectResults( CODE);
      m_Implementation->SelectResults( CODE);
    }

    //! @brief Set which id columns to retrieve
    //! @param CODE the id column names to retrieve
    void RetrieveDataSetYscramble::SelectIds( const util::ObjectDataLabel &CODE)
    {
      RetrieveDataSetBase::SelectIds( CODE);
      m_Implementation->SelectIds( CODE);
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetYscramble::GetFeatureLabelsWithSizes() const
    {
      return m_Implementation->GetFeatureLabelsWithSizes();
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetYscramble::GetResultCodeWithSizes() const
    {
      return m_Implementation->GetResultCodeWithSizes();
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDataSetYscramble::GetIdCodeWithSizes() const
    {
      return m_Implementation->GetIdCodeWithSizes();
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDataSetYscramble::GetNumberPartitionsAndIds() const
    {
      return m_Implementation->GetNumberPartitionsAndIds();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDataSetYscramble::GenerateDataSet()
    {
      util::ShPtr< descriptor::Dataset> dataset_sp( m_Implementation->GenerateDataSet());
      dataset_sp->YScramble();
      return dataset_sp;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDataSetYscramble::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Randomizes order of results in the dataset. "
        "This allows for validation (beyond cross-validation) of feature selection and training methods on a given dataset. "
        "If the y-scrambled dataset yields similar results upon training as the unperturbed dataset, then the dataset is "
        "unreliable for training"
      );
      parameters.AddInitializer
      (
        "",
        "dataset retriever to call to get the entire data set",
        io::Serialization::GetAgent( &m_Implementation)
      );

      return parameters;
    } // GetParameters

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDataSetYscramble::GetNominalSize() const
    {
      return m_Implementation->GetNominalSize();
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_dataset_subset.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_binary_serialize.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_thunk_job.h"

// external includes - sorted alphabetically
#include "bcl_version.h"
#include <fstream>

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> RetrieveDatasetSubset::s_Instance
    (
      util::Enumerated< RetrieveDataSetBase>::AddInstance( new RetrieveDatasetSubset())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    //! @param FILENAME the file to retrieve the data set from
    //! @param NUMBER_CHUNKS # of chunks the file should be split into (conceptually), used to divide the file into disparate datasets
    //! @param CHUNKS chunks to load
    RetrieveDatasetSubset::RetrieveDatasetSubset
    (
      const std::string &FILENAME,
      const size_t &NUMBER_CHUNKS,
      const math::RangeSet< size_t> &CHUNKS
    ) :
      m_Filename( FILENAME),
      m_NumberChunks( NUMBER_CHUNKS),
      m_ChunkRanges( CHUNKS)
    {
    }

    //! @brief Clone is the virtual Clone constructor
    //! @return a pointer to new LocatorAtomFurthest which is a copy of this
    RetrieveDatasetSubset *RetrieveDatasetSubset::Clone() const
    {
      return new RetrieveDatasetSubset( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &RetrieveDatasetSubset::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief returns the name used for this class in an object data label
    //! @return the name used for this class in an object data label
    const std::string &RetrieveDatasetSubset::GetAlias() const
    {
      static const std::string s_Name( "Subset");
      return s_Name;
    }

    //! @brief Get the code / label set for the feature (1st part) of the data set with sizes of each property
    //! @return the code / label for the feature (1st part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDatasetSubset::GetFeatureLabelsWithSizes() const
    {
      std::ifstream input;
      input.open( m_Filename.c_str(), std::ios::binary | std::ios::in);
      FeatureLabelSet all_feature_labels( ReadFeatureLabelSetBinary( input));
      io::File::CloseClearFStream( input);

      // determine which features were actually selected
      storage::Vector< size_t> feature_sizes;

      // check; if no features were given, then set all features
      if( GetFeatureCode().IsScalar())
      {
        return all_feature_labels;
      }
      feature_sizes.AllocateMemory( GetFeatureCode().GetNumberArguments());

      // determine the size of each result that was selected
      for
      (
        util::ObjectDataLabel::const_iterator
          itr_label( GetFeatureCode().Begin()),
          itr_label_end( GetFeatureCode().End());
        itr_label != itr_label_end;
        ++itr_label
      )
      {
        // find this feature label in the feature_labels object
        feature_sizes.PushBack( all_feature_labels.GetPropertyIndices( *itr_label).GetSize());
      }

      return FeatureLabelSet( all_feature_labels.GetAlias(), GetFeatureCode().GetArguments(), feature_sizes);
    }

    //! @brief Get the code / label for the result (2nd part) of the data set with sizes of each property
    //! @return the code / label for the result (2nd part) of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDatasetSubset::GetResultCodeWithSizes() const
    {
      std::ifstream input;
      input.open( m_Filename.c_str(), std::ios::binary | std::ios::in);
      // skip the feature labels
      ReadFeatureLabelSetBinary( input);
      // read the result labels
      FeatureLabelSet all_result_labels( ReadFeatureLabelSetBinary( input));
      io::File::CloseClearFStream( input);

      // check; if no result code was given, then return the entire result code object
      if( GetResultCode().IsScalar())
      {
        return all_result_labels;
      }

      // determine which results were actually selected
      storage::Vector< size_t> results_sizes;
      results_sizes.AllocateMemory( GetResultCode().GetNumberArguments());

      // determine the size of each result that was selected
      for
      (
        util::ObjectDataLabel::const_iterator
          itr_label( GetResultCode().Begin()),
          itr_label_end( GetResultCode().End());
        itr_label != itr_label_end;
        ++itr_label
      )
      {
        // find this feature label in the feature_labels object
        results_sizes.PushBack( all_result_labels.GetPropertyIndices( *itr_label).GetSize());
      }

      return FeatureLabelSet( all_result_labels.GetAlias(), GetResultCode().GetArguments(), results_sizes);
    }

    //! @brief Get the code / label for the ids of the data set with sizes of each property
    //! @return the code / label for the ids of the data set with sizes of each property
    //! the feature code set
    FeatureLabelSet RetrieveDatasetSubset::GetIdCodeWithSizes() const
    {
      std::ifstream input;
      input.open( m_Filename.c_str(), std::ios::binary | std::ios::in);
      // skip the feature labels
      ReadFeatureLabelSetBinary( input);
      // skip the result labels
      ReadFeatureLabelSetBinary( input);

      // read the id labels
      FeatureLabelSet all_id_labels( ReadFeatureLabelSetBinary( input));
      io::File::CloseClearFStream( input);

      // check; if no id code was given, then return the entire result code object
      if( GetIdCode().IsScalar())
      {
        return all_id_labels;
      }

      // determine which results were actually selected
      storage::Vector< size_t> id_sizes;
      id_sizes.AllocateMemory( GetIdCode().GetNumberArguments());

      // determine the size of each result that was selected
      for
      (
        util::ObjectDataLabel::const_iterator itr_label( GetIdCode().Begin()), itr_label_end( GetIdCode().End());
        itr_label != itr_label_end;
        ++itr_label
      )
      {
        // find this feature label in the feature_labels object
        id_sizes.PushBack( all_id_labels.GetPropertyIndices( *itr_label).GetSize());
      }

      return FeatureLabelSet( all_id_labels.GetAlias(), GetIdCode().GetArguments(), id_sizes);
    }

    //! @brief get the number of partitions requested by the user, along with the partition ids
    //! @return the number of partitions requested by the user, along with the partition ids
    storage::Pair< size_t, math::RangeSet< size_t> > RetrieveDatasetSubset::GetNumberPartitionsAndIds() const
    {
      return storage::Pair< size_t, math::RangeSet< size_t> >( m_NumberChunks, m_ChunkRanges);
    }

    //! @brief write attributes for the arff file header
    //! @param OSTREAM output stream
    //! @param ID_LABELS labels for IDS
    //! @param FEATURE_LABELS labels for the features
    //! @param RESULT_LABELS labels for the results
    void WriteArffAttributes
    (
      std::ostream &OSTREAM,
      const FeatureLabelSet &ID_LABELS,
      const FeatureLabelSet &FEATURE_LABELS,
      const FeatureLabelSet &RESULT_LABELS
    )
    {
      storage::Vector< std::string> ids( ID_LABELS.GetMemberLabels().Begin(), ID_LABELS.GetMemberLabels().End());

      // get the labels; one per actual numeric column
      FeatureLabelSet features_split( FEATURE_LABELS.SplitFeatureLabelSet( true));
      FeatureLabelSet results_split( RESULT_LABELS.SplitFeatureLabelSet( true));
      storage::Vector< std::string> features
      (
        features_split.GetMemberLabels().Begin(),
        features_split.GetMemberLabels().End()
      );
      storage::Vector< std::string> results
      (
        results_split.GetMemberLabels().Begin(),
        results_split.GetMemberLabels().End()
      );
      util::SiPtrVector< storage::Vector< std::string> > svect
      (
        util::SiPtrVector< storage::Vector< std::string> >::Create
        (
          ids,
          features,
          results
        )
      );
      // change all double quotes to single for weka
      for
      (
        util::SiPtrVector< storage::Vector< std::string> >::iterator itr_vec( svect.Begin()), itr_vec_end( svect.End());
        itr_vec != itr_vec_end;
        ++itr_vec
      )
      {
        for
        (
          storage::Vector< std::string>::iterator itr( ( *itr_vec)->Begin()), itr_end( ( *itr_vec)->End());
          itr != itr_end;
          ++itr
        )
        {
          std::replace( itr->begin(), itr->end(), '"', '\'');
        }
      }
      if( ids.GetSize())
      {
        OSTREAM << "% IDs section (used only for interpretation)\n";
        OSTREAM << "@attribute \"" << util::Join( "\" string\n@attribute \"", ids) << "\" string\n";
      }
      if( features.GetSize())
      {
        OSTREAM << "% Features section (descriptors used to predict with)\n";
        OSTREAM << "@attribute \"" << util::Join( "\" numeric\n@attribute \"", features) << "\" numeric\n";
      }
      if( results.GetSize())
      {
        OSTREAM << "% Results. For classification problems, replace \"numeric\" type below with the allowed class values\n";
        OSTREAM << "@attribute \"" << util::Join( "\" numeric\n@attribute \"", results) << "\" numeric\n";
      }
      OSTREAM << "\n@data\n";
    }

    //! @brief helper function to write out id labels with commas between the individual properties
    //! @param OSTREAM output stream
    //! @param ID_ROW row of the id matrix
    //! @param SIZES sizes of each descriptor in the row
    void WriteIdLabelsCSV( std::ostream &OSTREAM, const char *const &ID_ROW, const storage::Vector< size_t> &SIZES)
    {
      if( !SIZES.IsEmpty())
      {
        // write the first property to the ostream
        OSTREAM.write( ID_ROW, SIZES.FirstElement());

        // start of the next descriptor, updated in for loop
        const char *descriptor_start( ID_ROW + SIZES.FirstElement());

        for
        (
          storage::Vector< size_t>::const_iterator itr( SIZES.Begin() + 1), itr_end( SIZES.End());
          itr != itr_end;
          ++itr
        )
        {
          // write a comma
          OSTREAM << ',';

          // write the given property to the ostream
          OSTREAM.write( descriptor_start, *itr);

          // update the start pointer for the next descriptor
          descriptor_start += *itr;
        }
      }
    }

    //! @brief helper function to write features, results, and ids to a csv file
    void WriteCSV
    (
      std::ostream &OUTPUT,
      const linal::Matrix< float> &FEATURES,
      const linal::Matrix< float> &RESULTS,
      const linal::Matrix< char> &IDS,
      const storage::Vector< size_t> &ID_SIZES,
      const size_t &NUMBER_ROWS,
      const bool &ALLOW_INCOMPLETE_RECORDS
    )
    {
      // determine the feature and result sizes
      const size_t feature_size( FEATURES.GetNumberCols()), result_size( RESULTS.GetNumberCols());
      const size_t id_size( IDS.GetNumberCols());

      // csv output
      // write out each feature/result on one line, with line size exactly equal to line_width
      for( size_t feature_number( 0); feature_number < NUMBER_ROWS; ++feature_number)
      {
        if( !ALLOW_INCOMPLETE_RECORDS && !RESULTS.GetRow( feature_number).IsDefined())
        {
          continue;
        }

        // write out the ids
        WriteIdLabelsCSV( OUTPUT, IDS[ feature_number], ID_SIZES);

        // keep track of whether anything has been written on this line; necessary to keep all commas internal
        bool have_written_first( id_size);
        const float *itr_feature( FEATURES[ feature_number]), *itr_feature_end( itr_feature + feature_size);
        if( !have_written_first && feature_size)
        {
          have_written_first = true;
          // write out the first value
          io::Serialize::Write( *itr_feature, OUTPUT);
          ++itr_feature;
        }
        // write out the features
        for( ; itr_feature != itr_feature_end; ++itr_feature)
        {
          // csv
          OUTPUT << ',';
          io::Serialize::Write( *itr_feature, OUTPUT);
        }

        const float *itr_result( RESULTS[ feature_number]), *itr_result_end( itr_result + result_size);
        if( !have_written_first && result_size)
        {
          // write out the first value
          io::Serialize::Write( *itr_result, OUTPUT);
          ++itr_result;
        }
        // write out the results
        for( ; itr_result != itr_result_end; ++itr_result)
        {
          OUTPUT << ',';
          io::Serialize::Write( *itr_result, OUTPUT);
        }
        OUTPUT << '\n';
      }
    }

    //! @brief helper function to write features, results, and ids to a csv file
    void WriteBin
    (
      std::ostream &OUTPUT,
      const linal::Matrix< float> &FEATURES,
      const linal::Matrix< float> &RESULTS,
      const linal::Matrix< char> &IDS,
      const size_t &NUMBER_ROWS,
      const bool &ALLOW_INCOMPLETE_RECORDS
    )
    {
      // determine the feature and result sizes
      const size_t feature_size( FEATURES.GetNumberCols()), result_size( RESULTS.GetNumberCols());
      const size_t id_size( IDS.GetNumberCols());

      // write out each feature/result on one line, with line size exactly equal to line_width
      for( size_t feature_number( 0); feature_number < NUMBER_ROWS; ++feature_number)
      {
        if( !ALLOW_INCOMPLETE_RECORDS && !RESULTS.GetRow( feature_number).IsDefined())
        {
          continue;
        }
        // write out the ids
        for
        (
          const char *itr_feature( IDS[ feature_number]), *itr_feature_end( itr_feature + id_size);
          itr_feature != itr_feature_end;
          ++itr_feature
        )
        {
          // write binary output
          io::BinarySerialize::Write( *itr_feature, OUTPUT);
        }

        // write out the features
        for
        (
          const float *itr_feature( FEATURES[ feature_number]), *itr_feature_end( itr_feature + feature_size);
          itr_feature != itr_feature_end;
          ++itr_feature
        )
        {
          // binary
          io::BinarySerialize::Write( *itr_feature, OUTPUT);
        }

        // write out the results
        for
        (
          const float *itr_feature( RESULTS[ feature_number]), *itr_feature_end( itr_feature + result_size);
          itr_feature != itr_feature_end;
          ++itr_feature
        )
        {
          // write binary output
          io::BinarySerialize::Write( *itr_feature, OUTPUT);
        }
      }
    }

    // local enum to specify a storage type
    enum StorageType
    {
      e_Bin,  // .bin file
      e_Csv,  // .csv file, or csv.bz2, etc.
      e_Arff  // .arff file, used for interaction with WEKA (http://www.cs.waikato.ac.nz/ml/weka/arff.html)
    };

    //! @class struct to allow for threaded dataset creation
    struct BCL_API StoreThreadData
    {
      math::Range< size_t> m_Range; //!< Dataset range to construct
      util::Implementation< RetrieveDataSetBase> m_Retriever; //!< Retriever of the dataset
      linal::Matrix< float> m_Features; //!< Features calculated by the thread
      linal::Matrix< float> m_Results;  //!< Results calculated by the thread
      linal::Matrix< char>  m_Ids;      //!< IDs calculated by the thread
      StorageType           m_StorageType; //!< Type of storage to use
      size_t                m_NumberFeaturesComputed; //!< Actual # of features computed
      storage::Vector< size_t> m_IdSizes;
      std::string           m_LastString;
      bool                  m_AllowIncompleteRecords;

      //! @brief default constructor
      StoreThreadData() : m_NumberFeaturesComputed( 0) {}

      //! function that actually works with the thread
      void RunThread()
      {
        std::stringstream stream( std::ios::out | ( m_StorageType == e_Bin ? std::ios::binary : std::ios::out));
        m_NumberFeaturesComputed = m_Retriever->GenerateDataSubset( m_Range, m_Features, m_Results, m_Ids, 0);
        if( m_StorageType == e_Bin)
        {
          WriteBin( stream, m_Features, m_Results, m_Ids, m_NumberFeaturesComputed, m_AllowIncompleteRecords);
        }
        else
        {
          WriteCSV( stream, m_Features, m_Results, m_Ids, m_IdSizes, m_NumberFeaturesComputed, m_AllowIncompleteRecords);
        }
        m_LastString = stream.str();
      }
    };

    // trivial helper functions to allow usage of StoreThreadData in the scheduling framework
    std::ostream &operator <<( std::ostream &STREAM, const StoreThreadData &)
    {
      return STREAM;
    }

    // trivial helper functions to allow usage of StoreThreadData in the scheduling framework
    std::istream &operator >>( std::istream &STREAM, StoreThreadData &)
    {
      return STREAM;
    }

    //! @brief store the data from a dataset into a master dataset
    //! @param FILENAME file to store the dataset in
    //! @param DATA_SET_RETRIEVER object used to retrieve the dataset
    //! @param BLOCK_SIZE_MB maximum MB of data to generate per thread before writing to disk
    //! @param ALLOW_INCOMPLETE_RECORDS true to allow incomplete results to be stored; by default, all results
    //!        must be given for each record
    void RetrieveDatasetSubset::StoreMasterDataset
    (
      const std::string &FILENAME,
      RetrieveDataSetBase &DATA_SET_RETRIEVER,
      const double &BLOCK_SIZE_MB,
      const bool &ALLOW_INCOMPLETE_RECORDS
    )
    {
      // init storage type, assume binary file, since that is the default format
      StorageType storage_type( e_Bin);

      // look for csv in the filename
      const size_t csv_pos( FILENAME.rfind( ".csv"));
      // if there is a csv extension, assume .csv format, but still allow compressed alternatives
      if( csv_pos != std::string::npos && ( csv_pos + 4 == FILENAME.size() || FILENAME[ csv_pos + 4] == '.'))
      {
        storage_type = e_Csv;
      }
      else
      {
        // check for arff format
        const size_t arff_pos( FILENAME.rfind( ".arff"));
        if( arff_pos != std::string::npos && ( arff_pos + 5 == FILENAME.size() || FILENAME[ arff_pos + 5] == '.'))
        {
          storage_type = e_Arff;
        }
      }
      io::OFStream output;

      if( storage_type == e_Bin)
      {
        io::File::MustOpenOFStream( output, FILENAME, std::ios::binary);
      }
      else
      {
        io::File::MustOpenOFStream( output, FILENAME);
      }
      BCL_Assert( output.is_open(), "Could not open " + FILENAME + " for writing");

      // get the feature and result labels with sizes
      FeatureLabelSet feature_labels( DATA_SET_RETRIEVER.GetFeatureLabelsWithSizes());
      FeatureLabelSet result_labels( DATA_SET_RETRIEVER.GetResultCodeWithSizes());
      FeatureLabelSet id_labels( DATA_SET_RETRIEVER.GetIdCodeWithSizes());

      if( storage_type == e_Bin)
      {
        // write out the feature and result labels
        WriteFeatureLabelSetBinary( output, feature_labels);
        WriteFeatureLabelSetBinary( output, result_labels);
        WriteFeatureLabelSetBinary( output, id_labels);
      }
      else if( storage_type == e_Arff)
      {
        // write out the arff headers
        output << "% Written by " << GetVersion().GetDetailedInfoString() << '\n';
        std::string dataset_retriever( DATA_SET_RETRIEVER.GetCompleteSerializer().GetLabel().ToString());
        std::replace( dataset_retriever.begin(), dataset_retriever.end(), '"', '\'');
        output << "@relation \"" << dataset_retriever << "\"\n\n";
        WriteArffAttributes
        (
          output,
          id_labels,
          feature_labels,
          result_labels
        );
      }

      // determine the nominal size of the entire dataset
      const size_t dataset_size( DATA_SET_RETRIEVER.GetNominalSize());

      BCL_MessageStd
      (
        "Values per feature: " + util::Format()( feature_labels.GetSize())
        + " Values per result: " + util::Format()( result_labels.GetSize())
        + " Characters per id: " + util::Format()( id_labels.GetSize())
        + " Nominal dataset # feature rows: " + util::Format()( dataset_size)
      );

      // determine the feature and result sizes
      const size_t feature_size( feature_labels.GetSize()), result_size( result_labels.GetSize());
      const size_t id_size( id_labels.GetSize());

      // determine the # of features and results put together
      const size_t number_features_and_results( sizeof( float) * ( feature_size + result_size) + id_size);

      size_t number_features_to_generate_at_a_time( 0);

      // determine # cpus
      const size_t n_cpus( sched::GetScheduler().GetNumberUnusedCPUS());

      // determine the number of features to generate at a time
      if( DATA_SET_RETRIEVER.SupportsEfficientSubsetLoading())
      {
        const size_t max_desired_values_at_a_time( ( 1 << 20) * BLOCK_SIZE_MB); // generate <= 32 MB worth of floats at a time
        number_features_to_generate_at_a_time =
          std::min( 1 + max_desired_values_at_a_time / number_features_and_results, dataset_size);
        if( n_cpus > size_t( 1) && dataset_size < max_desired_values_at_a_time / number_features_and_results * n_cpus)
        {
          number_features_to_generate_at_a_time = size_t( 1) + dataset_size / n_cpus;
        }
      }
      else
      {
        BCL_MessageCrt
        (
          "Your choice of dataset retrievers requires that the entire dataset be generated at once\n"
          + std::string( "if this exceeds memory resources, use a different means of retrieval")
        );
        // have to load the entire dataset
        number_features_to_generate_at_a_time = dataset_size;
      }

      // temp_features, temp_results will store features and results from each round, until they can be written out
      // to avoid reallocating memory, just create it once
      linal::Matrix< float> temp_features( number_features_to_generate_at_a_time, feature_size);
      linal::Matrix< float> temp_results( number_features_to_generate_at_a_time, result_size);
      linal::Matrix< char> temp_ids( number_features_to_generate_at_a_time, id_size);
      const storage::Vector< size_t> &id_sizes( id_labels.GetPropertySizes());

      // store the final dataset size
      size_t final_dataset_size( 0);

      // handle the serial case
      if
      (
        number_features_to_generate_at_a_time == dataset_size
        || sched::GetScheduler().GetNumberUnusedCPUS() <= size_t( 1)
      )
      {
        // generate the features in several rounds, if possible
        // to avoid the memory overhead of holding the entire dataset at once in memory
        for
        (
          size_t lower_limit( 0);
          lower_limit < dataset_size;
          lower_limit += number_features_to_generate_at_a_time
        )
        {
          // get the next range to extract from the dataset
          math::Range< size_t> next_dataset_range
          (
            math::RangeBorders::e_LeftClosed,
            lower_limit,
            lower_limit + number_features_to_generate_at_a_time,
            math::RangeBorders::e_RightOpen
          );

          // generate the features; store how many were actually generated
          const size_t actual_number_of_generated_features
          (
            DATA_SET_RETRIEVER.GenerateDataSubset
            (
              next_dataset_range,
              temp_features,
              temp_results,
              temp_ids,
              0
            )
          );

          final_dataset_size += actual_number_of_generated_features;

          std::stringstream output_str;
          if( storage_type == e_Bin)
          {
            WriteBin( output_str, temp_features, temp_results, temp_ids, actual_number_of_generated_features, ALLOW_INCOMPLETE_RECORDS);
          }
          else
          {
            WriteCSV( output_str, temp_features, temp_results, temp_ids, id_sizes, actual_number_of_generated_features, ALLOW_INCOMPLETE_RECORDS);
          }
          const std::string output_str_out( output_str.str());
          output.write( output_str_out.data(), output_str_out.size());
        }
      }
      else
      {
        BCL_MessageStd
        (
          "Computing " + util::Format()( number_features_to_generate_at_a_time) + " features per thread launch"
        );
        // create ranges for each thread
        const size_t expected_thread_launches
        (
          ( dataset_size + number_features_to_generate_at_a_time - 1)
          / number_features_to_generate_at_a_time
        );
        const size_t n_threads( std::max( size_t( 1), std::min( expected_thread_launches, sched::GetNumberCPUs())));
        std::vector< StoreThreadData> thread_data( n_threads);
        util::ShPtrVector< sched::JobInterface> jobs( n_threads);

        // initialize data for each thread
        for( size_t thread_number( 0); thread_number < n_threads; ++thread_number)
        {
          thread_data[ thread_number].m_Features = temp_features;
          thread_data[ thread_number].m_Results = temp_results;
          thread_data[ thread_number].m_Ids = temp_ids;
          thread_data[ thread_number].m_Retriever = util::Implementation< RetrieveDataSetBase>( DATA_SET_RETRIEVER);
          thread_data[ thread_number].m_IdSizes = id_sizes;
          thread_data[ thread_number].m_StorageType = storage_type;
          thread_data[ thread_number].m_AllowIncompleteRecords = ALLOW_INCOMPLETE_RECORDS;
          jobs( thread_number) =
            util::ShPtr< sched::JobInterface>
            (
              new sched::ThunkJob< StoreThreadData, void>( 0, thread_data[ thread_number], &StoreThreadData::RunThread)
            );
        }
        size_t threads_last_round( 0);
        size_t threads_this_round( 0);
        size_t lower_limit( 0);
        for
        (
          ;
          threads_this_round < n_threads && lower_limit < dataset_size;
          ++threads_this_round, lower_limit += number_features_to_generate_at_a_time
        )
        {
          thread_data[ threads_this_round].m_Range =
            math::Range< size_t>
            (
              math::RangeBorders::e_LeftClosed,
              lower_limit,
              lower_limit + number_features_to_generate_at_a_time,
              math::RangeBorders::e_RightOpen
            );
          sched::GetScheduler().SubmitJob( jobs( threads_this_round));
        }

        size_t n_joined( 0);

        // status bar length
        const size_t status_bar_length( 20);

        while( lower_limit < dataset_size || threads_this_round)
        {
          threads_last_round = threads_this_round;
          threads_this_round = 0;
          for( size_t thread_number( 0); thread_number < threads_last_round; ++thread_number)
          {
            sched::GetScheduler().Join( jobs( thread_number));
            ++n_joined;

            // get a reference to the thread data
            const StoreThreadData &thread_data_reference( thread_data[ thread_number]);
            const size_t actual_number_of_generated_features( thread_data_reference.m_NumberFeaturesComputed);
            final_dataset_size += actual_number_of_generated_features;

            // copy the thread's data so that it can immediately be lauched on the next block while this block is
            const std::string thread_chars( thread_data_reference.m_LastString);

            // assign the thread a new work block
            if( lower_limit < dataset_size)
            {
              thread_data[ threads_this_round].m_Range =
                math::Range< size_t>
                (
                  math::RangeBorders::e_LeftClosed,
                  lower_limit,
                  lower_limit + number_features_to_generate_at_a_time,
                  math::RangeBorders::e_RightOpen
                );
              sched::GetScheduler().SubmitJob( jobs( threads_this_round));
              lower_limit += number_features_to_generate_at_a_time;
              ++threads_this_round;
            }

            // determine approximate # of features generated
            const size_t n_features_gen( std::min( n_joined * number_features_to_generate_at_a_time, dataset_size));

            // determine progress percent
            const size_t percent( float( n_joined) * 100.0f / float( expected_thread_launches));

            // determine number of stars in the status bar
            const size_t number_stars( percent * status_bar_length / 100);

            const std::string status
            (
              "["
              + std::string( number_stars, '*')
              + std::string( status_bar_length - number_stars, ' ')
              + "] "
              + util::Format()( percent) + "% "
              + util::Format()( final_dataset_size) + " / "
              + util::Format()( dataset_size - n_features_gen + final_dataset_size)
              + " features generated"
            );
            util::GetLogger().LogStatus( status);

            // write out the data
            output.write( thread_chars.data(), thread_chars.size());
          }
        }
      }

      BCL_MessageStd( "Actual final dataset size: " + util::Format()( final_dataset_size))

      // close the file
      output.close();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief generate dataset
    //! @return generated dataset
    util::ShPtr< descriptor::Dataset>
    RetrieveDatasetSubset::GenerateDataSet()
    {
      if( m_NumberChunks > 1)
      {
        // ensure the range set does not exceed the # of ranges
        BCL_Assert
        (
          m_ChunkRanges.GetMax() < m_NumberChunks,
          GetClassIdentifier() + " received range set with max >= # ranges! # ranges = " + util::Format()( m_NumberChunks)
          + " " + m_ChunkRanges.AsString()
        );

        // ensure the range set is not empty
        BCL_Assert
        (
          !m_ChunkRanges.IsEmpty(),
          GetClassIdentifier() + " received empty range set"
        );
      }
      else
      {
        m_NumberChunks = 1;
        m_ChunkRanges = math::RangeSet< size_t>( math::Range< size_t>( 0, 0));
      }

      // BCL_MessageStd( "total/actual size: " + util::Format()( total_size) + "/" + util::Format()( actual_dataset_size));

      // read the data set from the file
      std::ifstream input;

      // open the file, determine the size, setup the feature and result columns
      const size_t total_size( GetTotalSizeAndParseLabels( input));

      // get the position before reading this line
      std::istream::pos_type first_line_position( input.tellg());

      // determine the total # of features/results on each line
      const size_t total_features_results( m_Features.GetInputFeatureSize());

      // compute bytes per feautre
      std::istream::pos_type bytes_per_feature( sizeof( float) * total_features_results + m_Ids.GetInputFeatureSize());

      // get the actual ranges that will be taken from the dataset
      storage::Vector< math::Range< size_t> > ranges;
      ranges.AllocateMemory( m_ChunkRanges.GetRanges().GetSize());

      size_t actual_dataset_size( 0); // track the actual size of the resulting dataset
      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr_range( m_ChunkRanges.GetRanges().Begin()), itr_range_end( m_ChunkRanges.GetRanges().End());
        itr_range != itr_range_end;
        ++itr_range
      )
      {
        ranges.PushBack
        (
          RetrieveDataSetBase::GetStartEndPositionOfRange( *itr_range, m_NumberChunks, total_size)
        );
        actual_dataset_size += ranges.LastElement().GetWidth();
      }

      BCL_MessageStd
      (
        "result columns: " + util::Format()( m_Results.GetColumnIndices()( 0))
        + " - " + util::Format()( m_Results.GetColumnIndices().LastElement())
      );

      // create a new dataset
      util::ShPtr< descriptor::Dataset> dataset
      (
        new descriptor::Dataset
        (
          actual_dataset_size,
          GetFeatureLabelsWithSizes(),
          GetResultCodeWithSizes(),
          GetIdCodeWithSizes()
        )
      );

      // get referencest to the underlying matrices
      linal::MatrixReference< float> loaded_features( dataset->GetFeaturesReference());
      linal::MatrixReference< float> loaded_results( dataset->GetResultsReference());
      linal::MatrixReference< char> loaded_ids( dataset->GetIdsReference());

      // create a linal::Vector< float> object to hold the data loaded in
      linal::Vector< float> temp_vector( total_features_results);
      linal::Vector< char> temp_id_vector( loaded_ids.GetNumberCols());
      const size_t id_size( temp_id_vector.GetSize());

      // keep track of how many rows have been loaded
      size_t loaded_feature_counter( 0);

      // walk over the lines desired, loading only those columns and results that we want
      for
      (
        storage::Vector< math::Range< size_t> >::const_iterator itr_range( ranges.Begin()), itr_range_end( ranges.End());
        itr_range != itr_range_end;
        ++itr_range
      )
      {
        // move to the appropriate line
        input.seekg( first_line_position + std::istream::pos_type( itr_range->GetMin()) * bytes_per_feature);

        BCL_MessageVrb( "stream position: " + util::Format()( input.tellg()));

        // load in the desired lines
        for
        (
          size_t line_number( 0), number_lines( itr_range->GetWidth());
          line_number < number_lines;
          ++line_number, ++loaded_feature_counter
        )
        {
          // load the ids into temp_vector
          for( size_t feature_number( 0); feature_number < id_size; ++feature_number)
          {
            io::BinarySerialize::Read( temp_id_vector( feature_number), input);
          }

          // load the features and results into temp_vector
          for( size_t feature_number( 0); feature_number < total_features_results; ++feature_number)
          {
            io::BinarySerialize::Read( temp_vector( feature_number), input);
          }

          // put the chosen columns into the appropriate matrices
          m_Ids( temp_id_vector, loaded_ids[ loaded_feature_counter]);
          m_Features( temp_vector, loaded_features[ loaded_feature_counter]);
          m_Results( temp_vector, loaded_results[ loaded_feature_counter]);
        }
      }

      io::File::CloseClearFStream( input);

      // return the generated data set
      return dataset;
    } // GenerateDataSet

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer RetrieveDatasetSubset::GetSerializer() const
    {
      io::Serializer member_data;
      member_data.SetClassDescription( "reads a .bin file created with GenerateDataset");

      member_data.AddInitializer
      (
        "filename",
        "name of the file from which to load the dataset",
        io::Serialization::GetAgentInputFilename( &m_Filename)
      );

      member_data.AddInitializer
      (
        "number chunks",
        "# of chunks the file should be split into (conceptually), used to divide the file into disparate datasets",
        io::Serialization::GetAgent( &m_NumberChunks),
        "1"
      );

      member_data.AddInitializer
      (
        "chunks",
        "ranges of chunks to load, e.g. chunks=\"[ 0, 5) (7,10)\"",
        io::Serialization::GetAgent( &m_ChunkRanges),
        "[0]"
      );

      return member_data;
    } // GetParameters

    //! @brief get the size of the complete dataset without chunking
    //! @return the size of the complete dataset without chunking
    size_t RetrieveDatasetSubset::GetTotalDatasetSize() const
    {
      std::ifstream input;
      input.open( m_Filename.c_str(), std::ios::binary | std::ios::in);

      size_t byt_per_feature_result( 0);
      // skip over the feature and result labels
      {
        byt_per_feature_result += sizeof( float) * ReadFeatureLabelSetBinary( input).GetSize(); // feature
        byt_per_feature_result += sizeof( float) * ReadFeatureLabelSetBinary( input).GetSize(); // result
        byt_per_feature_result += ReadFeatureLabelSetBinary( input).GetSize(); // ids
      }

      std::ifstream::pos_type bytes_per_feature_result( byt_per_feature_result);

      // get the position before reading this line
      std::ifstream::pos_type first_feature_position( input.tellg());

      // determine the position at the end of the file
      input.seekg( 0, std::ios::end);

      std::ifstream::pos_type end_position( input.tellg());

      // close the file
      io::File::CloseClearFStream( input);

      // determine the # of chars in the file that are dedicated to the features/results
      std::ifstream::pos_type features_results_chars( end_position - first_feature_position);

      // Check that the size of the file for features is a multiple of line_width
      // Otherwise, the file is of the wrong format
      BCL_Assert
      (
        features_results_chars % bytes_per_feature_result == 0
        && first_feature_position > 0
        && bytes_per_feature_result > 0,
        "File " + m_Filename + " is of the wrong format; "
        + GetClassIdentifier() + " requires files to be in binary text format"
      );

      // determine the # of features
      return features_results_chars / bytes_per_feature_result;
    }

    //! @brief load a range of data from the dataset
    //! @param SUBSET the range of data to load
    //! @param FEATURES_STORAGE where to store features that are loaded, must be large enough to hold the subset without resizing
    //! @param RESULTS_STORAGE where to store the corresponding results, must be large enough to hold the subset without resizing
    //! @param START_FEATURE_NUMBER position to store the first feature in FEATURES_STORAGE
    //! @return # of features actually loaded
    //! Note: Implementations should overload this and SupportsEfficientSubsetLoading together
    size_t RetrieveDatasetSubset::GenerateDataSubset
    (
      const math::Range< size_t> &SUBSET,
      linal::MatrixInterface< float> &FEATURES_STORAGE,
      linal::MatrixInterface< float> &RESULTS_STORAGE,
      linal::MatrixInterface< char> &IDS_STORAGE,
      const size_t &START_FEATURE_NUMBER
    )
    {
      // read the data set from the file
      std::ifstream input;

      // open the file, determine the size, setup the feature and result columns
      const size_t total_dataset_size( GetTotalSizeAndParseLabels( input));

      // determine feature + result size
      const size_t feature_result_size( m_Features.GetInputFeatureSize());

      // determine id size
      const size_t ids_size( m_Ids.GetInputFeatureSize());

      // create a linal::Vector< float> object to hold the data loaded in
      linal::Vector< float> temp_vector( feature_result_size);
      linal::Vector< char> temp_id_vector( ids_size);

      // determine # of bytes per feature
      std::istream::pos_type bytes_per_feature( feature_result_size * sizeof( float) + ids_size);

      // get the actual ranges that will be taken from the dataset
      storage::Vector< math::Range< size_t> > ranges;
      ranges.AllocateMemory( m_ChunkRanges.GetRanges().GetSize());

      for
      (
        storage::Set< math::Range< size_t> >::const_iterator
          itr_range( m_ChunkRanges.GetRanges().Begin()), itr_range_end( m_ChunkRanges.GetRanges().End());
        itr_range != itr_range_end;
        ++itr_range
      )
      {
        ranges.PushBack
        (
          RetrieveDataSetBase::GetStartEndPositionOfRange( *itr_range, m_NumberChunks, total_dataset_size)
        );
      }

      // get the position before reading this line
      std::ifstream::pos_type first_feature_position( input.tellg());

      // keep track of how many rows have been loaded
      size_t loaded_feature_counter( 0), feature_position( START_FEATURE_NUMBER);

      // walk over the lines desired, loading only those columns and results that we want
      for
      (
        storage::Vector< math::Range< size_t> >::const_iterator itr_range( ranges.Begin()), itr_range_end( ranges.End());
        itr_range != itr_range_end && loaded_feature_counter < SUBSET.GetMax();
        ++itr_range
      )
      {
        if( itr_range->GetWidth() + loaded_feature_counter < SUBSET.GetMin())
        {
          loaded_feature_counter += itr_range->GetWidth();
          continue;
        }

        // determine 1st feature to load of this subset
        const size_t feature_offset
        (
          loaded_feature_counter >= SUBSET.GetMin()
          ? 0
          : SUBSET.GetMin() - loaded_feature_counter
        );

        loaded_feature_counter += feature_offset;

        // move to the appropriate line
        input.seekg
        (
          first_feature_position + std::istream::pos_type( itr_range->GetMin() + feature_offset) * bytes_per_feature
        );

        BCL_MessageVrb( "stream position: " + util::Format()( input.tellg()));

        // load in the desired lines
        for
        (
          size_t line_number( feature_offset), number_lines( itr_range->GetWidth());
          line_number < number_lines && loaded_feature_counter < SUBSET.GetMax();
          ++line_number, ++loaded_feature_counter
        )
        {
          // read the id vector
          io::BinarySerialize::ReadVector( temp_id_vector, input);
          // select the desired id columns
          m_Ids( temp_id_vector, IDS_STORAGE[ feature_position]);

          // read the features & results vector
          io::BinarySerialize::ReadVector( temp_vector, input);

          // put the chosen columns into the appropriate matrices
          m_Features( temp_vector, FEATURES_STORAGE[ feature_position]);
          m_Results( temp_vector, RESULTS_STORAGE[ feature_position]);
          ++feature_position;
        }
      }

      io::File::CloseClearFStream( input);

      return feature_position - START_FEATURE_NUMBER;
    }

    //! @brief get the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    //! @return the nominal (e.g. best estimate without generating the entire dataset) size of the dataset
    size_t RetrieveDatasetSubset::GetNominalSize() const
    {
      const size_t total_dataset_size( GetTotalDatasetSize());
      return RetrieveDataSetBase::GetSubsetSize( m_ChunkRanges, m_NumberChunks, total_dataset_size);
    }

    //! @brief read a feature label set written to a binary file
    FeatureLabelSet RetrieveDatasetSubset::ReadFeatureLabelSetBinary( std::ifstream &INPUT)
    {
      // get the position before reading this line
      std::ifstream::pos_type original_position( INPUT.tellg());

      FeatureLabelSet features;

      // read in the first string
      std::string class_name;
      INPUT >> class_name;
      if( class_name != features.GetClassIdentifier())
      {
        // bad string; probably an older format that had fewer feature label sets
        // reset the position
        INPUT.seekg( original_position);
        return features;
      }

      features.Read( INPUT);

      // read until finding the delimiter (null character).  This allows us to skip extraneous spaces
      if( INPUT.good() && !INPUT.eof())
      {
        char input( ' ');
        INPUT.get( input);
        while( !INPUT.eof() && input != '\0')
        {
          INPUT.get( input);
        }
      }

      // return the feature label
      return features;
    }

    //! @brief read a feature label set written to a binary file
    void RetrieveDatasetSubset::WriteFeatureLabelSetBinary( std::ostream &WRITE, const FeatureLabelSet &LABELS)
    {
      // write the string to output
      io::Serialize::Write( LABELS, WRITE);
      WRITE << '\0';
    }

    //! @brief read the feature and result labels and create DataSetSelectColumns from them
    //! @param INPUT the input stream to use; should not be open initially
    //!        after this function is called, it will be on the first line of the dataset
    //! @return the # of features in this file, which is determined here
    size_t RetrieveDatasetSubset::GetTotalSizeAndParseLabels( std::ifstream &INPUT)
    {
      io::File::CloseClearFStream( INPUT);
      INPUT.open( m_Filename.c_str(), std::ios::binary | std::ios::in);

      // load the feature and result labels
      FeatureLabelSet feature_labels, result_labels, id_labels;
      feature_labels = ReadFeatureLabelSetBinary( INPUT);
      result_labels = ReadFeatureLabelSetBinary( INPUT);
      id_labels = ReadFeatureLabelSetBinary( INPUT);

      // determine the total # of features/results on each line
      const size_t total_features_results( feature_labels.GetSize() + result_labels.GetSize());

      // determine # of features
      size_t number_features( 0);

      {
        // get the position of the first feature-result
        std::istream::pos_type first_feature_position( INPUT.tellg());

        // compute bytes per feautre
        std::istream::pos_type bytes_per_feature_result( sizeof( float) * total_features_results + id_labels.GetSize());

        // determine the position at the end of the file
        INPUT.seekg( 0, std::ios::end);
        std::ifstream::pos_type end_position( INPUT.tellg());

        // return the stream to the first feature
        INPUT.seekg( first_feature_position);

        // determine the # of bytes in the file that are dedicated to the features/results
        std::ifstream::pos_type features_results_bytes( end_position - first_feature_position);

        BCL_MessageDbg( "feature_labels: " + util::Format()( feature_labels));

        BCL_MessageDbg( "result_labels: " + util::Format()( result_labels));

        BCL_MessageDbg
        (
          "features_results_bytes: " + util::Format()( features_results_bytes) +
          "\nbytes_per_feature_result: " + util::Format()( bytes_per_feature_result) +
          "\nfirst_feature_position: " + util::Format()( first_feature_position) +
          "\nbytes_per_feature_result: " + util::Format()( bytes_per_feature_result)
        );

        // Check that the size of the file for features is a multiple of line_width
        // Otherwise, the file is of the wrong format
        BCL_Assert
        (
          features_results_bytes % bytes_per_feature_result == 0
          && first_feature_position > 0
          && bytes_per_feature_result > 0,
          "File " + m_Filename + " is of the wrong format; "
          + GetClassIdentifier() + " requires files to be in binary text format"
        );

        // determine the # of features
        number_features = features_results_bytes / bytes_per_feature_result;
      }

      // check if the features selector has not already been set up
      if( !m_Features.GetInputFeatureSize())
      {
        // determine which feature labels will be kept
        storage::Vector< size_t> features_kept;
        features_kept.AllocateMemory( feature_labels.GetSize());

        // check; if no features were given, then set all features
        if( GetFeatureCode().IsScalar())
        {
          features_kept.AllocateMemory( feature_labels.GetSize());
          for( size_t i( 0), feature_size( feature_labels.GetSize()); i < feature_size; ++i)
          {
            features_kept.PushBack( i);
          }
        }
        else
        {
          // walk over the arguments in the feature label
          for
          (
            util::ObjectDataLabel::const_iterator
              itr_label( GetFeatureCode().Begin()), itr_label_end( GetFeatureCode().End());
            itr_label != itr_label_end;
            ++itr_label
          )
          {
            // append the indices of the given feature
            features_kept.Append( feature_labels.GetPropertyIndices( *itr_label));
          }
        }
        // create an appropriate DataSetSelectColumns object to select the desired columns of the dataset
        m_Features = DataSetSelectColumns( total_features_results, features_kept);
        BCL_MessageDbg( "Feature selector: " + util::Format()( m_Features));
      }

      if( !m_Results.GetInputFeatureSize())
      {
        // determine which feature labels will be kept
        storage::Vector< size_t> results_kept;
        results_kept.AllocateMemory( result_labels.GetSize());

        // check; if no result code given, then set all results
        if( GetResultCode().IsScalar())
        {
          results_kept.AllocateMemory( result_labels.GetSize());
          for( size_t i( 0), result_size( result_labels.GetSize()); i < result_size; ++i)
          {
            results_kept.PushBack( i);
          }
        }
        else
        {
          // now add the results labels, remembering to always add the size of the features first
          for
          (
            util::ObjectDataLabel::const_iterator
              itr_label( GetResultCode().Begin()), itr_label_end( GetResultCode().End());
            itr_label != itr_label_end;
            ++itr_label
          )
          {
            // append the indices of the given result
            results_kept.Append( result_labels.GetPropertyIndices( *itr_label));
          }
        }
        // add the offset to the results kept vector, since results come after features
        for
        (
          storage::Vector< size_t>::iterator itr( results_kept.Begin()), itr_end( results_kept.End());
          itr != itr_end;
          ++itr
        )
        {
          *itr += feature_labels.GetSize();
        }

        m_Results = DataSetSelectColumns( total_features_results, results_kept);
        BCL_MessageDbg( "Result selector: " + util::Format()( m_Results));
      }

      if( !m_Ids.GetInputFeatureSize())
      {
        // determine which feature labels will be kept
        storage::Vector< size_t> ids_kept;
        ids_kept.AllocateMemory( id_labels.GetSize());

        // check; if no result code given, then set all results
        if( GetIdCode().IsScalar())
        {
          ids_kept.AllocateMemory( id_labels.GetSize());
          for( size_t i( 0), id_size( id_labels.GetSize()); i < id_size; ++i)
          {
            ids_kept.PushBack( i);
          }
        }
        else
        {
          // now add the results labels, remembering to always add the size of the features first
          for
          (
            util::ObjectDataLabel::const_iterator
              itr_label( GetIdCode().Begin()), itr_label_end( GetIdCode().End());
            itr_label != itr_label_end;
            ++itr_label
          )
          {
            // append the indices of the given result
            ids_kept.Append( id_labels.GetPropertyIndices( *itr_label));
          }
        }

        m_Ids = DataSetSelectColumns( id_labels.GetSize(), ids_kept);
        BCL_MessageDbg( "IDs selector: " + util::Format()( m_Ids));
      }
      BCL_MessageStd
      (
        "# features: " + util::Format()( number_features)
        + " feature size: " + util::Format()( m_Features.GetOutputFeatureSize())
        + " result size: " + util::Format()( m_Results.GetOutputFeatureSize())
        + " feature result size of data superset: " + util::Format()( m_Results.GetInputFeatureSize())
      );
      return number_features;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_retrieve_interface.h"

// includes from bcl - sorted alphabetically
#include "descriptor/bcl_descriptor_dataset.h"
#include "math/bcl_math_roc_curve.h"
#include "math/bcl_math_running_average.h"
#include "storage/bcl_storage_list.h"
#include "util/bcl_util_sh_ptr.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! @brief Retrieves all descriptors, asserts if they are not all identical
    //! @return the unique descriptor
    util::ObjectDataLabel RetrieveInterface::RetrieveUniqueDescriptor() const
    {
      storage::List< util::ObjectDataLabel> all_descriptors( RetrieveEnsembleDescriptors());
      BCL_Assert( !all_descriptors.IsEmpty(), "No descriptors retrieved!");
      const size_t n_descriptor_sets( all_descriptors.GetSize());
      BCL_Assert
      (
        n_descriptor_sets == size_t( 1)
        ||
        std::count_if
        (
          ++all_descriptors.Begin(),
          all_descriptors.End(),
          std::bind2nd( std::equal_to< util::ObjectDataLabel>(), all_descriptors.FirstElement())
        )
        == int( n_descriptor_sets - 1),
        "Multiple descriptor sets found!"
      );
      return all_descriptors.FirstElement();
    }

    //! @brief get the cv info that is held in common across all CV in the dataset
    //! @return the commonly-held cross-validation info
    CrossValidationInfo RetrieveInterface::RetrieveCommonCVInfo() const
    {
      storage::List< CrossValidationInfo> all_cv_info( RetrieveEnsembleCVInfo());
      if( all_cv_info.IsEmpty())
      {
        return CrossValidationInfo();
      }

      storage::List< CrossValidationInfo>::const_iterator itr( all_cv_info.Begin()), itr_end( all_cv_info.End());
      CrossValidationInfo common_cv( *itr);
      while( ++itr != itr_end)
      {
        common_cv.KeepSharedInfo( *itr);
      }
      return common_cv;
    }

    //! @brief read the merged independent dataset predictions into a dataset object, whose features are the predicted values
    //!        results are the same result values, and ids are the given ids
    util::ShPtr< descriptor::Dataset> RetrieveInterface::ReadMergedIndependentPredictions()
    {
      // map from cross-validation ids to filenames
      storage::Map< std::string, storage::Vector< CrossValidationInfo> > cv_filenames;
      // filenames need to be retrieved from the model storage
      storage::List< CrossValidationInfo> cv_info( this->RetrieveEnsembleCVInfo());
      storage::Vector< size_t> counts( cv_info.GetSize(), size_t( 0));
      for
      (
        storage::List< CrossValidationInfo>::const_iterator itr( cv_info.Begin()), itr_end( cv_info.End());
        itr != itr_end;
        ++itr
      )
      {
        storage::Vector< CrossValidationInfo> &vec
        (
          cv_filenames[ itr->GetIndependentDatasetRetriever().ToString()]
        );
        // track the # of times each cv independent id is seen
        ++counts( vec.GetSize());
        vec.PushBack( *itr);
      }

      // test whether all cv's have the same # of files. This command adds the number of times the counts histogram was
      // 0, which should be counts.GetSize() - 1 if all the cvs have the same # of files.
      const size_t count
      (
        std::count( counts.Begin(), counts.End(), size_t( 0))
        + std::count( counts.Begin(), counts.End(), counts( 0))
      );
      if( count != counts.GetSize())
      {
        // Not equal number of counts. Either there really were different numbers of cross validations run per model, or
        // else the cross validations were run from different clusters, in which case the apparent absolute path may
        // differ
        storage::Map< std::string, storage::Vector< CrossValidationInfo> > cv_filenames_diff;
        counts.SetAllElements( 0);
        for
        (
          storage::List< CrossValidationInfo>::const_iterator itr( cv_info.Begin()), itr_end( cv_info.End());
          itr != itr_end;
          ++itr
        )
        {
          // use the difference function to compute the difference of the independent retriever from the training retriever
          // The reason this is needed is because the underlying training dataset files may have been computed from
          // identical files at different locations (e.g. using scratch on a cluster vs. an explicit path when done locally)
          storage::Vector< CrossValidationInfo> &vec
          (
            cv_filenames_diff
            [
              itr->GetIndependentDatasetRetriever().Difference( itr->GetTrainingDatasetRetriever()).ToString()
            ]
          );
          // track the # of times each cv independent id is seen
          ++counts( vec.GetSize());
          vec.PushBack( *itr);
        }
        const size_t count_new
        (
          std::count( counts.Begin(), counts.End(), size_t( 0))
          + std::count( counts.Begin(), counts.End(), counts( 0))
        );
        if( count_new == counts.GetSize())
        {
          BCL_MessageCrt
          (
            "Warning; differing number of files for different cross validation independent sets. Fixed by "
            "assuming that the cross validations were run from different locations"
          );
          cv_filenames = cv_filenames_diff;
        }
        BCL_MessageStd
        (
          "Found " + util::Format()( cv_info.GetSize()) + " models with "
          + util::Format()( cv_filenames.GetSize()) + " independent chunks"
        );
      }

      // final container of prediction matrices
      storage::Vector< linal::Matrix< float> > final_prediction_matrices;
      storage::Vector< linal::Matrix< char> > final_id_matrices;
      storage::Vector< linal::Matrix< float> > final_result_matrices;

      CrossValidationInfo common_cv_info;
      bool first_in_all_cv( true);
      // iterate over all input file names and construct prediction matrices for each entry
      for
      (
        storage::Map< std::string, storage::Vector< CrossValidationInfo> >::iterator
          itr_independent_to_filenames( cv_filenames.Begin()), itr_independent_to_filenames_end( cv_filenames.End());
        itr_independent_to_filenames != itr_independent_to_filenames_end;
        ++itr_independent_to_filenames
      )
      {
        // container for prediction matrices
        storage::Vector< linal::Matrix< float> > prediction_matrices;
        FeatureDataSet< char> ids_matrix;
        FeatureDataSet< float> results_matrix;

        math::RunningAverage< linal::Matrix< float> > avg_prediction_matrices;
        bool is_first( true);
        for
        (
          storage::Vector< CrossValidationInfo>::iterator
            itr_cv_info( itr_independent_to_filenames->second.Begin()),
            itr_cv_info_end( itr_independent_to_filenames->second.End());
          itr_cv_info != itr_cv_info_end;
          ++itr_cv_info
        )
        {
          util::ShPtr< descriptor::Dataset> sp_dataset( itr_cv_info->ReadIndependentPredictions());
          BCL_Assert( sp_dataset.IsDefined(), "Could not read dataset given by " + itr_cv_info->GetLabel().ToString());

          // retain only the common cv information
          if( first_in_all_cv)
          {
            common_cv_info = *itr_cv_info;
            first_in_all_cv = false;
          }
          else
          {
            common_cv_info.KeepSharedInfo( *itr_cv_info);
          }
          if( is_first)
          {
            ids_matrix = sp_dataset->GetIds();
            results_matrix = sp_dataset->GetResults();
            is_first = false;
          }
          else
          {
            BCL_Assert( ids_matrix.GetMatrix() == sp_dataset->GetIdsReference(), "Incorrect ids matrix");
            BCL_Assert( results_matrix.GetMatrix() == sp_dataset->GetResultsReference(), "Incorrect results matrix");
          }

          avg_prediction_matrices += sp_dataset->GetFeaturesReference();
        }
        // append result for this cross validation to the final_prediction_matrices vector
        if( avg_prediction_matrices.GetWeight())
        {
          final_prediction_matrices.PushBack( avg_prediction_matrices.GetAverage());
        }
        final_result_matrices.PushBack( results_matrix.GetMatrix());
        final_id_matrices.PushBack( ids_matrix.GetMatrix());
      }

      // combine all the matrices together
      linal::Matrix< float> combined_prediction_matrix, combined_result_matrix;
      linal::Matrix< char> combined_id_matrix;
      combined_prediction_matrix.Append( final_prediction_matrices);
      combined_result_matrix.Append( final_result_matrices);
      combined_id_matrix.Append( final_id_matrices);

      util::ShPtr< descriptor::Dataset> sp_dataset
      (
        new descriptor::Dataset
        (
          combined_prediction_matrix,
          combined_result_matrix,
          combined_id_matrix
        )
      );

      if( common_cv_info.GetIDsFeatureLabelSet().GetSize())
      {
        util::ShPtr< FeatureDataSet< char> > sp_id_ptr( sp_dataset->GetIdsPtr());
        sp_id_ptr->SetFeatureLabelSet( common_cv_info.GetIDsFeatureLabelSet());
      }
      return sp_dataset;
    }

    //! @brief read the merged independent dataset predictions into a dataset object, whose features are the predicted values
    //!        results are the same result values, and ids are the given ids
    storage::Vector< math::ROCCurve> RetrieveInterface::GetMergedIndependentROCCurves()
    {
      // get the merged independent predictions
      util::ShPtr< descriptor::Dataset> sp_dataset( ReadMergedIndependentPredictions());
      const linal::MatrixReference< float> exp_ref( sp_dataset->GetResultsReference());
      const linal::MatrixReference< float> predictions_ref( sp_dataset->GetFeaturesReference());

      // get the objective function
      util::Implementation< ObjectiveFunctionInterface> objective( this->RetrieveCommonCVInfo().GetObjective());
      BCL_Assert
      (
        objective->GetGoalType() == ObjectiveFunctionInterface::e_RankClassification,
        "GetMergedIndependentROCCurve requires a model trained against a rank-classification-based classifier"
      );

      // construct roc curves
      return ROCCurvesFromDataset( exp_ref, predictions_ref, objective->GetThreshold(), objective->GetRankingParity());
    }

    //! @brief transform an experimental/predicted matrix into a series of ROC curves
    //! @param EXP experimental results
    //! @param PRED predicted results
    //! @param THRESHOLD threshold for defining Positive/Negative class split
    //! @param POS_ABOVE_THRESH true if positives are those above the threshold
    storage::Vector< math::ROCCurve> RetrieveInterface::ROCCurvesFromDataset
    (
      const linal::MatrixConstInterface< float> &EXP,
      const linal::MatrixConstInterface< float> &PRED,
      const float &THRESHOLD,
      const bool &POS_ABOVE_THRESH
    )
    {
      const size_t n_results( EXP.GetNumberCols());
      const size_t n_predictions( EXP.GetNumberRows());
      // create ROC curves for each result
      storage::Vector< math::ROCCurve> roc_curves( n_results);
      BCL_Assert( util::IsDefined( THRESHOLD), "A threshold is necessary for ROC curves");
      BCL_Assert( n_results == PRED.GetNumberCols(), "ConstructROCCurves given matrices of different sizes");
      for( size_t result_number( 0); result_number < n_results; ++result_number)
      {
        // collect all results for the result_number-th result
        storage::List< storage::Pair< double, double> > exp_predicted;
        for( size_t pred_number( 0); pred_number < n_predictions; ++pred_number)
        {
          exp_predicted.PushBack
          (
            storage::Pair< double, double>
            (
              PRED( pred_number, result_number),
              EXP( pred_number, result_number)
            )
          );
        }
        roc_curves( result_number) = math::ROCCurve( exp_predicted, THRESHOLD, POS_ABOVE_THRESH);
      }
      return roc_curves;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_score_dataset_binary_operation.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_divide_equals.h"
#include "math/bcl_math_minus_equals.h"
#include "math/bcl_math_plus_equals.h"
#include "math/bcl_math_power_equals.h"
#include "math/bcl_math_times_equals.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
  //////////
  // data //
  //////////

    const util::SiPtr< const util::ObjectInterface> ScoreDatasetBinaryOperation::s_Add
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance
      (
        new ScoreDatasetBinaryOperation( math::PlusEquals< float>())
      )
    );

    const util::SiPtr< const util::ObjectInterface> ScoreDatasetBinaryOperation::s_Subtract
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance
      (
        new ScoreDatasetBinaryOperation( math::MinusEquals< float>())
      )
    );

    const util::SiPtr< const util::ObjectInterface> ScoreDatasetBinaryOperation::s_Divide
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance
      (
        new ScoreDatasetBinaryOperation( math::DivideEquals< float>())
      )
    );

    const util::SiPtr< const util::ObjectInterface> ScoreDatasetBinaryOperation::s_Multiply
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance
      (
        new ScoreDatasetBinaryOperation( math::TimesEquals< float>())
      )
    );

    const util::SiPtr< const util::ObjectInterface> ScoreDatasetBinaryOperation::s_Exponentiate
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance
      (
        new ScoreDatasetBinaryOperation( math::PowerEquals< float>())
      )
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief constructor from an operation
    ScoreDatasetBinaryOperation::ScoreDatasetBinaryOperation
    (
      const math::AssignmentOperationInterface< float> &OPERATION
    ) :
      m_Scores( 2),
      m_Op( OPERATION)
    {
    }

    //! @brief virtual copy constructor
    ScoreDatasetBinaryOperation *ScoreDatasetBinaryOperation::Clone() const
    {
      return new ScoreDatasetBinaryOperation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &ScoreDatasetBinaryOperation::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief return the name of the property without any parameters
    //! @return name of the property as string
    const std::string &ScoreDatasetBinaryOperation::GetAlias() const
    {
      return m_Op->GetVerb();
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief score a given dataset
    //! @param DATASET dataset of interest
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetBinaryOperation::Score( const descriptor::Dataset &DATASET) const
    {
      // instantiate vector to store the result of the next property
      if( m_Scores.GetSize() < size_t( 2))
      {
        return linal::Vector< float>();
      }

      // ensure that the properties are defined
      if( !m_Scores( 0).IsDefined() || !m_Scores( 1).IsDefined())
      {
        return linal::Vector< float>();
      }

      // initialize the descriptor return vector with the first descriptor
      linal::Vector< float> result( m_Scores( 0)->Score( DATASET));

      // get a reference to the operation
      const math::AssignmentOperationInterface< float> &operation( *m_Op);

      for
      (
        storage::Vector< util::Implementation< ScoreDatasetInterface> >::const_iterator
          itr_scores( m_Scores.Begin() + 1), itr_scores_end( m_Scores.End());
        itr_scores != itr_scores_end;
        ++itr_scores
      )
      {
        if( !itr_scores->IsDefined())
        {
          return linal::Vector< float>();
        }

        // calculate the next property
        linal::Vector< float> rhs( ( *itr_scores)->Score( DATASET));

        // check for vectors of incorrect size -> indicates scores that were not calculated
        if( rhs.GetSize() != result.GetSize())
        {
          return linal::Vector< float>();
        }

        for // perform the operation on each element of the vector
        (
          linal::Vector< float>::iterator itr_result( result.Begin()), itr_rhs( rhs.Begin()), itr_rhs_end( rhs.End());
          itr_rhs != itr_rhs_end;
          ++itr_rhs, ++itr_result
        )
        {
          operation( *itr_result, *itr_rhs);
        }
      }

      // return vector of added properties.
      return result;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetBinaryOperation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( m_Op->GetVerb() + " two scores");

      // determine whether the lhs and rhs are interchangeable
      if( m_Op.GetAlias() == "+" || m_Op.GetAlias() == "*")
      {
        parameters.AddInitializer
        (
          "",
          "Scores to " + m_Op->GetVerb(),
          io::Serialization::GetAgentWithSizeLimits( &m_Scores, size_t( 2)) // require at least two scores
        );
      }
      else
      {
        parameters.AddInitializer
        (
          "lhs",
          "argument for the left hand side of the operation",
          io::Serialization::GetAgent( &m_Scores( 0))
        );
        parameters.AddInitializer
        (
          "rhs",
          "argument for the right hand side of the operation",
          io::Serialization::GetAgent( &m_Scores( 1))
        );
      }

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_f_score.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"
#include "util/bcl_util_enumerated.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetFScore::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetFScore())
    );

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetFScore
    ScoreDatasetFScore *ScoreDatasetFScore::Clone() const
    {
      return new ScoreDatasetFScore( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetFScore::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetFScore>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetFScore::GetAlias() const
    {
      static const std::string s_Name( "FScore");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief calculate f-score values for every column in combined active and inactive dataset
    //!                                     (X_avg_pos - X_avg)^2 + (X_avg_neg - X_avg)^2
    //! f(i)= -------------------------------------------------------------------------------------------------------
    //!       1/(n_pos - 1) SUM k=1 to n_pos (X_k_pos - X_avg)^2 + 1/(n_neg - 1) SUM k=1 to n_neg (X_k_neg - X_avg)^2
    //!
    //! @param FEATURES matrix of features
    //! @param RESULTS matrix of results
    //! @return scores of the dataset
    //! @brief score a given dataset
    //! @param DATASET dataset of interest
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetFScore::Score( const descriptor::Dataset &DATASET) const
    {
      // determine # of features, results, and dataset size
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t result_size( DATASET.GetResultSize());
      const size_t dataset_size( DATASET.GetSize());
      const linal::MatrixConstReference< float> features( DATASET.GetFeaturesReference());
      const linal::MatrixConstReference< float> results( DATASET.GetResultsReference());

      // handle empty dataset
      if( dataset_size == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      // for multiple result columns, return a score of the mean f-score for all result columns
      math::RunningAverage< linal::Vector< float> > mean_fscores( linal::Vector< float>( result_size, 0.0));

      for( size_t result_index( 0); result_index < result_size; ++result_index)
      {
        // setup running average / variancess for the actives, inactives, and variance
        // initialize them all with a feature vector so that the running average will be for vectors of the right size
        math::RunningAverageSD< linal::Vector< float> > actives_stats, inactives_stats;

        // compute averages
        for( size_t counter( 0); counter < dataset_size; ++counter)
        {
          // check if the result is above the cutoff, e.g. nominally active
          if( results[ counter][ result_index] > m_Cutoff)
          {
            // add the feature to the averages for the actives
            actives_stats += linal::VectorConstReference< float>( feature_size, features[ counter]);
          }
          else
          {
            // add the feature to the averages for the inactives
            inactives_stats += linal::VectorConstReference< float>( feature_size, features[ counter]);
          }
        }

        if( actives_stats.GetWeight() == double( 0.0) || inactives_stats.GetWeight() == double( 0.0))
        {
          BCL_MessageCrt
          (
            "Ignoring result column " + util::Format()( result_index) + " because it contained no values "
            + ( actives_stats.GetWeight() == double( 0.0) ? " above " : " below ") + util::Format()( m_Cutoff)
          );
          continue;
        }

        math::RunningAverage< linal::Vector< float> > averages_total( actives_stats.GetAverage());

        averages_total.AddWeightedObservation( actives_stats.GetAverage(), actives_stats.GetWeight());
        averages_total.AddWeightedObservation( inactives_stats.GetAverage(), inactives_stats.GetWeight());

        // calculate parts of f-score, beginning with the numerator

        // f-score initialized with variance between the average actives and average totals
        linal::Vector< float> fscore( linal::SqrVector( actives_stats.GetAverage() - averages_total.GetAverage()));

        // add the variance between the averages inactives and averages total
        fscore += linal::SqrVector( inactives_stats.GetAverage() - averages_total.GetAverage());

        // n / (n - 1) * sample-variance = population variance
        //! @see http://en.wikipedia.org/wiki/Bessel%27s_correction
        // the fscore formula uses the population variance, but we have computed the sample variance, so we need to
        // multiply the values by these ratios, for the actives and inactives, respectively
        const float sample_population_variance_ratio_actives
        (
          actives_stats.GetWeight() / ( actives_stats.GetWeight() - 1.0)
        );
        const float sample_population_variance_ratio_inactives
        (
          inactives_stats.GetWeight() / ( inactives_stats.GetWeight() - 1.0)
        );

        // get an iterator over the fscore vector
        float *itr_fscore( fscore.Begin());

        // iterate over all standard deviations of all columns
        for
        (
          const float *itr_var_actives( actives_stats.GetVariance().Begin()),
                      *itr_var_inactives( inactives_stats.GetVariance().Begin()),
                      *itr_var_actives_end( actives_stats.GetVariance().End());
          itr_var_actives != itr_var_actives_end;
          ++itr_var_actives, ++itr_var_inactives, ++itr_fscore
        )
        {
          // denominator
          float denominator( *itr_var_actives * sample_population_variance_ratio_actives);
          denominator += *itr_var_inactives * sample_population_variance_ratio_inactives;
          *itr_fscore /= denominator;
          if( !util::IsDefined( *itr_fscore))
          {
            *itr_fscore = 0.0;
          }
        }
        mean_fscores += fscore;
      }

      // return f-scores for every feature column in dataset
      return mean_fscores.GetAverage();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetFScore::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses fscore, see Yi-Wei Chen and Chih-Jen Lin. Combining SVMs with various feature selection strategies."
        "In Feature extraction, foundations and applications. Springer, 2006."
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates actives from inactives",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_input_sensitivity.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_feature_data_reference.h"
#include "model/bcl_model_score_dataset_neural_network_input_sensitivity.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"
#include "util/bcl_util_sh_ptr_list.h"
#include "util/bcl_util_time.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetInputSensitivity::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetInputSensitivity())
    );

    // Map from initializer string/type to the shptrvector of model interfaces; saves loading potentially
    // gigantic models repetitively
    storage::Map< std::string, RetrieveInterface::t_Container> ScoreDatasetInputSensitivity::s_Models =
      storage::Map< std::string, RetrieveInterface::t_Container>();

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetInputSensitivity
    ScoreDatasetInputSensitivity *ScoreDatasetInputSensitivity::Clone() const
    {
      return new ScoreDatasetInputSensitivity( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetInputSensitivity::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetInputSensitivity>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetInputSensitivity::GetAlias() const
    {
      static const std::string s_Name( "InputSensitivity");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score features and results
    //! @param FEATURES matrix of features
    //! @param RESULTS matrix of results
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetInputSensitivity::Score( const descriptor::Dataset &DATASET) const
    {
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t result_size( DATASET.GetResultSize());
      const size_t dataset_size( DATASET.GetSize());

      // handle empty dataset
      if( dataset_size == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      // get a reference to the features and results
      linal::MatrixConstReference< float> features_ref( DATASET.GetFeaturesReference());
      linal::MatrixConstReference< float> results_ref( DATASET.GetResultsReference());

      // standard deviations scaled by delta
      linal::Vector< float> scaled_sd;
      math::RunningAverageSD< linal::Vector< float> > result_std;
      {
        // setup running averages / standard deviations
        // initialize them all with a feature vector so that the running statistic will be for vectors of the right size
        math::RunningAverageSD< linal::Vector< float> > ave_std;

        // compute averages and variances
        for( size_t counter( 0); counter < dataset_size; ++counter)
        {
          // add the feature to the averages for the actives
          ave_std += features_ref.GetRow( counter);
          result_std += results_ref.GetRow( counter);
        }

        // compute standard deviation for each feature
        scaled_sd = ave_std.GetSampleStandardDeviation();
        scaled_sd *= m_Delta;
        m_ResultStd = result_std.GetSampleStandardDeviation();
        for( size_t counter( 0); counter < result_size; ++counter)
        {
          if( m_ResultStd( counter) < float( 0.001))
          {
            m_ResultStd( counter) = float( 0.001);
          }
        }
      }
      m_DescriptorStd = util::ToSiPtr( scaled_sd);
      m_DatasetPtr    = util::ToSiPtr( DATASET);
      m_FeatureNumber = 0;

      // determine the # of threads to use
      size_t n_threads( std::max( size_t( 1), sched::GetScheduler().GetNumberUnusedCPUS()));

      m_AveDescriptorScores =
        storage::Vector< math::RunningAverage< linal::Vector< float> > >
        (
          n_threads,
          math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( feature_size, 0.0))
        );

      // initially, just predict all results
      m_FeaturesToComputeSize = 0;
      m_Predictions =
          storage::Vector< linal::Matrix< float> >
          (
            dataset_size,
            linal::Matrix< float>( result_size, m_Models->GetSize())
          );
      m_ChoseFeatures = false;

      util::ShPtrVector< sched::JobInterface> jobs( n_threads);
      storage::Vector< size_t> thread_ids( n_threads, size_t( 0));

      // initialize data for each thread
      for( size_t thread_number( 0); thread_number < n_threads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
        jobs( thread_number) =
          util::ShPtr< sched::JobInterface>
          (
            new sched::UnaryFunctionJobWithData< const size_t, void, ScoreDatasetInputSensitivity>
            (
              0,
              *this,
              &ScoreDatasetInputSensitivity::RunThread,
              thread_ids( thread_number),
              sched::JobInterface::e_READY,
              NULL
            )
          );
        sched::GetScheduler().SubmitJob( jobs( thread_number));
      }
      sched::GetScheduler().Join( jobs( 0));
      for( size_t thread_number( 1); thread_number < n_threads; ++thread_number)
      {
        sched::GetScheduler().Join( jobs( thread_number));
        if( m_AveDescriptorScores( thread_number).GetWeight())
        {
          if( !m_AveDescriptorScores( 0).GetWeight())
          {
            m_AveDescriptorScores( 0) = m_AveDescriptorScores( thread_number);
          }
          else
          {
            m_AveDescriptorScores( 0).AddWeightedObservation
            (
              m_AveDescriptorScores( thread_number).GetAverage(),
              m_AveDescriptorScores( thread_number).GetWeight()
            );
          }
        }
      }

      // return input-sensitivity for every feature column in dataset
      return m_AveDescriptorScores( 0).GetAverage();
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ScoreDatasetInputSensitivity::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      // get the models if they are not already in memory
      RetrieveInterface::t_Container &models( s_Models[ GetString()]);

      if( m_Key.empty())
      {
        // get all the keys for this storage
        storage::Vector< std::string> keys( m_ModelStorage->GetAllKeys());

        BCL_MessageStd( "Found " + util::Format()( keys.GetSize()) + " models for input sensitivity");

        if( models.IsEmpty() && !keys.IsEmpty())
        {
          // no models already loaded
          RetrieveInterface::t_Container interfaces( m_ModelStorage->RetrieveEnsemble( keys));
          models = RetrieveInterface::t_Container( interfaces.Begin(), interfaces.End());
        }
        if( keys.GetSize() != models.GetSize())
        {
          ERR_STREAM << "# of models in " << m_ModelStorage->GetString() << " changed; aborting";
          return false;
        }
        else if( keys.IsEmpty())
        {
          ERR_STREAM << "No models found in storage! " << m_ModelStorage->GetString();
          return false;
        }
      }
      else if( models.IsEmpty())
      {
        // no models already loaded
        models.PushBack( m_ModelStorage->Retrieve( m_Key));
        if( !models.LastElement().IsDefined())
        {
          ERR_STREAM << m_Key << " is not a key for " << m_ModelStorage->GetString();
          return false;
        }
      }
      if( !m_Objective.TryRead( m_ModelStorage->RetrieveEnsembleCVInfo().FirstElement().GetObjective(), ERR_STREAM))
      {
        ERR_STREAM << "Could not read objective function, aborting";
        return false;
      }

      m_Models = util::ToSiPtr( models);
      return true;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetInputSensitivity::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Calculates the sensitivity of the model to a change in inputs");

      parameters.AddInitializer
      (
        "storage",
        "type of storage for models",
        io::Serialization::GetAgent( &m_ModelStorage)
      );

      parameters.AddInitializer
      (
        "key",
        "optional key of the desired model in the storage",
        io::Serialization::GetAgent( &m_Key),
        ""
      );

      parameters.AddInitializer
      (
        "delta",
        "change of each feature used to calculate the derivative; will be scaled by the standard deviation of the feature",
        io::Serialization::GetAgentWithRange( &m_Delta, 0.0, 200.0),
        "0.5"
      );

      parameters.AddInitializer
      (
        "weights",
        "Change the weighting of various measures the derivative scores",
        io::Serialization::GetAgent( &m_Scorer)
      );

      parameters.AddInitializer
      (
        "feature limit",
        "limit the # of features that are used.   Maximum # of features to compute input sensitivity for, 0 to use them all. "
        "If the final objective function was classification-based, selects features with the highest value of: "
        " # models correct * # models incorrect / ( # models correct ^ 2 + # models incorrect ^ 2) "
        " For regression targets, selects features with the highest result RMSD standard deviation",
        io::Serialization::GetAgent( &m_MaxFeatures),
        "0"
      );

      return parameters;
    }

    //! @brief Get the next feature for a thread to make predictions for
    //! @param LAST_FEATURE last feature that this thread computed
    //! @return the next feature for a thread to make predictions for
    size_t ScoreDatasetInputSensitivity::GetNextFeatureForPrediction( const size_t &LAST_FEATURE) const
    {
      m_FeatureNumberMutex.Lock();
      const size_t dataset_size( m_DatasetPtr->GetSize());

      if( util::IsDefined( LAST_FEATURE))
      {
        BCL_Assert
        (
          !m_ChoseFeatures,
          "Should not have already chosen features while other threads are still predicting!"
        );
        m_FeaturesToCompute.PushBack( std::make_pair( 0.0, LAST_FEATURE));
        ++m_FeaturesToComputeSize;
      }
      else if( m_ChoseFeatures)
      {
        m_FeatureNumberMutex.Unlock();
        // thread never had to predict anything; all predictions already made by other threads
        return dataset_size;
      }

      if( m_FeatureNumber == dataset_size)
      {
        if( m_FeaturesToComputeSize == dataset_size)
        {
          ChooseInputSensitivityFeatures();
          m_FeatureNumber = 0;
          m_ChoseFeatures = true;
        }
        m_FeatureNumberMutex.Unlock();
        return dataset_size;
      }

      // only write out status from the first thread
      // determine progress percent
      const size_t percent( float( m_FeatureNumber) * 100.0f / dataset_size);

      // determine number of stars in the status bar
      const size_t number_stars( percent / 5);

      const std::string status
      (
        "["
        + std::string( number_stars, '*')
        + std::string( 20 - number_stars, ' ')
        + "] "
        + util::Format()( percent) + "% "
        + util::Format()( m_FeatureNumber) + " / " + util::Format()( size_t( dataset_size))
        + " features predicted with"
      );

      const size_t number_to_return( m_FeatureNumber);

      ++m_FeatureNumber;
      util::GetLogger().LogStatus( status);
      m_FeatureNumberMutex.Unlock();
      return number_to_return;
    }

    //! @brief Get the next feature for a thread to compute sensitivity for
    //! @return the next feature for a thread to use input sensitivity with
    size_t ScoreDatasetInputSensitivity::GetNextFeatureForInputSensitivity() const
    {
      m_FeatureNumberMutex.Lock();
      const size_t dataset_size( m_DatasetPtr->GetSize());

      // check for whether this thread is done
      if( !m_FeaturesToComputeSize)
      {
        m_FeatureNumberMutex.Unlock();
        return dataset_size;
      }

      const size_t n_to_choose( m_MaxFeatures ? std::min( m_MaxFeatures, dataset_size) : dataset_size);

      // only write out status from the first thread
      // determine progress percent
      const size_t percent( float( m_FeatureNumber) * 100.0f / n_to_choose);

      // determine number of stars in the status bar
      const size_t number_stars( percent / 5);

      const std::string status
      (
        "["
        + std::string( number_stars, '*')
        + std::string( 20 - number_stars, ' ')
        + "] "
        + util::Format()( percent) + "% "
        + util::Format()( m_FeatureNumber) + " / " + util::Format()( size_t( n_to_choose))
        + " features tested"
      );

      const size_t number_to_return( m_FeaturesToCompute.FirstElement().second);
      m_FeaturesToCompute.PopFront();
      --m_FeaturesToComputeSize;

      ++m_FeatureNumber;
      util::GetLogger().LogStatus( status);
      m_FeatureNumberMutex.Unlock();
      return number_to_return;
    }

    //! @brief choose the features on which to compute input sensitivity
    void ScoreDatasetInputSensitivity::ChooseInputSensitivityFeatures() const
    {
      const size_t result_size( m_DatasetPtr->GetResultSize());
      const size_t dataset_size( m_DatasetPtr->GetSize());
      const size_t n_models( m_Models->GetSize());

      linal::Matrix< float> model_predictions( dataset_size, result_size);
      m_PredictionClassifications = storage::Vector< linal::Matrix< char> >( n_models);
      FeatureDataReference< float> experimental_results_fds( m_DatasetPtr->GetResultsReference());
      if( m_DatasetPtr->GetIdsPtr().IsDefined())
      {
        m_Objective->SetData( experimental_results_fds, *m_DatasetPtr->GetIdsPtr());
      }
      else
      {
        m_Objective->SetData( experimental_results_fds);
      }
      FeatureDataReference< float> model_predictions_fds( model_predictions);

      // iterate over each model's predictions
      for( size_t model( 0); model < n_models; ++model)
      {
        // translate the predictions from m_Predictions into the predictions for this model
        for( size_t datum( 0); datum < dataset_size; ++datum)
        {
          for( size_t result( 0); result < result_size; ++result)
          {
            model_predictions( datum, result) = m_Predictions( datum)( result, model);
          }
        }
        // have the objective function compute the classifications
        m_PredictionClassifications( model)
          = m_Objective->GetFeaturePredictionClassifications( experimental_results_fds, model_predictions_fds);
      }
      m_Scorer.InitializeBalancing( m_PredictionClassifications, m_DatasetPtr->GetFeatureSize());

      if( !m_MaxFeatures || m_FeaturesToComputeSize <= m_MaxFeatures)
      {
        return;
      }

      // score each feature
      for
      (
        storage::List< std::pair< float, size_t> >::iterator
          itr( m_FeaturesToCompute.Begin()), itr_end( m_FeaturesToCompute.End());
        itr != itr_end;
        ++itr
      )
      {
        storage::Vector< std::string> model_classifications
        (
          ScoreDatasetNeuralNetworkInputSensitivity::PartitionModels( m_PredictionClassifications, itr->second)
        );

        // compute a score based off of the number of models that got this feature wrong
        math::RunningAverage< float> ave_result_reduced_mass;
        for( size_t result( 0); result < result_size; ++result)
        {
          size_t number_correct( 0);
          for( size_t model_number( 0); model_number < n_models; ++model_number)
          {
            if( isupper( m_PredictionClassifications( model_number)( itr->second, result)))
            {
              ++number_correct;
            }
          }
          // compute reduced mass
          ave_result_reduced_mass +=
            float( 4 * number_correct * ( n_models - number_correct))
            / float( math::Sqr( number_correct) + math::Sqr( n_models - number_correct));
        }
        itr->first = ave_result_reduced_mass.GetAverage();
      }

      // descending sort
      m_FeaturesToCompute.Sort( std::greater< std::pair< float, size_t> >());

      // find the given location
      storage::List< std::pair< float, size_t> >::iterator itr( m_FeaturesToCompute.Begin());
      std::advance( itr, m_MaxFeatures);

      // get the value
      float score( itr->first);

      // find the surrounding range with the same value
      storage::List< std::pair< float, size_t> >::iterator itr_start( itr), itr_last( itr);
      for
      (
        storage::List< std::pair< float, size_t> >::iterator itr_begin( m_FeaturesToCompute.Begin());
        itr_start != itr_begin && itr_start->first == score;
        --itr_start
      )
      {
      }
      if( itr_start->first != score)
      {
        ++itr_start;
      }
      for
      (
        storage::List< std::pair< float, size_t> >::iterator itr_end( m_FeaturesToCompute.End());
        itr_last != itr_end && itr_last->first == score;
        ++itr_last
      )
      {
      }
      storage::List< std::pair< float, size_t> >::iterator itr_last_prev( itr_last);
      --itr_last_prev;
      if( itr_last != m_FeaturesToCompute.End())
      {
        m_FeaturesToCompute.Remove( itr_last, m_FeaturesToCompute.End());
        itr_last = m_FeaturesToCompute.End();
      }
      if( itr != itr_last_prev)
      {
        // create a vector with the cases that had the same scores
        std::vector< std::pair< float, size_t> > same_scores( itr_start, itr_last);
        std::random_shuffle( same_scores.begin(), same_scores.end());
        // erase the list following itr_start
        m_FeaturesToCompute.Remove( itr_start, m_FeaturesToCompute.End());
        m_FeaturesToComputeSize = m_FeaturesToCompute.GetSize();
        size_t n_to_append( m_MaxFeatures - m_FeaturesToComputeSize);
        // append the elements from the vector that were selected
        m_FeaturesToCompute.Append
        (
          same_scores.begin(),
          same_scores.begin() + n_to_append
        );
      }
      m_FeaturesToComputeSize = m_MaxFeatures;
    }

    //! @brief run a thread to compute the kernel for all features with ID = THREAD_ID % n_threads
    //! @param THREAD_ID id of the thread to run (0-indexed)
    void ScoreDatasetInputSensitivity::RunThread( const size_t &THREAD_ID) const
    {
      const size_t feature_size( m_DatasetPtr->GetFeatureSize());
      const size_t result_size( m_DatasetPtr->GetResultSize());
      const size_t dataset_size( m_DatasetPtr->GetSize());

      const RetrieveInterface::t_Container &models( *m_Models);
      const size_t n_models( models.GetSize());

      // get a reference to the features and results
      linal::MatrixConstReference< float> features_ref( m_DatasetPtr->GetFeaturesReference());

      // store all weight effect matrices
      storage::Vector< linal::Matrix< float> > weight_effects
      (
        n_models,
        linal::Matrix< float>( feature_size, result_size)
      );

      linal::MatrixConstReference< float> dataset_reference( m_DatasetPtr->GetFeaturesReference());
      linal::MatrixConstReference< float> results_reference( m_DatasetPtr->GetResultsReference());

      linal::Matrix< float> model_predictions( result_size, n_models);

      // make predictions.  Threads work together to generate all predictions
      for
      (
        size_t feature_number( GetNextFeatureForPrediction( util::GetUndefined< size_t>()));
        feature_number < dataset_size;
        feature_number = GetNextFeatureForPrediction( feature_number)
      )
      {
        // create a feature data set containing just that feature
        linal::Matrix< float> features_matrix( size_t( 1), feature_size, dataset_reference[ feature_number]);
        FeatureDataSet< float> original_feature( features_matrix);
        linal::Matrix< float> &model_predictions( m_Predictions( feature_number));
        // for each model
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          // get a reference to the model
          const Interface &model( *models( model_number));

          // compute the original results with this model
          const FeatureDataSet< float> original_results_fds( model( original_feature));
          const linal::VectorConstReference< float> original_results( original_results_fds.GetMatrix().GetRow( 0));
          for( size_t result_number( 0); result_number < result_size; ++result_number)
          {
            model_predictions( result_number, model_number) = original_results( result_number);
          }
        }
      }

      // barrier until all predictions were made
      while( !m_ChoseFeatures)
      {
        util::Time::Delay( util::Time( 0, 1));
      }

      // compute input sensitivities on the selected features
      for
      (
        size_t feature_number( GetNextFeatureForInputSensitivity());
        feature_number < dataset_size;
        feature_number = GetNextFeatureForInputSensitivity()
      )
      {
        // create a feature data set containing just that feature
        linal::Matrix< float> features_matrix( size_t( 1), feature_size, dataset_reference[ feature_number]);
        linal::Matrix< float> &model_predictions( m_Predictions( feature_number));

        // compute the original results with this model
        const linal::Matrix< float> original_results( model_predictions.Transposed());

        // for each model
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          linal::Matrix< float> &model_weight_effects( weight_effects( model_number));

          // get a reference to the model
          const Interface &model( *models( model_number));

          FeatureDataReference< float> features = linal::MatrixConstReference< float>( features_matrix);
          linal::VectorConstReference< float> original_results_row( original_results.GetRow( model_number));
          for( size_t descriptor_col( 0); descriptor_col < feature_size; ++descriptor_col)
          {
            const float descriptor_std( ( *m_DescriptorStd)( descriptor_col));
            features_matrix( 0, descriptor_col) += descriptor_std;

            // test the perturbed matrix
            FeatureDataSet< float> perturbed_results( model( features));
            float *itr_effect( model_weight_effects[ descriptor_col]);
            for
            (
              const float *itr( original_results_row.Begin()),
                          *itr_end( original_results_row.End()),
                          *itr_perturbed( perturbed_results.GetMatrix().Begin());
              itr != itr_end;
              ++itr, ++itr_perturbed, ++itr_effect
            )
            {
              *itr_effect = *itr_perturbed - *itr;
            }
            features_matrix( 0, descriptor_col) -= descriptor_std;
          }
        }

        m_Scorer.Score
        (
          weight_effects,
          ScoreDatasetNeuralNetworkInputSensitivity::PartitionModels( m_PredictionClassifications, feature_number),
          m_AveDescriptorScores( THREAD_ID)
        );
      }
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_input_sensitivity_discrete.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_feature_data_reference.h"
#include "model/bcl_model_score_dataset_neural_network_input_sensitivity.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"
#include "util/bcl_util_sh_ptr_list.h"
#include "util/bcl_util_time.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetInputSensitivityDiscrete::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetInputSensitivityDiscrete())
    );

    // Map from initializer string/type to the shptrvector of model interfaces; saves loading potentially
    // gigantic models repetitively
    storage::Map< std::string, RetrieveInterface::t_Container> ScoreDatasetInputSensitivityDiscrete::s_Models =
      storage::Map< std::string, RetrieveInterface::t_Container>();

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetInputSensitivityDiscrete
    ScoreDatasetInputSensitivityDiscrete *ScoreDatasetInputSensitivityDiscrete::Clone() const
    {
      return new ScoreDatasetInputSensitivityDiscrete( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief Get the name, description and function of the given derivative type
    //! @param DERIVATIVE the atom type used to calculate sensitivity score
    //! @return the short name or abbreviation of the class
    const storage::Pair< std::string, std::string> &ScoreDatasetInputSensitivityDiscrete::GetDerivativeTypeInfo(
      const Derivative &DERIVATIVE)
    {
      typedef storage::Pair< std::string, std::string> t_Info;
      static const t_Info s_info[ s_NumberDerivative] =
      {
          t_Info( "Decrement", "Effects when remove 1 in input value"),
          t_Info( "Increment", "Effects when adding 1 in input value"),
          t_Info
          (
            "Center", "non-negative : the input value is at optimal"
            "Positive: the input value is not at optimal and should be changed"
            "Magnitude: the abs(decrement derivative + abs(increment derivative)"
          ),
          t_Info
          (
            "Direction", "0 - the input value is in local optimum,"
            "Negative : the output can be improved the most when subtracting 1 from the value of the input,"
            "the absolute value is decrement derivative,"
            "Positive : the output can be improved the most when adding 1 to the value of the input,"
            "the absolute value is increment derivative"
          )
      };
      return s_info[ DERIVATIVE];
    }

    //! @brief derivative type as string
    //! @param DERIVATIVE the name of the derivative type
    //! @return the string for the derivative type
    const std::string &ScoreDatasetInputSensitivityDiscrete::GetDerivativeTypeName
    (
      const Derivative &DERIVATIVE
    )
    {
      return GetDerivativeTypeInfo( DERIVATIVE).First();
    }

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetInputSensitivityDiscrete::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetInputSensitivityDiscrete>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetInputSensitivityDiscrete::GetAlias() const
    {
      static const std::string s_Name( "InputSensitivityDiscrete");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score features and results
    //! @param FEATURES matrix of features
    //! @param RESULTS matrix of results
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetInputSensitivityDiscrete::Score( const descriptor::Dataset &DATASET) const
    {
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t result_size( DATASET.GetResultSize());
      const size_t dataset_size( DATASET.GetSize());

      // handle empty dataset
      if( dataset_size == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      m_DatasetPtr    = util::ToSiPtr( DATASET);
      m_FeatureNumber = 0;

      // determine the # of threads to use
      size_t n_threads( std::max( size_t( 1), sched::GetScheduler().GetNumberUnusedCPUS()));

      m_AveDescriptorScores =
        storage::Vector< math::RunningAverage< linal::Vector< float> > >
        (
          n_threads,
          math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( feature_size, 0.0))
        );

      // initially, just predict all results
      m_FeaturesToComputeSize = 0;
      m_Predictions =
          storage::Vector< linal::Matrix< float> >
          (
            dataset_size,
            linal::Matrix< float>( result_size, m_Models->GetSize())
          );
      m_ChoseFeatures = false;

      util::ShPtrVector< sched::JobInterface> jobs( n_threads);
      storage::Vector< size_t> thread_ids( n_threads, size_t( 0));

      // initialize data for each thread
      for( size_t thread_number( 0); thread_number < n_threads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
        jobs( thread_number) =
          util::ShPtr< sched::JobInterface>
          (
            new sched::UnaryFunctionJobWithData< const size_t, void, ScoreDatasetInputSensitivityDiscrete>
            (
              0,
              *this,
              &ScoreDatasetInputSensitivityDiscrete::RunThread,
              thread_ids( thread_number),
              sched::JobInterface::e_READY,
              NULL
            )
          );
        sched::GetScheduler().SubmitJob( jobs( thread_number));
      }
      sched::GetScheduler().Join( jobs( 0));
      for( size_t thread_number( 1); thread_number < n_threads; ++thread_number)
      {
        sched::GetScheduler().Join( jobs( thread_number));
        if( m_AveDescriptorScores( thread_number).GetWeight())
        {
          if( !m_AveDescriptorScores( 0).GetWeight())
          {
            m_AveDescriptorScores( 0) = m_AveDescriptorScores( thread_number);
          }
          else
          {
            m_AveDescriptorScores( 0).AddWeightedObservation
            (
              m_AveDescriptorScores( thread_number).GetAverage(),
              m_AveDescriptorScores( thread_number).GetWeight()
            );
          }
        }
      }

      // return input-sensitivity for every feature column in dataset
      return m_AveDescriptorScores( 0).GetAverage();
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ScoreDatasetInputSensitivityDiscrete::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      // get the models if they are not already in memory
      RetrieveInterface::t_Container &models( s_Models[ GetString()]);

      if( m_Key.empty())
      {
        // get all the keys for this storage
        storage::Vector< std::string> keys( m_ModelStorage->GetAllKeys());

        BCL_MessageStd( "Found " + util::Format()( keys.GetSize()) + " models for input sensitivity");

        if( models.IsEmpty() && !keys.IsEmpty())
        {
          // no models already loaded
          RetrieveInterface::t_Container interfaces( m_ModelStorage->RetrieveEnsemble( keys));
          models = RetrieveInterface::t_Container( interfaces.Begin(), interfaces.End());
        }
        if( keys.GetSize() != models.GetSize())
        {
          ERR_STREAM << "# of models in " << m_ModelStorage->GetString() << " changed; aborting";
          return false;
        }
        else if( keys.IsEmpty())
        {
          ERR_STREAM << "No models found in storage! " << m_ModelStorage->GetString();
          return false;
        }
      }
      else if( models.IsEmpty())
      {
        // no models already loaded
        models.PushBack( m_ModelStorage->Retrieve( m_Key));
        if( !models.LastElement().IsDefined())
        {
          ERR_STREAM << m_Key << " is not a key for " << m_ModelStorage->GetString();
          return false;
        }
      }
      if( !m_Objective.TryRead( m_ModelStorage->RetrieveEnsembleCVInfo().FirstElement().GetObjective(), ERR_STREAM))
      {
        ERR_STREAM << "Could not read objective function, aborting";
        return false;
      }

      m_Models = util::ToSiPtr( models);
      return true;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetInputSensitivityDiscrete::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Calculates the sensitivity of the model to a change in inputs");
      parameters.AddInitializer
      (
        "derivative",
        "Type of derivative to be computed : Decrement, Increment, Center, Direction. Center by default",
        io::Serialization::GetAgent( &m_Derivative),
        "Center"
      );

      parameters.AddInitializer
      (
        "storage",
        "type of storage for models",
        io::Serialization::GetAgent( &m_ModelStorage)
      );

      parameters.AddInitializer
      (
        "key",
        "optional key of the desired model in the storage",
        io::Serialization::GetAgent( &m_Key),
        ""
      );

      parameters.AddInitializer
      (
        "weights",
        "Change the weighting of various measures the derivative scores",
        io::Serialization::GetAgent( &m_Scorer)
      );

      parameters.AddInitializer
      (
        "feature limit",
        "limit the # of features that are used.   Maximum # of features to compute input sensitivity for, 0 to use them all. "
        "If the final objective function was classification-based, selects features with the highest value of: "
        " # models correct * # models incorrect / ( # models correct ^ 2 + # models incorrect ^ 2) "
        " For regression targets, selects features with the highest result RMSD standard deviation",
        io::Serialization::GetAgent( &m_MaxFeatures),
        "0"
      );

      return parameters;
    }

    //! @brief Get the next feature for a thread to make predictions for
    //! @param LAST_FEATURE last feature that this thread computed
    //! @return the next feature for a thread to make predictions for
    size_t ScoreDatasetInputSensitivityDiscrete::GetNextFeatureForPrediction( const size_t &LAST_FEATURE) const
    {
      m_FeatureNumberMutex.Lock();
      const size_t dataset_size( m_DatasetPtr->GetSize());

      if( util::IsDefined( LAST_FEATURE))
      {
        BCL_Assert
        (
          !m_ChoseFeatures,
          "Should not have already chosen features while other threads are still predicting!"
        );
        m_FeaturesToCompute.PushBack( std::make_pair( 0.0, LAST_FEATURE));
        ++m_FeaturesToComputeSize;
      }
      else if( m_ChoseFeatures)
      {
        m_FeatureNumberMutex.Unlock();
        // thread never had to predict anything; all predictions already made by other threads
        return dataset_size;
      }

      if( m_FeatureNumber == dataset_size)
      {
        if( m_FeaturesToComputeSize == dataset_size)
        {
          ChooseInputSensitivityFeatures();
          m_FeatureNumber = 0;
          m_ChoseFeatures = true;
        }
        m_FeatureNumberMutex.Unlock();
        return dataset_size;
      }

      // only write out status from the first thread
      // determine progress percent
      const size_t percent( float( m_FeatureNumber) * 100.0f / dataset_size);

      // determine number of stars in the status bar
      const size_t number_stars( percent / 5);

      const std::string status
      (
        "["
        + std::string( number_stars, '*')
        + std::string( 20 - number_stars, ' ')
        + "] "
        + util::Format()( percent) + "% "
        + util::Format()( m_FeatureNumber) + " / " + util::Format()( size_t( dataset_size))
        + " features predicted with"
      );

      const size_t number_to_return( m_FeatureNumber);

      ++m_FeatureNumber;
      util::GetLogger().LogStatus( status);
      m_FeatureNumberMutex.Unlock();
      return number_to_return;
    }

    //! @brief Get the next feature for a thread to compute sensitivity for
    //! @return the next feature for a thread to use input sensitivity with
    size_t ScoreDatasetInputSensitivityDiscrete::GetNextFeatureForInputSensitivity() const
    {
      m_FeatureNumberMutex.Lock();
      const size_t dataset_size( m_DatasetPtr->GetSize());

      // check for whether this thread is done
      if( !m_FeaturesToComputeSize)
      {
        m_FeatureNumberMutex.Unlock();
        return dataset_size;
      }

      const size_t n_to_choose( m_MaxFeatures ? std::min( m_MaxFeatures, dataset_size) : dataset_size);

      // only write out status from the first thread
      // determine progress percent
      const size_t percent( float( m_FeatureNumber) * 100.0f / n_to_choose);

      // determine number of stars in the status bar
      const size_t number_stars( percent / 5);

      const std::string status
      (
        "["
        + std::string( number_stars, '*')
        + std::string( 20 - number_stars, ' ')
        + "] "
        + util::Format()( percent) + "% "
        + util::Format()( m_FeatureNumber) + " / " + util::Format()( size_t( n_to_choose))
        + " features tested"
      );

      const size_t number_to_return( m_FeaturesToCompute.FirstElement().second);
      m_FeaturesToCompute.PopFront();
      --m_FeaturesToComputeSize;

      ++m_FeatureNumber;
      util::GetLogger().LogStatus( status);
      m_FeatureNumberMutex.Unlock();
      return number_to_return;
    }

    //! @brief choose the features on which to compute input sensitivity
    void ScoreDatasetInputSensitivityDiscrete::ChooseInputSensitivityFeatures() const
    {
      const size_t result_size( m_DatasetPtr->GetResultSize());
      const size_t dataset_size( m_DatasetPtr->GetSize());
      const size_t n_models( m_Models->GetSize());

      linal::Matrix< float> model_predictions( dataset_size, result_size);
      m_PredictionClassifications = storage::Vector< linal::Matrix< char> >( n_models);
      FeatureDataReference< float> experimental_results_fds( m_DatasetPtr->GetResultsReference());
      if( m_DatasetPtr->GetIdsPtr().IsDefined())
      {
        m_Objective->SetData( experimental_results_fds, *m_DatasetPtr->GetIdsPtr());
      }
      else
      {
        m_Objective->SetData( experimental_results_fds);
      }
      FeatureDataReference< float> model_predictions_fds( model_predictions);

      // iterate over each model's predictions
      for( size_t model( 0); model < n_models; ++model)
      {
        // translate the predictions from m_Predictions into the predictions for this model
        for( size_t datum( 0); datum < dataset_size; ++datum)
        {
          for( size_t result( 0); result < result_size; ++result)
          {
            model_predictions( datum, result) = m_Predictions( datum)( result, model);
          }
        }
        // have the objective function compute the classifications
        m_PredictionClassifications( model)
          = m_Objective->GetFeaturePredictionClassifications( experimental_results_fds, model_predictions_fds);
      }
      m_Scorer.InitializeBalancing( m_PredictionClassifications, m_DatasetPtr->GetFeatureSize());

      if( !m_MaxFeatures || m_FeaturesToComputeSize <= m_MaxFeatures)
      {
        return;
      }

      // score each feature
      for
      (
        storage::List< std::pair< float, size_t> >::iterator
          itr( m_FeaturesToCompute.Begin()), itr_end( m_FeaturesToCompute.End());
        itr != itr_end;
        ++itr
      )
      {
        storage::Vector< std::string> model_classifications
        (
          ScoreDatasetNeuralNetworkInputSensitivity::PartitionModels( m_PredictionClassifications, itr->second)
        );

        // compute a score based off of the number of models that got this feature wrong
        math::RunningAverage< float> ave_result_reduced_mass;
        for( size_t result( 0); result < result_size; ++result)
        {
          size_t number_correct( 0);
          for( size_t model_number( 0); model_number < n_models; ++model_number)
          {
            if( isupper( m_PredictionClassifications( model_number)( itr->second, result)))
            {
              ++number_correct;
            }
          }
          // compute reduced mass
          ave_result_reduced_mass +=
            float( 4 * number_correct * ( n_models - number_correct))
            / float( math::Sqr( number_correct) + math::Sqr( n_models - number_correct));
        }
        itr->first = ave_result_reduced_mass.GetAverage();
      }

      // descending sort
      m_FeaturesToCompute.Sort( std::greater< std::pair< float, size_t> >());

      // find the given location
      storage::List< std::pair< float, size_t> >::iterator itr( m_FeaturesToCompute.Begin());
      std::advance( itr, m_MaxFeatures);

      // get the value
      float score( itr->first);

      // find the surrounding range with the same value
      storage::List< std::pair< float, size_t> >::iterator itr_start( itr), itr_last( itr);
      for
      (
        storage::List< std::pair< float, size_t> >::iterator itr_begin( m_FeaturesToCompute.Begin());
        itr_start != itr_begin && itr_start->first == score;
        --itr_start
      )
      {
      }
      if( itr_start->first != score)
      {
        ++itr_start;
      }
      for
      (
        storage::List< std::pair< float, size_t> >::iterator itr_end( m_FeaturesToCompute.End());
        itr_last != itr_end && itr_last->first == score;
        ++itr_last
      )
      {
      }
      storage::List< std::pair< float, size_t> >::iterator itr_last_prev( itr_last);
      --itr_last_prev;
      if( itr_last != m_FeaturesToCompute.End())
      {
        m_FeaturesToCompute.Remove( itr_last, m_FeaturesToCompute.End());
        itr_last = m_FeaturesToCompute.End();
      }
      if( itr != itr_last_prev)
      {
        // create a vector with the cases that had the same scores
        std::vector< std::pair< float, size_t> > same_scores( itr_start, itr_last);
        std::random_shuffle( same_scores.begin(), same_scores.end());
        // erase the list following itr_start
        m_FeaturesToCompute.Remove( itr_start, m_FeaturesToCompute.End());
        m_FeaturesToComputeSize = m_FeaturesToCompute.GetSize();
        size_t n_to_append( m_MaxFeatures - m_FeaturesToComputeSize);
        // append the elements from the vector that were selected
        m_FeaturesToCompute.Append
        (
          same_scores.begin(),
          same_scores.begin() + n_to_append
        );
      }
      m_FeaturesToComputeSize = m_MaxFeatures;
    }

    //! @brief run a thread to compute the kernel for all features with ID = THREAD_ID % n_threads
    //! @param THREAD_ID id of the thread to run (0-indexed)
    void ScoreDatasetInputSensitivityDiscrete::RunThread( const size_t &THREAD_ID) const
    {
      const size_t feature_size( m_DatasetPtr->GetFeatureSize());
      const size_t result_size( m_DatasetPtr->GetResultSize());
      const size_t dataset_size( m_DatasetPtr->GetSize());

      const RetrieveInterface::t_Container &models( *m_Models);
      const size_t n_models( models.GetSize());

      // get a reference to the features and results
      linal::MatrixConstReference< float> features_ref( m_DatasetPtr->GetFeaturesReference());

      // store all weight effect matrices
      storage::Vector< linal::Matrix< float> > weight_effects
      (
        n_models,
        linal::Matrix< float>( feature_size, result_size)
      );

      linal::MatrixConstReference< float> dataset_reference( m_DatasetPtr->GetFeaturesReference());
      linal::MatrixConstReference< float> results_reference( m_DatasetPtr->GetResultsReference());

      linal::Matrix< float> model_predictions( result_size, n_models);

      // make predictions.  Threads work together to generate all predictions
      for
      (
        size_t feature_number( GetNextFeatureForPrediction( util::GetUndefined< size_t>()));
        feature_number < dataset_size;
        feature_number = GetNextFeatureForPrediction( feature_number)
      )
      {
        // create a feature data set containing just that feature
        linal::Matrix< float> features_matrix( size_t( 1), feature_size, dataset_reference[ feature_number]);
        FeatureDataSet< float> original_feature( features_matrix);
        linal::Matrix< float> &model_predictions( m_Predictions( feature_number));
        // for each model
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          // get a reference to the model
          const Interface &model( *models( model_number));

          // compute the original results with this model
          const FeatureDataSet< float> original_results_fds( model( original_feature));
          const linal::VectorConstReference< float> original_results( original_results_fds.GetMatrix().GetRow( 0));
          for( size_t result_number( 0); result_number < result_size; ++result_number)
          {
            model_predictions( result_number, model_number) = original_results( result_number);
          }
        }
      }

      // barrier until all predictions were made
      while( !m_ChoseFeatures)
      {
        util::Time::Delay( util::Time( 0, 1));
      }

      // compute input sensitivities on the selected features
      for
      (
        size_t feature_number( GetNextFeatureForInputSensitivity());
        feature_number < dataset_size;
        feature_number = GetNextFeatureForInputSensitivity()
      )
      {
        // create a feature data set containing just that feature
        linal::Matrix< float> features_matrix( size_t( 1), feature_size, dataset_reference[ feature_number]);
        linal::Matrix< float> &model_predictions( m_Predictions( feature_number));

        // compute the original results with this model
        const linal::Matrix< float> original_results( model_predictions.Transposed());

        // for each model
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          linal::Matrix< float> &model_weight_effects( weight_effects( model_number));

          // get a reference to the model
          const Interface &model( *models( model_number));

          FeatureDataReference< float> features = linal::MatrixConstReference< float>( features_matrix);
          linal::VectorConstReference< float> original_results_row( original_results.GetRow( model_number));

          for( size_t descriptor_col( 0); descriptor_col < feature_size; ++descriptor_col)
          {
            // Compute decrement derivative
            if( m_Derivative == ScoreDatasetInputSensitivityDiscrete::e_decrement)
            {
              if( features_matrix( 0, descriptor_col) != 0)
              {

                features_matrix( 0, descriptor_col) -= 1.0;
                FeatureDataSet< float> perturbed_decrement_results( model( features));

                // compute the final change in results
                float *itr_effect( model_weight_effects[ descriptor_col]);
                for
                (
                    const float *itr( original_results_row.Begin()),
                    *itr_end( original_results_row.End()),
                    *itr_perturbed_decrement( perturbed_decrement_results.GetMatrix().Begin());
                    itr != itr_end;
                    ++itr, ++itr_perturbed_decrement, ++itr_effect
                )
                {
                  *itr_effect = *itr_perturbed_decrement - *itr;
                  //std::cout << " wth : " << *itr_perturbed_decrement << " " << *itr << std::endl;
                }

                //! restore original features
                features_matrix( 0, descriptor_col) += 1.0;
              }
            }
            //! Compute the increment derivative
            else if( m_Derivative == ScoreDatasetInputSensitivityDiscrete::e_increment)
            {
              features_matrix( 0, descriptor_col) += 1.0;

              FeatureDataSet< float> perturbed_increment_results( model( features));

              // compute the final change in results
              float *itr_effect( model_weight_effects[ descriptor_col]);
              for
              (
                  const float *itr( original_results_row.Begin()),
                  *itr_end( original_results_row.End()),
                  *itr_perturbed_increment( perturbed_increment_results.GetMatrix().Begin());
                  itr != itr_end;
                  ++itr, ++itr_perturbed_increment, ++itr_effect
              )
              {
                *itr_effect = *itr_perturbed_increment - *itr;
              }

              // restore original features
              features_matrix( 0, descriptor_col) -= 1.0;
            }
            else
            {
              features_matrix( 0, descriptor_col) += 1.0;

              FeatureDataSet< float> perturbed_increment_results( model( features));

              // computes the results if each non 0 feature is increased by 1
              features_matrix( 0, descriptor_col) -= 1.0;
              bool not_0 = false;

              if( features_matrix( 0, descriptor_col) != 0.0)
              {
                not_0 = true;
                features_matrix( 0, descriptor_col) -= 1.0;
              }

              FeatureDataSet< float> perturbed_decrement_results( model( features));

              // compute the final change in results
              float *itr_effect( model_weight_effects[ descriptor_col]);
              for
              (
                  const float *itr( original_results_row.Begin()),
                  *itr_end( original_results_row.End()),
                  *itr_perturbed_decrement( perturbed_decrement_results.GetMatrix().Begin()),
                  *itr_perturbed_increment( perturbed_increment_results.GetMatrix().Begin());
                  itr != itr_end;
                  ++itr, ++itr_perturbed_increment, ++itr_perturbed_decrement, ++itr_effect
              )
              {
                const float &maximum( std::max( *itr, std::max( *itr_perturbed_decrement, *itr_perturbed_increment)));
                int a;
                float difference;

                //! Compute the direction derivative
                if( m_Derivative == ScoreDatasetInputSensitivityDiscrete::e_direction)
                {
                  difference = maximum - *itr;
                  if( maximum == *itr)
                  {
                    a = 0.0;
                  }
                  else if( maximum == *itr_perturbed_decrement)
                  {
                    a = -1.0;
                  }
                  else
                  {
                    a = 1.0;
                  }
                }
                else
                {
                  //! Compute the center derivative
                  if( maximum == *itr)
                  {
                    a = 1.0;
                    difference = std::abs( *itr - *itr_perturbed_decrement) + std::abs( *itr - *itr_perturbed_increment);
                  }
                  else
                  {
                    a = -1.0;
                    difference = maximum - *itr;
                  }
                }

                *itr_effect = a * ( difference);
              }
              // restore original features
              if( not_0)
              {
                features_matrix( 0, descriptor_col) += 1.0;
              }
            }
          }
        }

        m_Scorer.Score
        (
          weight_effects,
          ScoreDatasetNeuralNetworkInputSensitivity::PartitionModels( m_PredictionClassifications, feature_number),
          m_AveDescriptorScores( THREAD_ID)
        );
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_neural_network_input_sensitivity.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_reference.h"
#include "model/bcl_model_feature_data_reference.h"
#include "model/bcl_model_neural_network.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_unary_function_job_with_data.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetNeuralNetworkInputSensitivity::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetNeuralNetworkInputSensitivity())
    );

    // Map from initializer string/type to the shptrvector of model interfaces; saves loading potentially
    // gigantic models repetitively
    storage::Map< std::string, storage::Vector< util::OwnPtr< NeuralNetwork> > > ScoreDatasetNeuralNetworkInputSensitivity::s_Models =
      storage::Map< std::string, storage::Vector< util::OwnPtr< NeuralNetwork> > >();

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetNeuralNetworkInputSensitivity
    ScoreDatasetNeuralNetworkInputSensitivity *ScoreDatasetNeuralNetworkInputSensitivity::Clone() const
    {
      return new ScoreDatasetNeuralNetworkInputSensitivity( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetNeuralNetworkInputSensitivity::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetNeuralNetworkInputSensitivity>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetNeuralNetworkInputSensitivity::GetAlias() const
    {
      static const std::string s_Name( "InputSensitivityNeuralNetwork");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score features and results
    //! @param FEATURES matrix of features
    //! @param RESULTS matrix of results
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetNeuralNetworkInputSensitivity::Score( const descriptor::Dataset &DATASET) const
    {
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t dataset_size( DATASET.GetSize());

      // handle empty dataset
      if( dataset_size == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      // get a reference to the features and results
      linal::MatrixConstReference< float> features_ref( DATASET.GetFeaturesReference());
      linal::MatrixConstReference< float> results_ref( DATASET.GetResultsReference());

      m_DatasetPtr    = util::ToSiPtr( DATASET);
      m_FeatureNumber = 0;

      // make predictions with all models
      const size_t n_models( m_Models->GetSize());
      m_PredictionClassifications = storage::Vector< linal::Matrix< char> >( n_models);
      FeatureDataReference< float> experimental_results_fds( DATASET.GetResultsReference());
      if( DATASET.GetIdsPtr().IsDefined())
      {
        m_Objective->SetData( experimental_results_fds, *DATASET.GetIdsPtr());
      }
      else
      {
        m_Objective->SetData( experimental_results_fds);
      }
      for( size_t model_number( 0); model_number < n_models; ++model_number)
      {
        // get the predictions for this model
        FeatureDataSet< float> predictions
        (
          m_Models->operator ()( model_number)->operator()
          (
            FeatureDataReference< float>( DATASET.GetFeaturesReference())
          )
        );

        // have the objective function compute the classifications
        m_PredictionClassifications( model_number)
          = m_Objective->GetFeaturePredictionClassifications( experimental_results_fds, predictions);
      }
      m_Scorer.InitializeBalancing( m_PredictionClassifications, m_DatasetPtr->GetFeatureSize());

      // determine the # of threads to use
      size_t n_threads( std::max( size_t( 1), sched::GetScheduler().GetNumberUnusedCPUS()));

      m_AveDescriptorScores =
        storage::Vector< math::RunningAverage< linal::Vector< float> > >
        (
          n_threads,
          math::RunningAverage< linal::Vector< float> >( linal::Vector< float>( feature_size, 0.0))
        );

      // initially, just predict all results
      m_FeaturesToComputeSize = 0;

      util::ShPtrVector< sched::JobInterface> jobs( n_threads);
      storage::Vector< size_t> thread_ids( n_threads, size_t( 0));

      // initialize data for each thread
      for( size_t thread_number( 0); thread_number < n_threads; ++thread_number)
      {
        thread_ids( thread_number) = thread_number;
        jobs( thread_number) =
          util::ShPtr< sched::JobInterface>
          (
            new sched::UnaryFunctionJobWithData< const size_t, void, ScoreDatasetNeuralNetworkInputSensitivity>
            (
              0,
              *this,
              &ScoreDatasetNeuralNetworkInputSensitivity::RunThread,
              thread_ids( thread_number),
              sched::JobInterface::e_READY,
              NULL
            )
          );
        sched::GetScheduler().SubmitJob( jobs( thread_number));
      }
      sched::GetScheduler().Join( jobs( 0));
      for( size_t thread_number( 1); thread_number < n_threads; ++thread_number)
      {
        sched::GetScheduler().Join( jobs( thread_number));
        if( m_AveDescriptorScores( thread_number).GetWeight())
        {
          if( !m_AveDescriptorScores( 0).GetWeight())
          {
            m_AveDescriptorScores( 0) = m_AveDescriptorScores( thread_number);
          }
          else
          {
            m_AveDescriptorScores( 0).AddWeightedObservation
            (
              m_AveDescriptorScores( thread_number).GetAverage(),
              m_AveDescriptorScores( thread_number).GetWeight()
            );
          }
        }
      }
      m_Scorer.AddUtilityScore( m_AveDescriptorScores( 0));

      // return input-sensitivity for every feature column in dataset
      return m_AveDescriptorScores( 0).GetAverage();
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ScoreDatasetNeuralNetworkInputSensitivity::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      // get the models if they are not already in memory
      storage::Vector< util::OwnPtr< NeuralNetwork> > &models( s_Models[ GetString()]);

      if( m_Key.empty())
      {
        // get all the keys for this storage
        storage::Vector< std::string> keys( m_ModelStorage->GetAllKeys());

        BCL_MessageStd( "Found " + util::Format()( keys.GetSize()) + " models for input sensitivity");

        if( models.IsEmpty() && !keys.IsEmpty())
        {
          // no models already loaded
          RetrieveInterface::t_Container interfaces( m_ModelStorage->RetrieveEnsemble( keys));
          models = storage::Vector< util::OwnPtr< NeuralNetwork> >( interfaces.Begin(), interfaces.End());
        }
        if( keys.GetSize() != models.GetSize())
        {
          ERR_STREAM << "# of models in " << m_ModelStorage->GetString() << " changed; aborting";
          return false;
        }
        else if( keys.IsEmpty())
        {
          ERR_STREAM << "No models found in storage! " << m_ModelStorage->GetString();
          return false;
        }
      }
      else if( models.IsEmpty())
      {
        // no models already loaded
        models.PushBack( m_ModelStorage->Retrieve( m_Key));
        if( !models.LastElement().IsDefined())
        {
          ERR_STREAM << m_Key << " is not a key for " << m_ModelStorage->GetString();
          return false;
        }
      }
      if( !m_Objective.TryRead( m_ModelStorage->RetrieveEnsembleCVInfo().FirstElement().GetObjective(), ERR_STREAM))
      {
        ERR_STREAM << "Could not read objective function, aborting";
        return false;
      }

      m_Models = util::ToSiPtr( models);
      return true;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetNeuralNetworkInputSensitivity::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Calculates the sensitivity of the model to a change in inputs");

      parameters.AddInitializer
      (
        "storage",
        "type of storage for models",
        io::Serialization::GetAgent( &m_ModelStorage)
      );

      parameters.AddInitializer
      (
        "key",
        "optional key of the desired model in the storage",
        io::Serialization::GetAgent( &m_Key),
        ""
      );

      parameters.AddInitializer
      (
        "weights",
        "Change the weighting of various measures the derivative scores",
        io::Serialization::GetAgent( &m_Scorer)
      );

      return parameters;
    }

    //! @brief determine the performance of the model ensemble on a particular feature
    //! @param PREDICTION_CLASSIFICATIONS vector of model prediction classifications
    //! @param FEATURE_NR feature number of interest
    //! @return a vector containing strings with P|p|N|n|\0 for whether each result is a TP, FP, TN, FN< or NA for each model
    storage::Vector< std::string> ScoreDatasetNeuralNetworkInputSensitivity::PartitionModels
    (
      const storage::Vector< linal::Matrix< char> > &PREDICTION_CLASSIFICATIONS,
      const size_t &FEATURE_NR
    )
    {
      const size_t n_models( PREDICTION_CLASSIFICATIONS.GetSize());
      const size_t result_size( PREDICTION_CLASSIFICATIONS( 0).GetNumberCols());
      storage::Vector< std::string> partition( n_models, std::string( result_size, '\0'));
      // transfer model correctness into the desired format
      for( size_t model_number( 0); model_number < n_models; ++model_number)
      {
        for( size_t result_number( 0); result_number < result_size; ++result_number)
        {
          partition( model_number)[ result_number] = PREDICTION_CLASSIFICATIONS( model_number)( FEATURE_NR, result_number);
        }
      }
      return partition;
    }

    //! @brief determine the side of the actual result
    //! @param ACTUAL actual / experimental value
    //! @param CUTOFF the cutoff value
    //! @return a vector containing size_ts 0/1/undefined depending on whether the model was bad, good, or unknown
    storage::Vector< size_t> ScoreDatasetNeuralNetworkInputSensitivity::GetCutoffSides
    (
      const linal::VectorConstInterface< float> &ACTUAL,
      const float &CUTOFF
    )
    {
      const size_t results_size( ACTUAL.GetSize());
      if( !util::IsDefined( CUTOFF))
      {
        return storage::Vector< size_t>( results_size, size_t( 0));
      }
      storage::Vector< size_t> cutoff_side( results_size, size_t( 0));
      for( size_t i( 0); i < results_size; ++i)
      {
        if( ACTUAL( i) >= CUTOFF)
        {
          cutoff_side( i) = 1;
        }
      }

      return cutoff_side;
    }

    //! @brief Get the next feature for a thread to make predictions for
    //! @param LAST_FEATURE last feature that this thread computed
    //! @return the next feature for a thread to make predictions for
    size_t ScoreDatasetNeuralNetworkInputSensitivity::GetNextFeatureForPrediction( const size_t &LAST_FEATURE) const
    {
      m_FeatureNumberMutex.Lock();
      const size_t dataset_size( m_DatasetPtr->GetSize());

      if( util::IsDefined( LAST_FEATURE))
      {
        ++m_FeaturesToComputeSize;
      }

      // only write out status from the first thread
      // determine progress percent
      const size_t percent( float( m_FeaturesToComputeSize) * 100.0f / dataset_size);

      // determine number of stars in the status bar
      const size_t number_stars( percent / 5);

      const std::string status
      (
        "["
        + std::string( number_stars, '*')
        + std::string( 20 - number_stars, ' ')
        + "] "
        + util::Format()( percent) + "% "
        + util::Format()( m_FeaturesToComputeSize) + " / " + util::Format()( size_t( dataset_size))
        + " features predicted with"
      );

      if( m_FeatureNumber == dataset_size)
      {
        m_FeatureNumberMutex.Unlock();
        return dataset_size;
      }

      const size_t number_to_return( m_FeatureNumber);

      ++m_FeatureNumber;
      util::GetLogger().LogStatus( status);
      m_FeatureNumberMutex.Unlock();
      return number_to_return;
    }

    //! @brief run a thread to compute the kernel for all features with ID = THREAD_ID % n_threads
    //! @param THREAD_ID id of the thread to run (0-indexed)
    void ScoreDatasetNeuralNetworkInputSensitivity::RunThread( const size_t &THREAD_ID) const
    {
      const size_t feature_size( m_DatasetPtr->GetFeatureSize());
      const size_t result_size( m_DatasetPtr->GetResultSize());
      const size_t dataset_size( m_DatasetPtr->GetSize());

      const storage::Vector< util::OwnPtr< NeuralNetwork> > &models( *m_Models);
      const size_t n_models( models.GetSize());

      // get a reference to the features and results
      linal::MatrixConstReference< float> features_ref( m_DatasetPtr->GetFeaturesReference());

      // store all weight effect matrices
      storage::Vector< linal::Matrix< float> > weight_effects
      (
        n_models,
        linal::Matrix< float>( feature_size, result_size)
      );

      linal::MatrixConstReference< float> dataset_reference( m_DatasetPtr->GetFeaturesReference());
      linal::MatrixConstReference< float> results_reference( m_DatasetPtr->GetResultsReference());

      linal::Matrix< float> model_predictions( result_size, n_models);

      // make predictions.  Threads work together to generate all predictions
      for
      (
        size_t feature_number( GetNextFeatureForPrediction( util::GetUndefined< size_t>()));
        feature_number < dataset_size;
        feature_number = GetNextFeatureForPrediction( feature_number)
      )
      {
        // create a feature data set containing just that feature
        linal::Matrix< float> features_matrix( size_t( 1), feature_size, dataset_reference[ feature_number]);
        FeatureDataSet< float> original_feature( features_matrix);
        // for each model
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          // get a reference to the model
          const NeuralNetwork &model( *models( model_number));
          linal::Matrix< float> &model_weight_effects( weight_effects( model_number));
          original_feature.DeScale();
          original_feature.Rescale( *model.GetRescaleInput());

          // get the rescaled vector
          linal::VectorReference< float> original_feature_vector( original_feature.GetRawMatrix().GetRow( 0));

          // get the result and the input sensitivity
          storage::Pair< linal::Vector< float>, linal::Matrix< float> >
            result_input_sensitivity( model.ComputeResultInputSensitivity( original_feature_vector));

          // compute the original results with this model
          const FeatureDataSet< float> original_results_fds( model( original_feature));
          const linal::VectorConstReference< float> results( result_input_sensitivity.First());
          for( size_t result_number( 0); result_number < result_size; ++result_number)
          {
            model_predictions( result_number, model_number) = results( result_number);
            model_weight_effects = result_input_sensitivity.Second();
          }
        }

        m_Scorer.Score
        (
          weight_effects,
          PartitionModels( m_PredictionClassifications, feature_number),
          m_AveDescriptorScores( THREAD_ID)
        );
      }
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_neural_network_weights.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_neural_network.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetNeuralNetworkWeights::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetNeuralNetworkWeights())
    );

    // Map from initializer string/type to the shptrvector of model interfaces; saves loading potentially
    // gigantic models repetitively
    storage::Map< std::string, storage::Vector< util::OwnPtr< NeuralNetwork> > > ScoreDatasetNeuralNetworkWeights::s_Models =
      storage::Map< std::string, storage::Vector< util::OwnPtr< NeuralNetwork> > >();

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetNeuralNetworkWeights
    ScoreDatasetNeuralNetworkWeights *ScoreDatasetNeuralNetworkWeights::Clone() const
    {
      return new ScoreDatasetNeuralNetworkWeights( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetNeuralNetworkWeights::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetNeuralNetworkWeights>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetNeuralNetworkWeights::GetAlias() const
    {
      static const std::string s_Name( "NeuralNetworkWeights");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score features and results
    //! @param FEATURES matrix of features
    //! @param RESULTS matrix of results
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetNeuralNetworkWeights::Score( const descriptor::Dataset &DATASET) const
    {
      const size_t feature_size( DATASET.GetFeatureSize());

      // handle empty dataset
      if( DATASET.GetSize() == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      const util::ShPtrVector< NeuralNetwork> &models( *m_Models);
      const size_t n_models( models.GetSize());

      // ensure that the # of features and results was correct
      BCL_Assert
      (
        models.FirstElement()->GetNumberInputs() == feature_size,
        "Please specify the same features as was used to train the networks"
      );

      // store all weight effect matrices
      storage::Vector< linal::Matrix< float> > weight_effects( n_models);

      // for each model
      // 1. Obtain its matrix of weights
      for( size_t model_number( 0); model_number < n_models; ++model_number)
      {
        // get the weights from this network
        const storage::Vector< linal::Matrix< float> > &weights( models( model_number)->GetWeight());
        // get the first layer weights
        linal::Matrix< float> weights_mult( weights( 0).Transposed());
        for( size_t layer_number( 1), n_layers( weights.GetSize()); layer_number < n_layers; ++layer_number)
        {
          weights_mult = weights_mult * weights( layer_number).Transposed();
        }
        weight_effects( model_number) = weights_mult;
      }

      // load information about the cross validation and how it performed
      storage::List< CrossValidationInfo> cv_info( m_ModelStorage->RetrieveEnsembleCVInfo());

      // find the set of models that belonged to each cross-validation independent set
      storage::Map< util::ObjectDataLabel, storage::Vector< float> > independent_to_result;
      storage::Map< util::ObjectDataLabel, float> independent_to_median;

      // iterate through the list of cv info
      for
      (
        storage::List< CrossValidationInfo>::const_iterator itr( cv_info.Begin()), itr_end( cv_info.End());
        itr != itr_end;
        ++itr
      )
      {
        independent_to_result[ itr->GetIndependentDatasetRetriever()].PushBack( itr->GetResult());
      }

      // determine the directionality of improvement to create
      opti::ImprovementTypeEnum
        obj_improvement_type( cv_info.FirstElement().GetImprovementType());

      const size_t results_size( DATASET.GetResultSize());
      storage::Vector< linal::Matrix< char> > prediction_classifications
      (
        n_models,
        linal::Matrix< char>( size_t( 1), results_size, '\0')
      );

      // determine the cutoff for each independent set
      for
      (
        storage::Map< util::ObjectDataLabel, storage::Vector< float> >::iterator
          itr( independent_to_result.Begin()), itr_end( independent_to_result.End());
        itr != itr_end;
        ++itr
      )
      {
        itr->second.Sort( std::less< float>());
        const size_t size( itr->second.GetSize());
        const size_t median( size / 2);
        const float low_value( itr->second.FirstElement());
        const float median_value( itr->second( median));
        const float high_value( itr->second.LastElement());
        if( itr->second.FirstElement() == itr->second.LastElement())
        {
          independent_to_median[ itr->first] = util::GetUndefined< float>();
        }
        else if( median_value == low_value || high_value - median_value > median_value - low_value)
        {
          size_t new_median( median + 1);
          while( median_value == itr->second( new_median))
          {
            ++new_median;
          }
          independent_to_median[ itr->first] = 0.5 * ( median_value + itr->second( new_median));
        }
        else
        {
          size_t new_median( median - 1);
          while( median_value == itr->second( new_median))
          {
            --new_median;
          }
          independent_to_median[ itr->first] = 0.5 * ( median_value + itr->second( new_median));
        }
      }

      // create a vector with size-t's of 0 for poorly-performing models in the CV, 1 for better-performing models
      // undefined for models for which this information is not available
      storage::Vector< std::string> model_score( n_models, std::string( results_size, '\0'));
      storage::List< CrossValidationInfo>::const_iterator itr_cv( cv_info.Begin());
      bool larger_is_better( obj_improvement_type == opti::e_LargerEqualIsBetter);
      for( size_t model_number( 0); model_number < n_models; ++model_number, ++itr_cv)
      {
        // get the independent result median
        const float ind_result_median( independent_to_median[ itr_cv->GetIndependentDatasetRetriever()]);
        if( !util::IsDefined( ind_result_median))
        {
          // undefined median, continue
          continue;
        }
        if( larger_is_better == ( itr_cv->GetResult() > ind_result_median))
        {
          prediction_classifications( model_number) = 'P';
          std::fill( model_score( model_number).begin(), model_score( model_number).end(), 'P');
        }
        else
        {
          prediction_classifications( model_number) = 'n';
          std::fill( model_score( model_number).begin(), model_score( model_number).end(), 'n');
        }
      }
      m_Scorer.InitializeBalancing( prediction_classifications, feature_size);
      return m_Scorer.Score( weight_effects, model_score);
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ScoreDatasetNeuralNetworkWeights::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      // get the models if they are not already in memory
      storage::Vector< util::OwnPtr< NeuralNetwork> > &models( s_Models[ GetString()]);

      if( m_Key.empty())
      {
        // get all the keys for this storage
        storage::Vector< std::string> keys( m_ModelStorage->GetAllKeys());

        BCL_MessageVrb
        (
          "Found " + util::Format()( keys.GetSize()) + " models for neural network weights"
        );

        if( models.IsEmpty() && !keys.IsEmpty())
        {
          // no models already loaded
          RetrieveInterface::t_Container interfaces( m_ModelStorage->RetrieveEnsemble( keys));
          models = storage::Vector< util::OwnPtr< NeuralNetwork> >( interfaces.Begin(), interfaces.End());
        }
        if( keys.GetSize() != models.GetSize())
        {
          ERR_STREAM << "# of neural networks in " << m_ModelStorage->GetString() << " changed; aborting";
          return false;
        }
        else if( keys.IsEmpty())
        {
          ERR_STREAM << "No neural networks found in storage! " << m_ModelStorage->GetString();
          return false;
        }
      }
      else if( models.IsEmpty())
      {
        // no models already loaded
        models.PushBack( m_ModelStorage->Retrieve( m_Key));
        if( !models.LastElement().IsDefined())
        {
          ERR_STREAM << m_Key << " is not a key for " << m_ModelStorage->GetString();
          return false;
        }
      }

      m_Models = util::ToSiPtr( models);
      return true;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetNeuralNetworkWeights::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "Uses neural network weights as a derivative proxy for scoring");

      parameters.AddInitializer
      (
        "storage",
        "type of storage for models",
        io::Serialization::GetAgent( &m_ModelStorage)
      );

      parameters.AddInitializer
      (
        "key",
        "optional key of the desired model in the storage",
        io::Serialization::GetAgent( &m_Key),
        ""
      );

      parameters.AddInitializer
      (
        "weights",
        "Change the weighting of various measures of the neural networks' weights",
        io::Serialization::GetAgent( &m_Scorer)
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_non_redundant.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_running_average_sd.h"
#include "util/bcl_util_enumerated.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetNonRedundant::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetNonRedundant())
    );

    //! @brief default constructor
    ScoreDatasetNonRedundant::ScoreDatasetNonRedundant() :
      m_ZScoreTolerance( 0.1),
      m_MaxOutliers( 3),
      m_AllowOneConstant( false),
      m_MinSpan( std::numeric_limits< float>::epsilon()),
      m_MinStd( std::numeric_limits< float>::epsilon()),
      m_MinAbsCrossCorrelation( 0.95)
    {
    }

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetNonRedundant
    ScoreDatasetNonRedundant *ScoreDatasetNonRedundant::Clone() const
    {
      return new ScoreDatasetNonRedundant( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetNonRedundant::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetNonRedundant>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetNonRedundant::GetAlias() const
    {
      static const std::string s_Name( "NonRedundant");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score a given dataset
    //! @param DATASET dataset of interest
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetNonRedundant::Score( const descriptor::Dataset &DATASET) const
    {
      // determine # of features, results, and dataset size
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t dataset_size( DATASET.GetSize());
      const linal::MatrixConstReference< float> features( DATASET.GetFeaturesReference());

      linal::Vector< float> is_non_redundant( feature_size, float( 1.0));
      // handle empty dataset
      if( dataset_size == size_t( 0) || feature_size <= size_t( 1))
      {
        return is_non_redundant;
      }

      // create rescale object, z-score and for min-max
      // This is an easy way of getting the min/max/ave/std for each column
      RescaleFeatureDataSet rescaler_ave_std
      (
        features,
        math::Range< float>( 0.0, 1.0),
        RescaleFeatureDataSet::e_AveStd
      );
      RescaleFeatureDataSet rescaler_min_max
      (
        features,
        math::Range< float>( 0.0, 1.0),
        RescaleFeatureDataSet::e_MinMax
      );

      // Get the feature labels for display to the user
      const FeatureLabelSet feature_labels( DATASET.GetFeaturesPtr()->GetFeatureLabelSet()->SplitFeatureLabelSet( true));

      // first, handle constant descriptors. Only one constant descriptor is allowed, and then only if its constant
      // value is non-zero
      bool have_constant_non_zero_descriptor( false);
      size_t constant_feature_index( util::GetUndefined< size_t>());
      for( size_t feature_number( 0); feature_number < feature_size; ++feature_number)
      {
        const math::Range< float> &range( rescaler_min_max.GetRescaleRanges()( feature_number));
        const float std_span( rescaler_ave_std.GetRescaleRanges()( feature_number).GetWidth() * 0.25);

        if( range.GetWidth() < m_MinSpan || std_span < m_MinStd)
        {
          // constant feature
          is_non_redundant( feature_number) = 0.0;
          if
          (
            m_AllowOneConstant && !have_constant_non_zero_descriptor
            && math::Absolute( range.GetMiddle()) > 0.01
            && range.GetMin() == range.GetMax()
          )
          {
            // first non-zero, constant feature; it'll still have a 0 score to simplify the checks
            // in the main redundancy check loop. Afterwards, it'll be set back to 1.0
            have_constant_non_zero_descriptor = true;
            constant_feature_index = feature_number;
          }
          else
          {
            BCL_MessageStd
            (
              feature_labels.GetMemberLabels()( feature_number).ToString()
              + " is a constant " + util::Format()( rescaler_ave_std.GetRescaleRanges()( feature_number).GetMiddle())
            );
          }
        }
      }

      // for each feature
      const math::Range< float> rescale_to( -1.0, 1.0);
      for( size_t feature_number( 1); feature_number < feature_size; ++feature_number)
      {
        if( is_non_redundant( feature_number) == size_t( 0))
        {
          // skip constant descriptors
          continue;
        }

        // get the relevant range parameters for this descriptor
        const math::Range< float> ave_std_range_a( rescaler_ave_std.GetRescaleRanges()( feature_number));

        float range_a( ave_std_range_a.GetWidth());

        // construct a linear function to rescale this descriptor
        const math::LinearFunction a_to_zscore( ave_std_range_a, rescale_to);

        // for each other descriptor before this one that was non-redundant
        for( size_t feature_number_b( 0); feature_number_b < feature_number; ++feature_number_b)
        {
          if( is_non_redundant( feature_number_b) == size_t( 0))
          {
            // skip constant descriptors
            continue;
          }

          // get the relevant range parameters for this descriptor
          const math::Range< float> ave_std_range_b( rescaler_ave_std.GetRescaleRanges()( feature_number_b));

          // test Std(A)/Range(A) = Std(B)/Range(B)
          float range_b( ave_std_range_b.GetWidth());

          // construct a linear function to rescale this descriptor
          const math::LinearFunction b_to_zscore( ave_std_range_b, rescale_to);

          size_t n_outliers_pos( 0), n_outliers_neg( 0);
          math::RunningAverage< float> average_product;
          // run through dataset, find whether the columns disagree by more than zscore_tolerance at any point
          for( size_t row_index( 0); row_index < dataset_size; ++row_index)
          {
            const float *row( features[ row_index]);
            const float a_zscore( a_to_zscore( row[ feature_number]));
            const float b_zscore( b_to_zscore( row[ feature_number_b]));
            if( n_outliers_pos <= m_MaxOutliers && math::Absolute( a_zscore - b_zscore) > m_ZScoreTolerance)
            {
              ++n_outliers_pos;
              if( n_outliers_pos > m_MaxOutliers && n_outliers_neg > m_MaxOutliers)
              {
                break;
              }
            }
            if( n_outliers_neg <= m_MaxOutliers && math::Absolute( a_zscore + b_zscore) > m_ZScoreTolerance)
            {
              ++n_outliers_neg;
              if( n_outliers_pos > m_MaxOutliers && n_outliers_neg > m_MaxOutliers)
              {
                break;
              }
            }
            average_product += a_zscore * b_zscore;
          }
          const float rsq( std::min( math::Absolute( 4.0 * average_product.GetAverage()), 1.0));
          if( ( n_outliers_pos <= m_MaxOutliers || n_outliers_neg <= m_MaxOutliers) && rsq >= m_MinAbsCrossCorrelation)
          {
            size_t redundant( feature_number), kept( feature_number_b);
            if
            (
              feature_labels.GetMemberLabels()( kept).GetValue() != "Partial"
              && feature_labels.GetMemberLabels()( redundant).GetValue() == "Partial"
            )
            {
              // prefer to keep the partial over a scalar
              // this results in fewer descriptors being calculated
              std::swap( redundant, kept);
            }
            if( ave_std_range_a.GetMin() >= 0.0 && ave_std_range_b.GetMin() >= 0.0)
            {
              range_b = ave_std_range_b.GetMiddle();
              range_a = ave_std_range_a.GetMiddle();
            }
            BCL_MessageStd
            (
              feature_labels.GetMemberLabels()( redundant).ToString()
              + " is redundant with " + feature_labels.GetMemberLabels()( kept).ToString()
              + std::string( n_outliers_pos < n_outliers_neg ? " Multiplier +" : " Multiplier -")
              + util::Format()( redundant != feature_number ? range_b / range_a : range_a / range_b)
              + " Cross correlation: " + util::Format()( rsq)
            );
            is_non_redundant( redundant) = 0.0;
            break;
          }
        }
      }

      if( have_constant_non_zero_descriptor)
      {
        is_non_redundant( constant_feature_index) = 1.0;
      }

      // return f-scores for every feature column in dataset
      return is_non_redundant;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetNonRedundant::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "1 for each descriptor that is not a linear function of a previously seen descriptor, 0 otherwise. "
        "Also returns 0 for descriptors that are always 0"
      );
      parameters.AddInitializer
      (
        "max outliers",
        "Maximum number of data points that can violate the z-score tolerance for a particular descriptor before declaring "
        "a pair of descriptors non-redundant",
        io::Serialization::GetAgent( &m_MaxOutliers),
        "3"
      );
      parameters.AddInitializer
      (
        "tol",
        "Maximum difference in z-score between two redundant descriptors",
        io::Serialization::GetAgent( &m_ZScoreTolerance),
        "0.1"
      );
      parameters.AddInitializer
      (
        "allow a constant",
        "allow up to one constant, non-zero, descriptor in a dataset. "
        "This is useful for keeping an offset for linear regression",
        io::Serialization::GetAgent( &m_AllowOneConstant),
        "False"
      );
      parameters.AddInitializer
      (
        "min span",
        "Minimum range of values that a descriptor needs to span to be considered non-constant",
        io::Serialization::GetAgent( &m_MinSpan),
        util::Format()( std::numeric_limits< float>::epsilon())
      );
      parameters.AddInitializer
      (
        "min std",
        "Minimum standard deviation of values that a descriptor needs to span to be considered non-constant",
        io::Serialization::GetAgent( &m_MinStd),
        util::Format()( std::numeric_limits< float>::epsilon())
      );
      parameters.AddInitializer
      (
        "min rsq",
        "Minimum r-squared for declaration of feature as redundant",
        io::Serialization::GetAgent( &m_MinAbsCrossCorrelation),
        "0.95"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_partition.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "model/bcl_model_approximator_decision_tree.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetPartition::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetPartition())
    );

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetPartition
    ScoreDatasetPartition *ScoreDatasetPartition::Clone() const
    {
      return new ScoreDatasetPartition( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetPartition::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetPartition>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetPartition::GetAlias() const
    {
      static const std::string s_Name( "Partition");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief score a given dataset
    //! @param DATASET dataset of interest
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetPartition::Score( const descriptor::Dataset &DATASET) const
    {
      // create an iterate to setup the feature result and state vector
      ApproximatorDecisionTree iterate
      (
        m_Partitioner,
        util::ShPtr< ObjectiveFunctionWrapper>(),
        m_Cutoff,
        util::ShPtr< descriptor::Dataset>()
      );

      // create the data set references
      util::ShPtr< storage::Vector< FeatureResultAndState> > processed_dataset
      (
        iterate.CreateDataSetReferences( DATASET.GetFeaturesReference(), DATASET.GetResultsReference())
      );

      return m_Partitioner->GetAllPartitionRatings( *processed_dataset, true);
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetPartition::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "uses the split rating from a decision tree dataset partitioner"
      );

      parameters.AddInitializer
      (
        "cutoff",
        "result value that separates actives from inactives",
        io::Serialization::GetAgent( &m_Cutoff),
        "0"
      );

      parameters.AddInitializer
      (
        "partitioner",
        "method of deciding which component of the feature vector to use to add a new node to the decision tree",
        io::Serialization::GetAgent( &m_Partitioner),
        "InformationGain"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_dataset_pearson_correlation.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"
#include "util/bcl_util_enumerated.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // instantiate s_Instance
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetPearsonCorrelation::s_Instance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetPearsonCorrelation( false))
    );
    const util::SiPtr< const util::ObjectInterface> ScoreDatasetPearsonCorrelation::s_AutoInstance
    (
      util::Enumerated< ScoreDatasetInterface>::AddInstance( new ScoreDatasetPearsonCorrelation( true))
    );

    //! @brief Clone function
    //! @return pointer to new ScoreDatasetPearsonCorrelation
    ScoreDatasetPearsonCorrelation *ScoreDatasetPearsonCorrelation::Clone() const
    {
      return new ScoreDatasetPearsonCorrelation( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDatasetPearsonCorrelation::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDatasetPearsonCorrelation>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDatasetPearsonCorrelation::GetAlias() const
    {
      static const std::string s_Name( "PearsonCorrelation"), s_AutoName( "PearsonCorrelationNonRedundancy");
      return m_AutoCorrelation ? s_AutoName : s_Name;
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score a given dataset
    //! @param DATASET dataset of interest
    //! @return scores of the dataset
    linal::Vector< float> ScoreDatasetPearsonCorrelation::Score( const descriptor::Dataset &DATASET) const
    {
      // determine # of features, results, and dataset size
      const size_t feature_size( DATASET.GetFeatureSize());
      const size_t result_size( m_AutoCorrelation ? feature_size : DATASET.GetResultSize());
      const size_t dataset_size( DATASET.GetSize());
      const linal::MatrixConstReference< float> features( DATASET.GetFeaturesReference());
      const linal::MatrixConstReference< float> results( m_AutoCorrelation ? features : DATASET.GetResultsReference());

      // handle empty dataset
      if( dataset_size == size_t( 0))
      {
        return linal::Vector< float>( feature_size, float( 0.0));
      }

      // compute the averages for each feature and result column
      math::RunningAverageSD< linal::Vector< float> > mean_feature, mean_result;
      // compute averages
      for( size_t counter( 0); counter < dataset_size; ++counter)
      {
        // average all features and results
        mean_feature += features.GetRow( counter);
      }
      // compute average results
      if( !m_AutoCorrelation)
      {
        for( size_t counter( 0); counter < dataset_size; ++counter)
        {
          mean_result += results.GetRow( counter);
        }
      }
      else
      {
        mean_result = mean_feature;
      }
      // for multiple result columns, return a score of the mean correlation for all feature_result pairings
      storage::Vector< storage::Vector< math::RunningAverage< float> > >
        mean_correlation( feature_size, storage::Vector< math::RunningAverage< float> >( result_size));

      linal::Vector< float>
        feature_temp( mean_feature.GetAverage()),
        result_temp( mean_result.GetAverage());

      // compute correlations
      for( size_t counter( 0); counter < dataset_size; ++counter)
      {
        // compute the difference from the mean
        feature_temp = features.GetRow( counter);
        result_temp = results.GetRow( counter);
        feature_temp -= mean_feature.GetAverage();
        result_temp -= mean_result.GetAverage();
        // BCL_Debug( counter);
        for( size_t feature_index( 0); feature_index < feature_size; ++feature_index)
        {
          for( size_t result_index( m_AutoCorrelation ? feature_index + 1 : 0); result_index < result_size; ++result_index)
          {
            mean_correlation( feature_index)( result_index) += feature_temp( feature_index) * result_temp( result_index);
          }
        }
      }

      // find the max absolute correlation for each feature
      linal::Vector< float> max_abs_correlations( feature_size, float( m_AutoCorrelation ? 0.0 : -1.0));
      for( size_t feature_index( 0); feature_index < feature_size; ++feature_index)
      {
        float max_abs_correlation( max_abs_correlations( feature_index));
        float std_feature( mean_feature.GetStandardDeviation()( feature_index));
        if( std_feature <= float( 0.0) || !util::IsDefined( std_feature))
        {
          if( m_AutoCorrelation)
          {
            max_abs_correlations( feature_index) = 1.0;
          }
          continue;
        }
        for( size_t result_index( m_AutoCorrelation ? feature_index + 1 : 0); result_index < result_size; ++result_index)
        {
          float std_result( mean_result.GetStandardDeviation()( result_index));
          if( std_result <= float( 0.0) || !util::IsDefined( std_result))
          {
            // skip results that did not vary
            continue;
          }
          float abs_correlation
          (
            math::Absolute( mean_correlation( feature_index)( result_index)) / ( std_feature * std_result)
          );

          // due to numeric truncation and roundoff, abs correlation may come out slightly above 1
          if( abs_correlation > 1.0)
          {
            abs_correlation = 1.0;
          }
          if( m_AutoCorrelation && abs_correlation > max_abs_correlations( result_index))
          {
            max_abs_correlations( result_index) = abs_correlation;
          }
          max_abs_correlation = std::max( abs_correlation, max_abs_correlation);
        }
        max_abs_correlations( feature_index) = max_abs_correlation;
      }
      if( m_AutoCorrelation)
      {
        max_abs_correlations *= -1.0;
        max_abs_correlations += float( 1.0);
      }

      // return f-scores for every feature column in dataset
      return max_abs_correlations;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDatasetPearsonCorrelation::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "computes absolute pearson correlation between features and results."
        "If multiple results are present, takes the maximum absolute correlation."
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include forward header of this class
#include "model/bcl_model_score_derivative_ensemble.h"

// include other forward headers - sorted alphabetically

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "math/bcl_math_running_average_sd.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! @brief default constructor
    ScoreDerivativeEnsemble::ScoreDerivativeEnsemble() :
      m_ConsistencyWeight( 0.0),
      m_ConsistencyBestWeight( 0.0),
      m_WeightSquareWeight( 0.0),
      m_AverageWeightAbsWeight( 0.0),
      m_RawAverageWeight( 0.0),
      m_UtilityWeight( 0.0),
      m_Balance( true),
      m_UseCategorical( true)
    {
    }

    //! @brief Clone function
    //! @return pointer to new ScoreDerivativeEnsemble
    ScoreDerivativeEnsemble *ScoreDerivativeEnsemble::Clone() const
    {
      return new ScoreDerivativeEnsemble( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name of the object behind a pointer or the current object
    //! @return the class name
    const std::string &ScoreDerivativeEnsemble::GetClassIdentifier() const
    {
      return GetStaticClassName< ScoreDerivativeEnsemble>();
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &ScoreDerivativeEnsemble::GetAlias() const
    {
      static const std::string s_Name( "ScoreDerivativeEnsemble");
      return s_Name;
    }

    //! @brief test whether or not this score requires model scoring
    //! @return true if this scores uses model scoring
    bool ScoreDerivativeEnsemble::GetUsesModelScores() const
    {
      return m_ConsistencyBestWeight || m_UtilityWeight;
    }

    //! @brief initialize balancing (weighting of columns based on prior distribution)
    //! @param MATRIX results matrix
    //! @param NR_DESCRIPTORS number of descriptor columns
    void ScoreDerivativeEnsemble::InitializeBalancing
    (
      const storage::Vector< linal::Matrix< char> > &MODEL_CLASSIFICATIONS,
      const size_t &NR_DESCRIPTORS
    ) const
    {
      const size_t result_size( MODEL_CLASSIFICATIONS( 0).GetNumberCols());
      const size_t n_features( MODEL_CLASSIFICATIONS( 0).GetNumberRows());
      m_WeightsTruePositives = m_WeightsFalsePositives = m_WeightsTrueNegatives = m_WeightsFalseNegatives =
        linal::Vector< float>( result_size, float( 1.0) / float( result_size));
      m_AveWeightSomeTP = m_AveWeightSomeTN = m_AveWeightSomeFP = m_AveWeightSomeFN
        = storage::Vector< storage::Vector< math::RunningAverage< float> > >
          (
            result_size,
            storage::Vector< math::RunningAverage< float> >( NR_DESCRIPTORS)
          );
      m_ConsistencySomeTP = m_ConsistencySomeTN = m_ConsistencySomeFP = m_ConsistencySomeFN = m_ConsistencyP = m_ConsistencyN
        = storage::Vector< storage::Vector< math::RunningAverage< float> > >
          (
            result_size,
            storage::Vector< math::RunningAverage< float> >( NR_DESCRIPTORS)
          );
      const size_t n_models( MODEL_CLASSIFICATIONS.GetSize());
      storage::Vector< linal::Vector< size_t> > count_models_correct_p
      (
        result_size,
        linal::Vector< size_t>( size_t( n_models + 1), size_t( 0))
      );
      storage::Vector< linal::Vector< size_t> > count_models_correct_n
      (
        result_size,
        linal::Vector< size_t>( size_t( n_models + 1), size_t( 0))
      );

      // iterate over each model's predictions
      // translate the predictions from m_Predictions into the predictions for this model
      linal::Vector< size_t> positive_counts( result_size, size_t( 0));
      linal::Vector< size_t> true_positive_counts( result_size, size_t( 0));
      linal::Vector< size_t> false_positive_counts( result_size, size_t( 0));
      linal::Vector< size_t> true_negative_counts( result_size, size_t( 0));
      linal::Vector< size_t> false_negative_counts( result_size, size_t( 0));

      for( size_t datum( 0); datum < n_features; ++datum)
      {
        for( size_t result( 0); result < result_size; ++result)
        {
          size_t count_correct( 0);
          const bool is_positive
          (
            MODEL_CLASSIFICATIONS( 0)( datum, result) == 'P'
            || MODEL_CLASSIFICATIONS( 0)( datum, result) == 'n'
          );
          const char target_char( is_positive ? 'P' : 'N');
          for( size_t model( 0); model < n_models; ++model)
          {
            if( MODEL_CLASSIFICATIONS( model)( datum, result) == target_char)
            {
              ++count_correct;
            }
          }
          if( is_positive)
          {
            ++count_models_correct_p( result)( count_correct);
            positive_counts( result) += n_models;
            true_positive_counts( result) += count_correct;
            false_negative_counts( result) += n_models - count_correct;
          }
          else
          {
            ++count_models_correct_n( result)( count_correct);
            true_negative_counts( result) += count_correct;
            false_positive_counts( result) += n_models - count_correct;
          }
        }
      }

      BCL_MessageStd( "# models correct histogram for positives: " + util::Format()( count_models_correct_p));
      BCL_MessageStd( "# models correct histogram for negatives: " + util::Format()( count_models_correct_n));

      m_PChanceConsistency = linal::Vector< float>( n_models + 1);
      for( size_t model_tally( 0); model_tally <= n_models; ++model_tally)
      {
        math::RunningAverage< float> ave;
        double divisor( math::Pow( 2.0, model_tally - 1.0));
        for( size_t i( 0), end( model_tally / 2); i <= end; ++i)
        {
          double prob( 0);
          // determine the probability of randomly choosing i particular models or less
          for( size_t j( 0); j <= i; ++j)
          {
            prob += math::BinomialCoefficient( model_tally, j);
          }
          prob /= divisor;
          ave += std::max( 0.0, 1.0 - prob);
        }
        m_PChanceConsistency( model_tally) = ave.GetAverage();
      }

      // if not balancing or weighting non-equally, just continue
      if( !m_Balance && !m_UseCategorical)
      {
        return;
      }

      // for balancing, balance the influence of P vs N, and separately T vs F, so
      // P' = N'
      // T' = F'
      // where P' = x1 * TP + x2 * FN, N' = x3 * TN + x4 * FP, T' = x1 * TP + x3 * TN, F' = x2 * FN + x4 * FP
      // to properly constraint the equations, also require that
      // P = TP + FN = P'
      // x1 * TP = x3 * TN
      // This yields the solutions:
      // x1 = P / ( 2 * TP)
      // x2 = P / ( 2 * FN)
      // x3 = P / ( 2 * TN)
      // x4 = P / ( 2 * FP)
      if( m_Balance)
      {
        for( size_t j( 0); j < result_size; ++j)
        {
          // get half the positives
          const float half_positives( 0.5 * positive_counts( j));
          if( true_positive_counts( j))
          {
            m_WeightsTruePositives( j) = half_positives / float( true_positive_counts( j));
          }
          if( false_negative_counts( j))
          {
            m_WeightsFalseNegatives( j) = half_positives / float( false_negative_counts( j));
          }
          if( true_negative_counts( j))
          {
            m_WeightsTrueNegatives( j) = half_positives / float( true_negative_counts( j));
          }
          if( false_positive_counts( j))
          {
            m_WeightsFalsePositives( j) = half_positives / float( false_positive_counts( j));
          }
        }
      }
      if( m_UseCategorical && result_size > size_t( 1))
      {
        size_t sum_counts( positive_counts.Sum());
        size_t excess( sum_counts % n_features);

        // if there is a nearly integral # of counts per feature and there are least 1 point per feature
        if( sum_counts >= n_features && !excess)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            // get half the positives
            const float positive_frequency( float( positive_counts( j)) / float( n_features));
            m_WeightsTruePositives( j) *= positive_frequency;
            m_WeightsFalseNegatives( j) *= positive_frequency;
            m_WeightsTrueNegatives( j) *= positive_frequency;
            m_WeightsFalsePositives( j) *= positive_frequency;
          }
        }
        else
        {
          BCL_MessageCrt( "Requested categorical balancing but it appears invalid for this dataset!");
        }
      }
    }

  ////////////////
  // operations //
  ////////////////

  ///////////////
  // operators //
  ///////////////

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief score a given vector of matrices
    //! @param MODEL_DESCRIPTOR_DERIVATIVES(x)(y,z) corresponds to the derivative of feature y on output z for model x
    //! @param PREDICTION_CLASS binary (0,1) score for each model; used for calculating f-score measure only
    //! @return an agglomerated score
    linal::Vector< float> ScoreDerivativeEnsemble::Score
    (
      const storage::Vector< linal::Matrix< float> > &MODEL_DESCRIPTOR_DERIVATIVES,
      const storage::Vector< std::string> &PREDICTION_CLASS
    ) const
    {
      math::RunningAverage< linal::Vector< float> > averages;
      this->Score( MODEL_DESCRIPTOR_DERIVATIVES, PREDICTION_CLASS, averages);
      return averages.GetAverage();
    }

    //! @brief score a given vector of matrices and add it to a running average with the appropriate weight
    //! @param MODEL_DESCRIPTOR_DERIVATIVES(x)(y,z) corresponds to the derivative of feature y on output z for model x
    //! @param PREDICTION_CLASS PpNn\0 for each model, indicating whether the result was a TP,FP,TN,FN, or NA for each model
    //! @param AVERAGES running average of vector with one value for each descriptor column
    //! This overloaded version allows for balancing
    void ScoreDerivativeEnsemble::Score
    (
      const storage::Vector< linal::Matrix< float> > &MODEL_DESCRIPTOR_DERIVATIVES,
      const storage::Vector< std::string> &PREDICTION_CLASS,
      math::RunningAverage< linal::Vector< float> > &AVERAGES
    ) const
    {
      const size_t n_models( MODEL_DESCRIPTOR_DERIVATIVES.GetSize());

      // handle trivial case where no models or no derivatives were given
      if( n_models == size_t( 0) || MODEL_DESCRIPTOR_DERIVATIVES.GetSize() == size_t( 0))
      {
        return;
      }

      const size_t feature_size( MODEL_DESCRIPTOR_DERIVATIVES( 0).GetNumberRows());
      const size_t result_size( MODEL_DESCRIPTOR_DERIVATIVES( 0).GetNumberCols());
      BCL_Assert
      (
        m_ResultColumnWeighting.IsEmpty() || m_ResultColumnWeighting.GetSize() == result_size,
        "Need same number of result column weights as outputs or none at all!"
      );

      // create multiplier for each row, based on WAS_ABOVE_CUTOFF
      linal::Vector< float> multiplier( result_size, float( 1.0));
      for( size_t i( 0); i < result_size; ++i)
      {
        size_t n_correct( 0);
        const char type( PREDICTION_CLASS( 0)[ i]);
        const bool is_positive( type == 'P' || type == 'n' || type == '\0');
        for( size_t model( 0); model < n_models; ++model)
        {
          if( !islower( PREDICTION_CLASS( model)[ i]))
          {
             ++n_correct;
          }
        }
        if( n_correct == 0)
        {
          // no models got this result correct; set weight to 0
          multiplier( i) = 0;
        }
        else if( is_positive)
        {
          multiplier( i) = m_WeightsTruePositives( i);
        }
        else
        {
          multiplier( i) = m_WeightsTrueNegatives( i);
        }
        if( !m_ResultColumnWeighting.IsEmpty())
        {
          multiplier( i) *= m_ResultColumnWeighting( i);
        }
      }

      float multiplier_sum( multiplier.Sum());
      if( !multiplier_sum)
      {
        // no weight for this class; likely all results were wrong
        return;
      }
      multiplier /= multiplier_sum;

      math::RunningAverage< linal::Matrix< float> > weights_stats;

      linal::Matrix< size_t> counts_above_zero( feature_size, result_size, size_t( 0));
      linal::Matrix< float> abs_derivative;
      math::RunningAverage< linal::Matrix< float> > abs_weights_stats;

      // for each model
      // 1. Obtain its matrix of weights
      // 2. Compute the average absolute value for each feature weight
      for( size_t model_number( 0); model_number < n_models; ++model_number)
      {
        // make a reference to the derivative vector
        const linal::Matrix< float> &derivative_matrix( MODEL_DESCRIPTOR_DERIVATIVES( model_number));
        weights_stats += derivative_matrix;
        abs_derivative = derivative_matrix;

        // update the count of values above zero
        for( size_t feature( 0); feature < feature_size; ++feature)
        {
          for( size_t result( 0); result < result_size; ++result)
          {
            if( derivative_matrix( feature, result) > 0.0)
            {
              ++counts_above_zero( feature, result);
            }
            else
            {
              abs_derivative( feature, result) = -abs_derivative( feature, result);
            }
          }
        }
        abs_weights_stats += abs_derivative;
      }

      linal::Matrix< float> abs_weights_ave( weights_stats.GetAverage());
      math::Absolute( abs_weights_ave);

      linal::Matrix< float> sign_purity( feature_size, result_size);

      const float n_modelsf( n_models);

      // Sum the matrices
      linal::Matrix< float> combined_measures( feature_size, result_size, float( 0.0));

      if( m_RawAverageWeight)
      {
        combined_measures += weights_stats.GetAverage();
        combined_measures *= m_RawAverageWeight;
      }

      if( m_ConsistencyWeight)
      {
        for( size_t i( 0); i < feature_size; ++i)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            // calculate the consistency of signs for this weight, scaled between 0 and 1
            // at 0, half the models have the opposite effect as the other half
            sign_purity( i, j) = std::max( counts_above_zero( i, j), n_models - counts_above_zero( i, j));
          }
        }
        // normalize sign purity, scaled between 0 and 1
        // at 0, half the models have the opposite effect as the other half
        sign_purity -= float( 0.75) * n_modelsf;
        sign_purity /= n_modelsf;
        sign_purity *= m_ConsistencyWeight * float( 4.0);
        combined_measures += sign_purity;
      }

      // rescale the weights matrix between between 0-1
      if( m_WeightSquareWeight)
      {
        math::RunningAverage< linal::Matrix< float> > weights_sq_stats;

        linal::Matrix< float> square_weight( feature_size, result_size);

        // for each model
        // 1. Obtain its matrix of weights
        // 2. Compute the average absolute value for each feature weight
        for( size_t model_number( 0); model_number < n_models; ++model_number)
        {
          // make a reference to the derivative vector
          const linal::Matrix< float> &derivative_matrix( MODEL_DESCRIPTOR_DERIVATIVES( model_number));

          // make a copy of the matrix, so that we can square it
          square_weight = derivative_matrix;

          // update the count of values above zero
          for( size_t feature( 0); feature < feature_size; ++feature)
          {
            for( size_t result( 0); result < result_size; ++result)
            {
              square_weight( feature, result) *= square_weight( feature, result);
            }
          }
          weights_sq_stats += square_weight;
        }

        square_weight = weights_sq_stats.GetAverage();
        for( size_t i( 0); i < feature_size; ++i)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            // calculate the consistency of signs for this weight, scaled between 0 and 1
            // at 0, half the models have the opposite effect as the other half
            square_weight( i, j) = math::Sqrt( square_weight( i, j));
          }
        }
        RescaleFeatureDataSet rescale_weight( square_weight, math::Range< float>( -1.0, 1.0), RescaleFeatureDataSet::e_AveStd);
        rescale_weight.RescaleMatrix( square_weight);
        square_weight *= m_WeightSquareWeight;
        combined_measures += square_weight;
      }

      // rescale the ave weights matrix between 0-1
      if( m_AverageWeightAbsWeight)
      {
        linal::Matrix< float> tmp_weight_ave( abs_weights_ave);
        RescaleFeatureDataSet rescale_weight( tmp_weight_ave, math::Range< float>( -1.0, 1.0), RescaleFeatureDataSet::e_AveStd);
        rescale_weight.RescaleMatrix( tmp_weight_ave); // comment out if you want raw rescaled changes
        tmp_weight_ave *= m_AverageWeightAbsWeight;
        combined_measures += tmp_weight_ave;
      }

      // model-performance based scores
      if( GetUsesModelScores())
      {
        counts_above_zero.SetZero();
        // take stats of the better and worse matrices
        storage::Vector< std::string>::const_iterator itr_good( PREDICTION_CLASS.Begin());
        linal::Matrix< size_t> counts_above_zero_worse( feature_size, result_size);
        linal::Vector< size_t> count_good_models( result_size, size_t( 0));
        linal::Vector< size_t> count_bad_models( result_size, n_models);
        for( size_t model_number( 0); model_number < n_models; ++model_number, ++itr_good)
        {
          std::string::const_iterator itr_result_good( itr_good->begin());
          const linal::Matrix< float> &derivative_matrix( MODEL_DESCRIPTOR_DERIVATIVES( model_number));
          linal::Matrix< float> derivatives_transposed( derivative_matrix.Transposed());
          for( size_t result( 0); result < result_size; ++result, ++itr_result_good)
          {
            if( !islower( *itr_result_good))
            {
              ++count_good_models( result);
              --count_bad_models( result);
              for( size_t feature( 0); feature < feature_size; ++feature)
              {
                if( derivative_matrix( feature, result) > 0.0)
                {
                  ++counts_above_zero( feature, result);
                }
              }
            }
            else if( m_UtilityWeight)
            {
              for( size_t feature( 0); feature < feature_size; ++feature)
              {
                if( derivative_matrix( feature, result) > 0.0)
                {
                  ++counts_above_zero_worse( feature, result);
                }
              }
            }
          }
        }

        sign_purity.SetZero();
        for( size_t i( 0); i < feature_size; ++i)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            const size_t n_better_models( count_good_models( j));
            if( n_better_models >= 2)
            {
              const float offset( 0.5 * n_better_models);
              // calculate the consistency of signs for this weight, scaled between 0 and 1
              // at 0, half the models have the opposite effect as the other half
              sign_purity( i, j) = ( std::max( counts_above_zero( i, j), n_better_models - counts_above_zero( i, j)) - offset) / offset;
            }
          }
        }
        linal::Matrix< float> worse_sign_purity( feature_size, result_size, float( 0.0));
        for( size_t i( 0); i < feature_size; ++i)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            const size_t n_worse_models( count_bad_models( j));
            if( n_worse_models >= 2)
            {
              const float offset( 0.5 * n_worse_models);
              // calculate the consistency of signs for this weight, scaled between 0 and 1
              // at 0, half the models have the opposite effect as the other half
              worse_sign_purity( i, j) = ( std::max( counts_above_zero_worse( i, j), n_worse_models - counts_above_zero_worse( i, j)) - offset) / offset;
            }
          }
        }

        // take stats of the better and worse matrices
        storage::Vector< math::RunningAverage< linal::Vector< float> > > better_weights_abs_stats( result_size);
        storage::Vector< math::RunningAverage< linal::Vector< float> > > worse_weights_abs_stats( result_size);
        itr_good = PREDICTION_CLASS.Begin();
        for( size_t model_number( 0); model_number < n_models; ++model_number, ++itr_good)
        {
          std::string::const_iterator itr_result_good( itr_good->begin());
          const linal::Matrix< float> &derivative_matrix( MODEL_DESCRIPTOR_DERIVATIVES( model_number));
          linal::Matrix< float> derivatives_transposed( derivative_matrix.Transposed());
          math::Absolute( derivatives_transposed);
          for( size_t result( 0); result < result_size; ++result, ++itr_result_good)
          {
            // check for P or N, which indicate TP & TN, respectively
            if( !islower( *itr_result_good))
            {
              better_weights_abs_stats( result) += derivatives_transposed.GetRow( result);
            }
            else
            {
              worse_weights_abs_stats( result) += derivatives_transposed.GetRow( result);
            }
          }
        }

        if( m_ConsistencyBestWeight)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            const size_t count_good( count_good_models( j));
            if( count_good < 2)
            {
              continue;
            }
            const bool is_positive( PREDICTION_CLASS( 0)[ j] == 'P' || PREDICTION_CLASS( 0)[ j] == 'n');
            const float multiplier_this_result( multiplier( j) * m_PChanceConsistency( count_good));
            storage::Vector< math::RunningAverage< float> > &consistency_array( is_positive ? m_ConsistencyP( j) : m_ConsistencyN( j));
            const linal::Vector< float> &better_model_weight_aves( better_weights_abs_stats( j).GetAverage());
            m_Mutex.Lock();
            for( size_t feature( 0); feature < feature_size; ++feature)
            {
              consistency_array( feature).AddWeightedObservation
              (
                sign_purity( feature, j),
                multiplier_this_result * better_model_weight_aves( feature)
              );
            }
            m_Mutex.Unlock();
          }
        }
        if( m_UtilityWeight)
        {
          for( size_t j( 0); j < result_size; ++j)
          {
            const size_t count_good( count_good_models( j));
            const size_t count_bad( count_bad_models( j));
            if( count_good && count_bad)
            {
              const linal::Vector< float> &better_model_weight_aves( better_weights_abs_stats( j).GetAverage());
              const linal::Vector< float> &worse_model_weight_aves( worse_weights_abs_stats( j).GetAverage());
              const float good_feature_multiplier( m_PChanceConsistency( count_good));
              const float bad_feature_multiplier( m_PChanceConsistency( count_bad));

              const bool is_positive( PREDICTION_CLASS( 0)[ j] == 'P' || PREDICTION_CLASS( 0)[ j] == 'n');

              storage::Vector< math::RunningAverage< float> > &weight_array_true( is_positive ? m_AveWeightSomeTP( j) : m_AveWeightSomeTN( j));
              storage::Vector< math::RunningAverage< float> > &weight_array_false( is_positive ? m_AveWeightSomeFN( j) : m_AveWeightSomeFP( j));
              storage::Vector< math::RunningAverage< float> > &consistency_array_true( is_positive ? m_ConsistencySomeTP( j) : m_ConsistencySomeTN( j));
              storage::Vector< math::RunningAverage< float> > &consistency_array_false( is_positive ? m_ConsistencySomeFN( j) : m_ConsistencySomeFP( j));

              m_Mutex.Lock();
              for( size_t feature( 0); feature < feature_size; ++feature)
              {
                consistency_array_true( feature).AddWeightedObservation
                (
                  sign_purity( feature, j),
                  good_feature_multiplier * better_model_weight_aves( feature)
                );
                weight_array_true( feature).AddWeightedObservation( better_model_weight_aves( feature), count_good);
                consistency_array_false( feature).AddWeightedObservation
                (
                  worse_sign_purity( feature, j),
                  bad_feature_multiplier * worse_model_weight_aves( feature)
                );
                weight_array_false( feature).AddWeightedObservation( worse_model_weight_aves( feature), count_bad);
              }
              m_Mutex.Unlock();
            }
          }
        }
      }
      if( m_NonUtilityWeight)
      {
        linal::Vector< float> norm( feature_size, float( 0));
        for( size_t i( 0); i < feature_size; ++i)
        {
          norm( i) = linal::VectorConstReference< float>( result_size, combined_measures[ i]) * multiplier;
        }

        AVERAGES.AddWeightedObservation( norm, multiplier_sum * m_NonUtilityWeight);
      }
    }

    //! @brief add the utility score to the overall score
    void ScoreDerivativeEnsemble::AddUtilityScore
    (
      math::RunningAverage< linal::Vector< float> > &AVERAGES
    ) const
    {
      if( !m_UtilityWeight && !m_ConsistencyBestWeight)
      {
        return;
      }

      const size_t n_features( m_ConsistencySomeTP( 0).GetSize()), n_results( m_ConsistencySomeTP.GetSize());
      linal::Vector< float> utility( n_features, float( 0.0));
      linal::Vector< float> consistency_best( n_features, float( 0.0));

      const double original_weight( AVERAGES.GetWeight());

      linal::Vector< float> result_weights( n_results);
      for( size_t j( 0); j < n_results; ++j)
      {
        result_weights( j) = m_WeightsTruePositives( j) + m_WeightsFalsePositives( j) + m_WeightsTrueNegatives( j) + m_WeightsTruePositives( j);
      }
      result_weights *= float( n_results) / result_weights.Sum();
      if( m_UtilityWeight)
      {
        //math::RunningAverage< float> average;
        math::RunningMinMax< float> minmax;
        math::RunningAverage< float> ave_ctp, ave_cfp, ave_ctn, ave_cfn;
        for( size_t i( 0); i < n_features; ++i)
        {
          for( size_t j( 0); j < n_results; ++j)
          {
            ave_ctp += m_ConsistencySomeTP( j)( i);
            ave_cfp += m_ConsistencySomeFP( j)( i);
            ave_ctn += m_ConsistencySomeTN( j)( i);
            ave_cfn += m_ConsistencySomeFN( j)( i);
          }
        }
        if( ave_ctp.GetAverage() < 1.0e-8)
        {
          ave_ctp.Reset();
          ave_ctp += 1.0e-8;
        }
        if( ave_cfp.GetAverage() < 1.0e-8)
        {
          ave_cfp.Reset();
          ave_cfp += 1.0e-8;
        }
        if( ave_ctn.GetAverage() < 1.0e-8)
        {
          ave_ctn.Reset();
          ave_ctn += 1.0e-8;
        }
        if( ave_cfn.GetAverage() < 1.0e-8)
        {
          ave_cfn.Reset();
          ave_cfn += 1.0e-8;
        }
        const float alpha( ave_ctp.GetAverage() / ave_cfn.GetAverage());
        const float beta( ave_ctp.GetAverage() / ave_cfp.GetAverage());
        const float chi( ave_ctp.GetAverage() / ave_ctn.GetAverage());
        for( size_t i( 0); i < n_features; ++i)
        {
          for( size_t j( 0); j < n_results; ++j)
          {
            float consistency_add( 0);
            const float consistency_tp( m_ConsistencySomeTP( j)( i).GetAverage());
            const float consistency_fp( beta * m_ConsistencySomeFP( j)( i).GetAverage());
            const float consistency_tn( chi * m_ConsistencySomeTN( j)( i).GetAverage());
            const float consistency_fn( alpha * m_ConsistencySomeFN( j)( i).GetAverage());
            if( consistency_tp > consistency_fn)
            {
              consistency_add += ( consistency_tp - consistency_fn) * m_AveWeightSomeTP( j)( i).GetAverage();
            }
            else
            {
              consistency_add += ( consistency_tp - consistency_fn) * m_AveWeightSomeFN( j)( i).GetAverage();
            }
            if( consistency_tn > consistency_fp)
            {
              consistency_add += ( consistency_tn - consistency_fp) * m_AveWeightSomeTN( j)( i).GetAverage();
            }
            else
            {
              consistency_add += ( consistency_tn - consistency_fp) * m_AveWeightSomeFP( j)( i).GetAverage();
            }
            consistency_add *= result_weights( j);
            utility( i) += consistency_add;
          }
          minmax += utility( i);
        }
        utility *= float( n_features) / float( n_results);
        if( m_NonUtilityWeight)
        {
          AVERAGES.AddWeightedObservation( utility, original_weight * m_UtilityWeight / m_NonUtilityWeight);
        }
        else
        {
          AVERAGES.AddWeightedObservation( utility, m_UtilityWeight);
        }
      }
      if( m_ConsistencyBestWeight)
      {
        for( size_t i( 0); i < n_features; ++i)
        {
          for( size_t j( 0); j < n_results; ++j)
          {
            consistency_best( i) += result_weights( j) * ( m_ConsistencyN( j)( i).GetAverage() + m_ConsistencyP( j)( i).GetAverage()) / 2.0;
          }
        }
        if( m_NonUtilityWeight)
        {
          AVERAGES.AddWeightedObservation( consistency_best, original_weight * m_ConsistencyBestWeight / m_NonUtilityWeight);
        }
        else
        {
          AVERAGES.AddWeightedObservation( consistency_best, m_ConsistencyBestWeight);
        }
      }
      for( size_t i( 0); i < n_features; ++i)
      {
        for( size_t j( 0); j < n_results; ++j)
        {
          BCL_MessageVrb
          (
            "Feature: " + util::Format()( i) + " Result: " + util::Format()( j)
            + " utility: " + util::Format()( utility( i))
            + " consistency-best: " + util::Format()( consistency_best( i))
          );
        }
      }
    }

    //! @brief Set the members of this property from the given LABEL
    //! @param LABEL the label to parse
    //! @param ERR_STREAM stream to write errors out to
    bool ScoreDerivativeEnsemble::ReadInitializerSuccessHook
    (
      const util::ObjectDataLabel &LABEL,
      std::ostream &ERR_STREAM
    )
    {
      const double sum
      (
        m_ConsistencyWeight + m_ConsistencyBestWeight
        + m_WeightSquareWeight + m_AverageWeightAbsWeight
        + m_RawAverageWeight + m_UtilityWeight
      );
      m_ConsistencyWeight /= sum;
      m_ConsistencyBestWeight /= sum;
      m_WeightSquareWeight /= sum;
      m_AverageWeightAbsWeight /= sum;
      m_RawAverageWeight /= sum;
      m_UtilityWeight /= sum;
      m_NonUtilityWeight = 1.0 - m_UtilityWeight - m_ConsistencyBestWeight;
      return true;
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer ScoreDerivativeEnsemble::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Calculates a weighted sum of desired measures based on derivatives or pseudo-derivatives of a model"
      );

      parameters.AddInitializer
      (
        "consistency",
        "Change the weight of sign consistency on the final result. Higher values place more emphasis on columns that have the same effect on the "
        "output across the models",
        io::Serialization::GetAgentWithRange( &m_ConsistencyWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "consistency best",
        "Change the weight of sign consistency for the best models on the final result",
        io::Serialization::GetAgentWithRange( &m_ConsistencyBestWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "square",
        "Change the weight of the average (weight influence ^ 2) (and rescaled) on the score output",
        io::Serialization::GetAgentWithRange( &m_WeightSquareWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "absolute",
        "Change the weight of the average (weight) (and rescaled) on the score output",
        io::Serialization::GetAgentWithRange( &m_AverageWeightAbsWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "utility",
        "Change the weight of the utility function of input sensitivity.  The utility function considers whether models "
        "that gave incorrect output for a particular feature consistently used the feature more than the models that performed "
        "well on that particular feature",
        io::Serialization::GetAgentWithRange( &m_UtilityWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "average",
        " This score should never be used for feature selection, but is sometimes useful in analysis of the directional "
        "influence of descriptors.  Likewise, it should always be used by itself alone",
        io::Serialization::GetAgentWithRange( &m_RawAverageWeight, 0.0, 1.0),
        "0.0"
      );
      parameters.AddInitializer
      (
        "balance",
        "True to weight columns by 1-column frequency",
        io::Serialization::GetAgent( &m_Balance),
        "True"
      );
      parameters.AddInitializer
      (
        "categorical",
        "True to have a constant weight per feature, divided equally amongst columns above vs below the cutoff",
        io::Serialization::GetAgent( &m_UseCategorical),
        "False"
      );
      parameters.AddOptionalInitializer
      (
        "result weights",
        "vector of weights; 1 value per result column in the dataset, or empty to weight all result columns equally",
        io::Serialization::GetAgent( &m_ResultColumnWeighting)
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl

// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_support_vector_kernel_base.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_range.h"
#include "model/bcl_model_feature_data_set.h"
#include "sched/bcl_sched_scheduler_interface.h"
#include "sched/bcl_sched_tertiary_function_job_with_data.h"
#include "storage/bcl_storage_triplet.h"
#include "util/bcl_util_sh_ptr_list.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! @brief compute kernel values for every pair input_vector_i with each other vector in data set
    //! @param TRAINING_DATA featurevector data set without labels
    //! @param INPUT_VECTOR_I feature vector
    //! @return vector with all combinations towards INPUT_VECTOR_I
    linal::Vector< float> SupportVectorKernelBase::GetInputVectorIKernelMatrixMultiOutput
    (
      const FeatureDataSetInterface< float> &TRAINING_DATA,
      const size_t &INPUT_VECTOR_I
    ) const
    {
      // size of data set
      const size_t data_set_size( TRAINING_DATA.GetNumberFeatures());
      BCL_Assert( INPUT_VECTOR_I < data_set_size, "vector i is outside the data set");

      // create storage::Vector of size corresponding to entries in TRAINING_DATA initialized with 0.0
      linal::Vector< float> kernel_data( data_set_size, float( 0.0));
      // reference to index_vector_i in training data
      const FeatureReference< float> input_vector_i( TRAINING_DATA( INPUT_VECTOR_I));

      // define a group id for threads that will be created
      const size_t group_id( 0);

      // number of available cpus
      const size_t number_available_cpus( sched::GetNumberCPUs());

      const size_t interval( data_set_size / number_available_cpus);

      BCL_Assert( interval != 0, "Interval is zero! Less data points then processors!");

      // number of jobs
      size_t number_jobs( number_available_cpus);

      storage::Triplet
      <
        util::SiPtr< const FeatureDataSetInterface< float> >,
        const FeatureReference< float>,
        linal::Vector< float>
      > train_index_result
      (
        util::SiPtr< const FeatureDataSetInterface< float> >( TRAINING_DATA),
        input_vector_i,
        kernel_data
      );

      util::ShPtrList< sched::JobInterface> schedule;

      // ranges for jobs
      storage::Vector< math::Range< size_t> > ranges;
      ranges.AllocateMemory( number_available_cpus);

      // create ranges
      for
      (
        size_t job_number( 0), start( 0), between( interval);
        job_number < number_jobs;
        ++job_number, start = between + 1, between += interval
      )
      {
        // if last job the extend till dataset size
        if( job_number + 1 == number_jobs)
        {
          between = data_set_size - 1;
        }
        // push back result
        ranges.PushBack( math::Range< size_t>( start, between));
      }

      // for every interval create a job
      for
      (
        storage::Vector< math::Range< size_t> >::const_iterator itr_ranges( ranges.Begin()), itr_ranges_end( ranges.End());
        itr_ranges != itr_ranges_end;
        ++itr_ranges
      )
      {
        // create and pushback jobs into scheduler
        schedule.PushBack
        (
          util::ShPtr< sched::JobInterface>
          (
            new sched::TertiaryFunctionJobWithData
            <
              storage::Triplet
              <
                util::SiPtr< const FeatureDataSetInterface< float> >,
                const FeatureReference< float>,
                linal::Vector< float>
              >,
              const size_t,
              const size_t,
              void,
              SupportVectorKernelBase
            >
            (
              group_id,
              *this,
              &SupportVectorKernelBase::ComputePartialInputVectorIKernelMatrix,
              train_index_result,
              itr_ranges->GetMin(),
              itr_ranges->GetMax(),
              sched::JobInterface::e_READY,
              NULL
            )
          )
        );
      }

      // submit jobs
      for
      (
        util::ShPtrList< sched::JobInterface>::iterator itr_jobs( schedule.Begin()), itr_jobs_end( schedule.End());
        itr_jobs != itr_jobs_end;
        ++itr_jobs
      )
      {
        // submit job for descriptor combination
        sched::GetScheduler().RunJob( *itr_jobs);
      }

      // join all jobs ( need to be finished)
      for
      (
        util::ShPtrList< sched::JobInterface>::iterator itr_jobs( schedule.Begin()), itr_jobs_end( schedule.End());
        itr_jobs != itr_jobs_end;
        ++itr_jobs
      )
      {
        sched::GetScheduler().Join( *itr_jobs);
      }

      // initialize kernel data
      const linal::Vector< float> &kernel_values( train_index_result.Third());
      return kernel_values;
    }

        //! @brief compute kernel values for every pair input_vector_i with each other vector in data set
    //! @param TRAINING_DATA featurevector data set without labels
    //! @param INPUT_VECTOR_I feature vector
    //! @param SIGNS
    //! @param PROBLENGTH
    //! @return vector with all combinations towards INPUT_VECTOR_I
    linal::Vector< float> SupportVectorKernelBase::GetInputVectorIKernelMatrix
    (
      const FeatureDataSetInterface< float> &TRAINING_DATA,
      const size_t &INPUT_VECTOR_I,
      const storage::Vector< int> &SIGNS,
      const size_t &PROBLENGTH
    ) const
    {
      // size of data set
      const size_t data_set_size( TRAINING_DATA.GetNumberFeatures());

      // create storage::Vector of size corresponding to entries in TRAINING_DATA initialized with 0.0
      linal::Vector< float> kernel_data( data_set_size, float( 0.0));

      // calculate index value for input vector i
      size_t index_vector_i( 0);

      // adjust index for input vector i
      if( INPUT_VECTOR_I > ( data_set_size - 1))
      {
        index_vector_i = ( INPUT_VECTOR_I - PROBLENGTH / 2);
      }
      else
      {
        index_vector_i = INPUT_VECTOR_I;
      }

      // reference to index_vector_i in training data
      const FeatureReference< float> input_vector_i( TRAINING_DATA( index_vector_i));

      // define a group id for threads that will be created
      const size_t group_id( 0);

      // number of available cpus
      const size_t number_available_cpus( sched::GetNumberCPUs());

      const size_t interval( data_set_size / number_available_cpus);

      BCL_Assert( interval != 0, "Interval is zero! Less data points then processors!");

      // number of jobs
      size_t number_jobs( number_available_cpus);

      storage::Triplet
      <
        util::SiPtr< const FeatureDataSetInterface< float> >,
        const FeatureReference< float>,
        linal::Vector< float>
      > train_index_result
      (
        util::SiPtr< const FeatureDataSetInterface< float> >( TRAINING_DATA),
        input_vector_i,
        kernel_data
      );

      util::ShPtrList< sched::JobInterface> schedule;

      // ranges for jobs
      storage::Vector< math::Range< size_t> > ranges;
      ranges.AllocateMemory( number_available_cpus);

      // create ranges
      for
      (
        size_t job_number( 0), start( 0), between( interval);
        job_number < number_jobs;
        ++job_number, start = between + 1, between += interval
      )
      {
        // if last job the extend till dataset size
        if( job_number + 1 == number_jobs)
        {
          between = data_set_size - 1;
        }
        // push back result
        ranges.PushBack( math::Range< size_t>( start, between));
      }

      // for every interval create a job
      for
      (
        storage::Vector< math::Range< size_t> >::const_iterator itr_ranges( ranges.Begin()), itr_ranges_end( ranges.End());
        itr_ranges != itr_ranges_end;
        ++itr_ranges
      )
      {
        // create and pushback jobs into scheduler
        schedule.PushBack
        (
          util::ShPtr< sched::JobInterface>
          (
            new sched::TertiaryFunctionJobWithData
            <
              storage::Triplet
              <
                util::SiPtr< const FeatureDataSetInterface< float> >,
                const FeatureReference< float>,
                linal::Vector< float>
              >,
              const size_t,
              const size_t,
              void,
              SupportVectorKernelBase
            >
            (
              group_id,
              *this,
              &SupportVectorKernelBase::ComputePartialInputVectorIKernelMatrix,
              train_index_result,
              itr_ranges->GetMin(),
              itr_ranges->GetMax(),
              sched::JobInterface::e_READY,
              NULL
            )
          )
        );
      }

      // submit jobs
      for
      (
        util::ShPtrList< sched::JobInterface>::iterator itr_jobs( schedule.Begin()), itr_jobs_end( schedule.End());
        itr_jobs != itr_jobs_end;
        ++itr_jobs
      )
      {
        // submit job for descriptor combination
        sched::GetScheduler().RunJob( *itr_jobs);
      }

      // join all jobs ( need to be finished)
      for
      (
        util::ShPtrList< sched::JobInterface>::iterator itr_jobs( schedule.Begin()), itr_jobs_end( schedule.End());
        itr_jobs != itr_jobs_end;
        ++itr_jobs
      )
      {
        sched::GetScheduler().Join( *itr_jobs);
      }

      // initialize kernel data
      const linal::Vector< float> &kernel_values( train_index_result.Third());

      // initialize vector for kernel results
      linal::Vector< float> result_data( PROBLENGTH, 0.0);
      // iterator over all signs on training data
      storage::Vector< int>::const_iterator itr_begin_signs( SIGNS.Begin());

      size_t index( 0);
      const size_t problem_real_size( PROBLENGTH / 2);
      const int sign_vector_i( SIGNS( INPUT_VECTOR_I));

      for
      (
        linal::Vector< float>::iterator
          itr_begin_result_data( result_data.Begin()),
          itr_end_result_data( result_data.End());
        itr_begin_result_data != itr_end_result_data;
        ++itr_begin_result_data, ++itr_begin_signs, ++index
      )
      {
        // decide whether you reach outside of the
        if( index >= problem_real_size)
        {
          *itr_begin_result_data = sign_vector_i * ( *itr_begin_signs) * kernel_values( index - problem_real_size);
        }
        else
        {
          *itr_begin_result_data = sign_vector_i * ( *itr_begin_signs) * kernel_values( index);
        }
      }

      return result_data;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief compute kernel matrix for one particular feature with all feature values in training data
    //! @param TRAIN_INDEX_RESULT data structure which contains pointer to training data, current vector,
    //!        and result vector
    void SupportVectorKernelBase::ComputePartialInputVectorIKernelMatrix
    (
      storage::Triplet
      <
        util::SiPtr< const FeatureDataSetInterface< float> >,
        const FeatureReference< float>,
        linal::Vector< float>
      > &TRAIN_INDEX_RESULT,
      const size_t &START_INDEX,
      const size_t &END_INDEX
    ) const
    {
      // starting index of current feature vector in training dataset
      size_t index_vector_j( START_INDEX);

      // iterate over range of result vector
      for
      (
        float *itr_result( TRAIN_INDEX_RESULT.Third().Begin() + START_INDEX);
        index_vector_j <= END_INDEX;
        ++itr_result, ++index_vector_j
      )
      {
        *itr_result = operator()
        (
          TRAIN_INDEX_RESULT.Second(), TRAIN_INDEX_RESULT.First()->operator()( index_vector_j)
        );
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_support_vector_kernel_polynomial.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_feature_reference.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> SupportVectorKernelPolynomial::s_Instance
    (
      util::Enumerated< SupportVectorKernelBase>::AddInstance( new SupportVectorKernelPolynomial())
    );

    //! @brief clone
    //! @return pointer to cloned object
    SupportVectorKernelPolynomial *SupportVectorKernelPolynomial::Clone() const
    {
      return new SupportVectorKernelPolynomial( *this);
    }

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &SupportVectorKernelPolynomial::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief return a string to call the proper device kernel function for opencl
    //! @return a string that launches the opencl device function
    std::string SupportVectorKernelPolynomial::GetDeviceKernelFunctionCallString() const
    {
      return "PolynomialKernelDevice( feature, support_vector, length,   " + util::Format()( m_Exponent) + ", " + util::Format()( m_Homogeneous) + ")";
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &SupportVectorKernelPolynomial::GetAlias() const
    {
      static const std::string s_name( "Polynomial");
      return s_name;
    }

    //! @brief gets the scheme for this function class
    //! @return the scheme for this function class
    const std::string &SupportVectorKernelPolynomial::GetScheme() const
    {
      static const std::string s_desciption( "k(xi, xj) = (xi * xj + homogeneity) ^ exponent");
      return s_desciption;
    }

    //! @brief calculate the kernel value for the linear kernel according to two input vectors
    float SupportVectorKernelPolynomial::operator()
    (
      const FeatureReference< float> &VECTOR_A,
      const FeatureReference< float> &VECTOR_B
    ) const
    {
      // scalar product  of two vectors
      return math::Pow< float>( linal::ScalarProduct( VECTOR_A, VECTOR_B) + float( m_Homogeneous), float( m_Exponent));
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer SupportVectorKernelPolynomial::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( GetScheme());

      parameters.AddInitializer
      (
        "exponent",
        "exponent of the function; allows to model feature conjunctions",
        io::Serialization::GetAgentWithRange( &m_Exponent, 0, 10),
        "1"
      );
      parameters.AddInitializer
      (
        "homogeneity",
        "homogeneity of the polynomial - 0 for homogenous, 1 for inhomogenous",
        io::Serialization::GetAgent( &m_Homogeneous),
        "1"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_support_vector_kernel_rbf.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "model/bcl_model_feature_reference.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> SupportVectorKernelRBF::s_Instance
    (
      util::Enumerated< SupportVectorKernelBase>::AddInstance( new SupportVectorKernelRBF())
    );

    //! @brief constructor
    SupportVectorKernelRBF::SupportVectorKernelRBF() :
      m_Gamma( 1.0)
    {
    }

    //! @brief constructor
    SupportVectorKernelRBF::SupportVectorKernelRBF( const float &GAMMA) :
      m_Gamma( GAMMA)
    {
    }

    //! @brief virtual copy constructor
    SupportVectorKernelRBF *SupportVectorKernelRBF::Clone() const
    {
      return new SupportVectorKernelRBF( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &SupportVectorKernelRBF::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief return a string to call the proper device kernel function for opencl
    //! @return a string that launches the opencl device function
    std::string SupportVectorKernelRBF::GetDeviceKernelFunctionCallString() const
    {
      return "RBFKernelDevice( feature, support_vector, length,   " + util::Format()( m_Gamma) + ")";
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &SupportVectorKernelRBF::GetAlias() const
    {
      static const std::string s_Name( "RBF");
      return s_Name;
    }

    //! @brief gets the scheme for this function class
    //! @return the scheme for this function class
    const std::string &SupportVectorKernelRBF::GetScheme() const
    {
      static const std::string s_scheme( "k( xi, xj)=e^( -gamma * Variance(xi, xj))");
      return s_scheme;
    }

    //! @brief calculate a kernel value for two vectors A and B
    //! @return a kernel value for two vectors A and B
    float SupportVectorKernelRBF::operator()
    (
      const FeatureReference< float> &VECTOR_A,
      const FeatureReference< float> &VECTOR_B
    ) const
    {
      float square_norm( 0.0);
      for
      (
        const float *vec_a( VECTOR_A.Begin()), *vec_b( VECTOR_B.Begin()), *vec_a_end( VECTOR_A.End());
        vec_a != vec_a_end;
        ++vec_a, ++vec_b
      )
      {
        square_norm += math::Sqr( *vec_a - *vec_b);
      }

      return std::exp( -m_Gamma * square_norm);
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer SupportVectorKernelRBF::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( GetScheme());

      parameters.AddInitializer
      (
        "gamma",
        "coefficient in exponential of kernel, larger values attenuate the returned result",
        io::Serialization::GetAgent( &m_Gamma),
        "1.0"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_support_vector_machine.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range for SVM normalization range
    const math::Range< float> SupportVectorMachine::s_DefaultInputRange( 0, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> SupportVectorMachine::s_Instance
    (
      GetObjectInstances().AddInstance( new SupportVectorMachine())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief standard constructor
    SupportVectorMachine::SupportVectorMachine() :
      m_Alpha(),
      m_Bias(),
      m_Kernel(),
      m_NumberBoundSupportVectors(),
      m_NumberSupportVectors(),
      m_RescaleInput(),
      m_RescaleOutput(),
      m_SupportVectors()
    {
    }

    //! @brief standard constructor
    //! @param KERNEL
    //! @param RESCALE_INPUT
    //! @param RESCALE_OUTPUT
    SupportVectorMachine::SupportVectorMachine
    (
      const util::Implementation< SupportVectorKernelBase> &KERNEL,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_INPUT,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_OUTPUT
    ) :
      m_Alpha( storage::Vector< float>()),
      m_Bias( float( 0)),
      m_Kernel( KERNEL),
      m_NumberBoundSupportVectors( size_t( 0)),
      m_NumberSupportVectors( size_t( 0)),
      m_RescaleInput( RESCALE_INPUT),
      m_RescaleOutput( RESCALE_OUTPUT),
      m_SupportVectors( FeatureDataSet< float>( linal::MatrixConstReference< float>()))
    {
    }

    //! @brief standard constructor
    //! @param KERNEL
    SupportVectorMachine::SupportVectorMachine
    (
      const util::Implementation< SupportVectorKernelBase> &KERNEL
    ) :
      m_Alpha( storage::Vector< float>()),
      m_Bias( float( 0)),
      m_Kernel( KERNEL),
      m_NumberBoundSupportVectors( size_t( 0)),
      m_NumberSupportVectors( size_t( 0)),
      m_RescaleInput( new RescaleFeatureDataSet()),
      m_RescaleOutput( new RescaleFeatureDataSet()),
      m_SupportVectors( FeatureDataSet< float>())
    {
    }

    //! @brief virtual copy constructor
    SupportVectorMachine *SupportVectorMachine::Clone() const
    {
      return new SupportVectorMachine( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &SupportVectorMachine::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &SupportVectorMachine::GetAlias() const
    {
      static const std::string s_Name( "SupportVectorMachine");
      return s_Name;
    }

    //! @brief get the output feature size for this model
    //! @return the output feature size for this model
    size_t SupportVectorMachine::GetNumberOutputs() const
    {
      return size_t( 1);
    }

  //////////////
  // operator //
  //////////////

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void SupportVectorMachine::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_RescaleInput);
      }
    }

    //! @brief predict with normalized data
    FeatureDataSet< float> SupportVectorMachine::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURE
    ) const
    {
      const size_t num_points( FEATURE.GetNumberFeatures());
      storage::Vector< float> pred;
      pred.AllocateMemory( num_points);

      for( size_t i( 0); i < num_points; ++i)
      {
        // initialize classification result with bias
        linal::Vector< float> classification( 1, -m_Bias);

        // progress counter for accessing data set with support vectors
        size_t progress( 0);

        //iterator over all LaGrange multiplier alpha
        for
        (
          storage::Vector< float>::const_iterator iter_alpha_begin( GetAlpha().Begin()),
          iter_alpha_end( GetAlpha().End());
          iter_alpha_begin != iter_alpha_end;
          ++iter_alpha_begin, ++progress
        )
        {
          // calculate classification value of alpha and kernel value of unknown vector and a support vector
          classification( 0) +=
            *iter_alpha_begin * m_Kernel->operator()( FEATURE( i), m_SupportVectors( progress));
        }
        pred.PushBack( classification( 0));
      }

      // classification result
      return FeatureDataSet< float>
      (
        linal::Matrix< float>( num_points, 1, linal::Vector< float>( pred.Begin(), pred.End()).Begin()),
        *m_RescaleOutput
      );
    }

    //! predict with not normalized data vector
    FeatureDataSet< float> SupportVectorMachine::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // handle the case where rescaling is necessary
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FeatureDataSet< float> feature( FEATURE);
        feature.Rescale( *m_RescaleInput);
        return PredictWithoutRescaling( feature).DeScale();
      }

      // data is already rescaled
      return PredictWithoutRescaling( FEATURE).DeScale();
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read NeuralNetwork from std::istream
    std::istream &SupportVectorMachine::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_Alpha, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Kernel, ISTREAM);
      io::Serialize::Read( m_NumberBoundSupportVectors, ISTREAM);
      io::Serialize::Read( m_NumberSupportVectors, ISTREAM);
      io::Serialize::Read( m_RescaleInput, ISTREAM);
      io::Serialize::Read( m_RescaleOutput, ISTREAM);
      io::Serialize::Read( m_SupportVectors, ISTREAM);

      // end
      return ISTREAM;
    }

    //! write NeuralNetwork into std::ostream
    std::ostream &SupportVectorMachine::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_Alpha, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Kernel, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NumberBoundSupportVectors, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NumberSupportVectors, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleInput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleOutput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SupportVectors, OSTREAM, INDENT);

      // end
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer SupportVectorMachine::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
        (
          "see http://en.wikipedia.org/wiki/Support_vector_machine"
        );

      parameters.AddInitializer
      (
        "kernel",
        "kernel used to map pairs of features onto a hyperplane",
        io::Serialization::GetAgent( &m_Kernel),
        "RBF(gamma=0.5)"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_support_vector_machine_multi_output.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "linal/bcl_linal_vector_operations.h"
#include "model/bcl_model_rescale_feature_data_set.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range for SVM normalization range
    const math::Range< float> SupportVectorMachineMultiOutput::s_DefaultInputRange( 0, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> SupportVectorMachineMultiOutput::s_Instance
    (
      GetObjectInstances().AddInstance( new SupportVectorMachineMultiOutput())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief standard constructor
    SupportVectorMachineMultiOutput::SupportVectorMachineMultiOutput() :
      m_Beta(),
      m_Bias( linal::Vector< float>( 1, float(0))),
      m_Kernel(),
      m_NumberBoundSupportVectors(),
      m_NumberSupportVectors(),
      m_RescaleInput(),
      m_RescaleOutput(),
      m_SupportVectors()
    {
    }

    //! @brief standard constructor
    //! @param KERNEL used Kernel Function
    //! @param RESCALE_INPUT Rescale Function for inputs (descriptors)
    //! @param RESCALE_OUTPUT Rescale Function for outputs (results)
    SupportVectorMachineMultiOutput::SupportVectorMachineMultiOutput
    (
      const util::Implementation< SupportVectorKernelBase> &KERNEL,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_INPUT,
      const util::ShPtr< RescaleFeatureDataSet> &RESCALE_OUTPUT
    ) :
      m_Beta( storage::Vector< linal::Vector< float> >()),
      m_Bias( linal::Vector< float>( 1, float( 0))),
      m_Kernel( KERNEL),
      m_NumberBoundSupportVectors( size_t( 0)),
      m_NumberSupportVectors( size_t( 0)),
      m_RescaleInput( RESCALE_INPUT),
      m_RescaleOutput( RESCALE_OUTPUT),
      m_SupportVectors( FeatureDataSet< float>( linal::MatrixReference< float>()))
    {
    }

    //! @brief standard constructor
    //! @param KERNEL used Kernel Function
    SupportVectorMachineMultiOutput::SupportVectorMachineMultiOutput
    (
      const util::Implementation< SupportVectorKernelBase> &KERNEL
    ) :
      m_Beta(),
      m_Bias( linal::Vector< float>( 1, float(0))),
      m_Kernel( KERNEL),
      m_NumberBoundSupportVectors( size_t( 0)),
      m_NumberSupportVectors( size_t( 0)),
      m_RescaleInput( new RescaleFeatureDataSet()),
      m_RescaleOutput( new RescaleFeatureDataSet()),
      m_SupportVectors( FeatureDataSet< float>())
    {
    }

    //! @brief virtual copy constructor
    SupportVectorMachineMultiOutput *SupportVectorMachineMultiOutput::Clone() const
    {
      return new SupportVectorMachineMultiOutput( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &SupportVectorMachineMultiOutput::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &SupportVectorMachineMultiOutput::GetAlias() const
    {
      static const std::string s_Name( "SupportVectorMachineMultiOutput");
      return s_Name;
    }

    //! @brief Get a description for what this class does (used when writing help)
    //! @return a description for what this class does (used when writing help)
    const std::string &SupportVectorMachineMultiOutput::GetClassDescription() const
    {
      static const std::string s_Description( "see http://en.wikipedia.org/wiki/Support_vector_machine");
      return s_Description;
    }

  //////////////
  // operator //
  //////////////

  ////////////////
  // operations //
  ////////////////

    //! @brief Set the scaling of a feature set according to the model
    //! @param FEATURES feature set of interest
    //! @note this allows for external classes that own a dataset to ensure that a new dataset is never created
    //!       when operator() is called
    void SupportVectorMachineMultiOutput::Rescale( FeatureDataSet< float> &FEATURE) const
    {
      if( !FEATURE.IsRescaled() || *FEATURE.GetScaling() != *m_RescaleInput)
      {
        FEATURE.DeScale();
        FEATURE.Rescale( *m_RescaleInput);
      }
    }

    //! @brief predict result with model using a rescaled feature vector
    //! @param FEATURE normalized or rescaled feature vector
    //! @return predcited normalized result vector using a model
    FeatureDataSet< float> SupportVectorMachineMultiOutput::PredictWithoutRescaling
    (
      const FeatureDataSetInterface< float> &FEATURE
    ) const
    {
      const size_t num_points( FEATURE.GetNumberFeatures());
      linal::Matrix< float> prediction( num_points, m_Bias.GetSize(), float( 0.0));

      for( size_t i( 0); i < num_points; ++i)
      {
        // initialize classification result with bias
        linal::Vector< float> classification( m_Bias);

        // progress counter for accessing data set with support vectors
        size_t progress( 0);

        //iterator over all LaGrange multiplier alpha
        for
        (
          storage::Vector< linal::Vector< float> >::const_iterator iter_beta( GetBeta().Begin()),
          iter_beta_end( GetBeta().End());
          iter_beta != iter_beta_end;
          ++iter_beta, ++progress
        )
        {
          // calculate classification value of beta and kernel value of unknown vector and a support vector
          classification +=
            *iter_beta * m_Kernel->operator()( FEATURE( i), m_SupportVectors( progress));
        }
        prediction.ReplaceRow( i, classification);
      }

      // classification result
      return FeatureDataSet< float>( prediction);

    }

    //! @brief predict result with model using a NOT rescaled feature vector
    //! @param FEATURE not rescaled feature vector
    //! @return predcited result vector using a model
    FeatureDataSet< float> SupportVectorMachineMultiOutput::operator()( const FeatureDataSetInterface< float> &FEATURE) const
    {
      // predict on an unnormalized feature vector
      const FeatureDataSet< float> result( PredictWithoutRescaling( m_RescaleInput->operator()( FEATURE)));
      return m_RescaleOutput->DeScale( result);
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! @brief read SupportVectorMachineMultiOutput from std::istream
    //! @param ISTREAM input stream containing object
    //! @return modified input stream
    std::istream &SupportVectorMachineMultiOutput::Read( std::istream &ISTREAM)
    {
      // read member
      io::Serialize::Read( m_Beta, ISTREAM);
      io::Serialize::Read( m_Bias, ISTREAM);
      io::Serialize::Read( m_Kernel, ISTREAM);
      io::Serialize::Read( m_NumberSupportVectors, ISTREAM);
      io::Serialize::Read( m_RescaleInput, ISTREAM);
      io::Serialize::Read( m_RescaleOutput, ISTREAM);
      io::Serialize::Read( m_SupportVectors, ISTREAM);
      // end
      return ISTREAM;
    }

    //! @brief write SupportVectorMachineMultiOutput into std::ostream
    //! @param OSTREAM original output stream
    //! @param INDENT number of spaces for indent
    //! @return stream with appended data
    std::ostream &SupportVectorMachineMultiOutput::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      // write member
      io::Serialize::Write( m_Beta, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Bias, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_Kernel, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NumberSupportVectors, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleInput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_RescaleOutput, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_SupportVectors, OSTREAM, INDENT);
      // end
      return OSTREAM;
    }

  //////////////////////
  // helper functions //
  //////////////////////

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer SupportVectorMachineMultiOutput::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
        (
          "see http://en.wikipedia.org/wiki/Support_vector_machine"
        );

      parameters.AddInitializer
      (
        "kernel",
        "kernel used to map pairs of features onto a hyperspace",
        io::Serialization::GetAgent( &m_Kernel),
        "RBF(gamma=0.5)"
      );

      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_train_restricted_boltzmann_machine_layer.h"

// includes from bcl - sorted alphabetically
#include "linal/bcl_linal_matrix_const_reference.h"
#include "linal/bcl_linal_vector_operations.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    //! the default input range for neural network transfer functions
    const math::Range< float> TrainRestrictedBoltzmannMachineLayer::s_DefaultInputRange( 0, 1);

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> TrainRestrictedBoltzmannMachineLayer::s_Instance
    (
      GetObjectInstances().AddInstance( new TrainRestrictedBoltzmannMachineLayer())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    TrainRestrictedBoltzmannMachineLayer::TrainRestrictedBoltzmannMachineLayer() :
      m_StochasticStepCount( 0),
      m_NumberFeaturesSeen( 0)
    {
    }

    //! @brief constructor from an untrained network
    //! @param LAYER the layer that this trainer will handle
    //! @param STOCHASTIC_STEP_COUNT helps control generalizability, see note above on m_StochasticStepCount
    TrainRestrictedBoltzmannMachineLayer::TrainRestrictedBoltzmannMachineLayer
    (
      RestrictedBoltzmannMachineLayer &LAYER,
      const size_t &STOCHASTIC_STEP_COUNT
    ) :
      m_Layer( LAYER),
      m_StochasticStepCount( STOCHASTIC_STEP_COUNT),
      m_WeightGradient( LAYER.GetNumberInputNeurons(), LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_BiasVisibleGradient( LAYER.GetNumberInputNeurons(), float( 0)),
      m_BiasHiddenGradient( LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_NoiseVisibleGradient( LAYER.GetNumberInputNeurons(), float( 0)),
      m_NoiseHiddenGradient( LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_Hidden( LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_VisibleReconstructed( LAYER.GetNumberInputNeurons(), float( 0)),
      m_HiddenReconstructed( LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_VisibleSample( LAYER.GetNumberInputNeurons(), float( 0)),
      m_HiddenSample( LAYER.GetNumberHiddenNeurons(), float( 0)),
      m_NumberFeaturesSeen( 0)
    {
    }

    //! copy constructor
    TrainRestrictedBoltzmannMachineLayer *TrainRestrictedBoltzmannMachineLayer::Clone() const
    {
      return new TrainRestrictedBoltzmannMachineLayer( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TrainRestrictedBoltzmannMachineLayer::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief core function : takes a feature and uses it to update the associated gradients
    //! @param INPUT the input feature of interest
    //! @return reference to the hidden layer output
    linal::VectorConstReference< float> TrainRestrictedBoltzmannMachineLayer::Train( const linal::VectorConstInterface< float> &INPUT)
    {
      BCL_Assert
      (
        INPUT.GetSize() == m_Layer->GetNumberInputNeurons(),
        "INPUT.GetSize() does not match the size of the layer!"
      );

      const float *const itr_bias_vg_end( m_BiasVisibleGradient.End());
      const float *const itr_bias_hg_end( m_BiasHiddenGradient.End());
      const float *const itr_weight_gradient_end( m_WeightGradient.End());
      const float *const itr_hid_recon_end( m_HiddenReconstructed.End());

      // perform 1 round of Gibbs sampling
      m_Layer->GibbsSample
      (
        INPUT,
        m_Hidden,
        m_VisibleReconstructed,
        m_HiddenReconstructed,
        m_StochasticStepCount,
        m_HiddenSample,
        m_VisibleSample
      );

      // Update gradients
      // Update Visible bias gradient with FEATURE - visible_reconstructed
      {
        const float *itr_vis( INPUT.Begin());
        for
        (
          float *itr_bias_vg( m_BiasVisibleGradient.Begin()), *itr_vis_recon( m_VisibleReconstructed.Begin());
          itr_bias_vg != itr_bias_vg_end;
          ++itr_vis, ++itr_bias_vg, ++itr_vis_recon
        )
        {
          *itr_bias_vg += *itr_vis - *itr_vis_recon;
        }
      }
      {
        const float *itr_hidden( m_Hidden.Begin());
        // Update Hidden bias gradient hidden - hidden_reconstructed
        for
        (
          float *itr_bias_hg( m_BiasHiddenGradient.Begin()), *itr_hid_recon( m_HiddenReconstructed.Begin());
          itr_bias_hg != itr_bias_hg_end;
          ++itr_hidden, ++itr_bias_hg, ++itr_hid_recon
        )
        {
          *itr_bias_hg += *itr_hidden - *itr_hid_recon;
        }
      }
      // noise update
      if( m_Layer->m_NetworkType == RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
      {
        const float *itr_vis_recon( m_VisibleReconstructed.Begin()), *itr_vis_recon_end( m_VisibleReconstructed.End());
        const float *itr_vis( INPUT.Begin());
        for
        (
          float *itr_grad_noise( m_NoiseVisibleGradient.Begin());
          itr_vis_recon != itr_vis_recon_end;
          ++itr_vis, ++itr_vis_recon, ++itr_grad_noise
        )
        {
          *itr_grad_noise += math::Sqr( *itr_vis) - math::Sqr( *itr_vis_recon);
        }
        const float *itr_hid_recon( m_HiddenReconstructed.Begin()), *itr_hidden( m_Hidden.Begin());
        for
        (
          float *itr_grad_noise( m_NoiseHiddenGradient.Begin());
          itr_hid_recon != itr_hid_recon_end;
          ++itr_hidden, ++itr_hid_recon, ++itr_grad_noise
        )
        {
          *itr_grad_noise += math::Sqr( *itr_hidden) - math::Sqr( *itr_hid_recon);
        }
      }
      {
        // Update weight gradient with hidden' * FEATURE - hidden_reconstructed' * visible_reconstructed;
        float *itr_weight_gradient( m_WeightGradient.Begin());
        for
        (
          const float *itr_visible( INPUT.Begin()), *itr_visible_recon( m_VisibleReconstructed.Begin());
          itr_weight_gradient != itr_weight_gradient_end;
          ++itr_visible, ++itr_visible_recon
        )
        {
          const float *itr_hidden( m_Hidden.Begin());
          for
          (
            const float *itr_hid_recon( m_HiddenReconstructed.Begin());
            itr_hid_recon != itr_hid_recon_end;
            ++itr_hid_recon, ++itr_hidden, ++itr_weight_gradient
          )
          {
            *itr_weight_gradient += *itr_hidden * *itr_visible - *itr_hid_recon * *itr_visible_recon;
          }
        }
      }
      ++m_NumberFeaturesSeen;
      return linal::VectorConstReference< float>( m_Hidden);
    }

    //! @brief core function : takes a feature and a result and uses it to update the associated gradients
    //! @param INPUT the input feature of interest
    //! @param OUTPUT the output feature of interest
    //! @return reference to reconstructed output
    linal::VectorConstReference< float> TrainRestrictedBoltzmannMachineLayer::Train
    (
      const linal::VectorConstInterface< float> &INPUT,
      const linal::VectorConstInterface< float> &OUTPUT
    )
    {
      BCL_Assert
      (
        INPUT.GetSize() + OUTPUT.GetSize() == m_Layer->GetNumberInputNeurons(),
        "INPUT.GetSize() + OUTPUT.GetSize() does not match the size of the layer!"
      );

      const float *const itr_bias_hg_end( m_BiasHiddenGradient.End());
      const float *const itr_hid_recon_end( m_HiddenReconstructed.End());

      // perform 1 round of Gibbs sampling
      m_Layer->GibbsSample
      (
        INPUT,
        m_Hidden,
        m_VisibleReconstructed,
        m_HiddenReconstructed,
        m_StochasticStepCount,
        m_HiddenSample,
        m_VisibleSample
      );

      // Update gradients
      // Update Visible bias gradient with FEATURE - visible_reconstructed
      {
        float *itr_bias_vg( m_BiasVisibleGradient.Begin()), *itr_vis_recon( m_VisibleReconstructed.Begin());
        for
        (
          const float *itr_vis( INPUT.Begin()), *itr_vis_end( INPUT.End());
          itr_vis != itr_vis_end;
          ++itr_vis, ++itr_bias_vg, ++itr_vis_recon
        )
        {
          *itr_bias_vg += *itr_vis - *itr_vis_recon;
        }
        for
        (
          const float *itr_out( OUTPUT.Begin()), *itr_out_end( OUTPUT.End());
          itr_out != itr_out_end;
          ++itr_out, ++itr_bias_vg, ++itr_vis_recon
        )
        {
          *itr_bias_vg += *itr_out - *itr_vis_recon;
        }
      }
      {
        const float *itr_hidden( m_Hidden.Begin());
        // Update Hidden bias gradient hidden - hidden_reconstructed
        for
        (
          float *itr_bias_hg( m_BiasHiddenGradient.Begin()), *itr_hid_recon( m_HiddenReconstructed.Begin());
          itr_bias_hg != itr_bias_hg_end;
          ++itr_hidden, ++itr_bias_hg, ++itr_hid_recon
        )
        {
          *itr_bias_hg += *itr_hidden - *itr_hid_recon;
        }
      }
      // noise update
      if( m_Layer->m_NetworkType == RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
      {
        {
          const float *itr_vis_recon( m_VisibleReconstructed.Begin());
          float *itr_grad_noise( m_NoiseVisibleGradient.Begin());
          for
          (
            const float *itr_vis( INPUT.Begin()), *itr_vis_end( INPUT.End());
            itr_vis != itr_vis_end;
            ++itr_vis, ++itr_vis_recon, ++itr_grad_noise
          )
          {
            *itr_grad_noise += math::Sqr( *itr_vis) - math::Sqr( *itr_vis_recon);
          }
          for
          (
            const float *itr_out( OUTPUT.Begin()), *itr_out_end( OUTPUT.End());
            itr_out != itr_out_end;
            ++itr_out, ++itr_vis_recon, ++itr_grad_noise
          )
          {
            *itr_grad_noise += math::Sqr( *itr_out) - math::Sqr( *itr_vis_recon);
          }
        }
        const float *itr_hid_recon( m_HiddenReconstructed.Begin()), *itr_hidden( m_Hidden.Begin());
        for
        (
          float *itr_grad_noise( m_NoiseHiddenGradient.Begin());
          itr_hid_recon != itr_hid_recon_end;
          ++itr_hidden, ++itr_hid_recon, ++itr_grad_noise
        )
        {
          *itr_grad_noise += math::Sqr( *itr_hidden) - math::Sqr( *itr_hid_recon);
        }
      }
      {
        // Update weight gradient with hidden' * FEATURE - hidden_reconstructed' * visible_reconstructed;
        float *itr_weight_gradient( m_WeightGradient.Begin());
        const float *itr_visible_recon( m_VisibleReconstructed.Begin());
        for
        (
          const float *itr_visible( INPUT.Begin()), *itr_visible_end( INPUT.End());
          itr_visible != itr_visible_end;
          ++itr_visible, ++itr_visible_recon
        )
        {
          const float *itr_hidden( m_Hidden.Begin());
          for
          (
            const float *itr_hid_recon( m_HiddenReconstructed.Begin());
            itr_hid_recon != itr_hid_recon_end;
            ++itr_hid_recon, ++itr_hidden, ++itr_weight_gradient
          )
          {
            *itr_weight_gradient += *itr_hidden * *itr_visible - *itr_hid_recon * *itr_visible_recon;
          }
        }
        for
        (
          const float *itr_out( OUTPUT.Begin()), *itr_out_end( OUTPUT.End());
          itr_out != itr_out_end;
          ++itr_out, ++itr_visible_recon
        )
        {
          const float *itr_hidden( m_Hidden.Begin());
          for
          (
            const float *itr_hid_recon( m_HiddenReconstructed.Begin());
            itr_hid_recon != itr_hid_recon_end;
            ++itr_hid_recon, ++itr_hidden, ++itr_weight_gradient
          )
          {
            *itr_weight_gradient += *itr_hidden * *itr_out - *itr_hid_recon * *itr_visible_recon;
          }
        }
      }
      ++m_NumberFeaturesSeen;

      return linal::VectorConstReference< float>( OUTPUT.GetSize(), m_VisibleReconstructed.Begin() + INPUT.GetSize());
    }

    //! @brief Update layer to update the associated RestrictedBoltzmannMachineLayer's parameters
    //! @param WEIGHT_COST relative cost of weight
    //! @param WEIGHT_UPDATE update object for the weight
    //! @param VISIBLE_BIAS_UPDATE update object for the visible layer's bias
    //! @param HIDDEN_BIAS_UPDATE update object for the hidden layer's bias
    //! @param VISIBLE_NOISE_UPDATE update object for the visible layer's noise (for StochasticSigmoid RBMs)
    //! @param HIDDEN_NOISE_UPDATE update object for the hidden layer's noise (for StochasticSigmoid RBMs)
    void TrainRestrictedBoltzmannMachineLayer::UpdateLayer
    (
      const float &WEIGHT_COST,
      NeuralNetworkUpdateWeightsInterface &WEIGHT_UPDATE,
      NeuralNetworkUpdateWeightsInterface &VISIBLE_BIAS_UPDATE,
      NeuralNetworkUpdateWeightsInterface &HIDDEN_BIAS_UPDATE,
      NeuralNetworkUpdateWeightsInterface &VISIBLE_NOISE_UPDATE,
      NeuralNetworkUpdateWeightsInterface &HIDDEN_NOISE_UPDATE
    )
    {
      // divide all weight vectors by the number of feature seen
      m_BiasHiddenGradient /= float( m_NumberFeaturesSeen);
      m_BiasVisibleGradient /= float( m_NumberFeaturesSeen);
      m_NoiseHiddenGradient /= float( m_NumberFeaturesSeen);
      m_NoiseVisibleGradient /= float( m_NumberFeaturesSeen);
      m_WeightGradient /= float( m_NumberFeaturesSeen);

      const float *const itr_weight_gradient_end( m_WeightGradient.End());
      // weight decay and update of change weight
      // This code tries to just do an efficient version of the following:
      // weight_gradient -= WEIGHT_COST * m_Weight;
      // m_Weight += change_weight_gradient;
      for
      (
        float *itr_weight_grad( m_WeightGradient.Begin()), *itr_weight( m_Layer->m_Weight.Begin());
        itr_weight_grad != itr_weight_gradient_end;
        ++itr_weight_grad, ++itr_weight
      )
      {
        // weight decay on the gradient
        *itr_weight_grad -= *itr_weight * WEIGHT_COST;
      }
      // update weights
      WEIGHT_UPDATE( m_Layer->m_Weight.Begin(), m_WeightGradient.Begin());

      // update bias
      VISIBLE_BIAS_UPDATE( m_Layer->m_BiasVisible.Begin(), m_BiasVisibleGradient.Begin());
      HIDDEN_BIAS_UPDATE( m_Layer->m_BiasHidden.Begin(), m_BiasHiddenGradient.Begin());

      // update noise for stochastic sigmoidal network
      if( m_Layer->m_NetworkType == RestrictedBoltzmannMachineLayer::e_StochasticSigmoid)
      {
        m_NoiseVisibleGradient /= m_Layer->m_SlopeVisible;
        m_NoiseVisibleGradient /= m_Layer->m_SlopeVisible;
        m_NoiseHiddenGradient /= m_Layer->m_SlopeHidden;
        m_NoiseHiddenGradient /= m_Layer->m_SlopeHidden;

        VISIBLE_NOISE_UPDATE( m_Layer->m_SlopeVisible.Begin(), m_NoiseVisibleGradient.Begin());
        HIDDEN_NOISE_UPDATE( m_Layer->m_SlopeHidden.Begin(), m_NoiseHiddenGradient.Begin());
      }
    }

    //! @brief Reset this object, to prepare for the next batch of data
    void TrainRestrictedBoltzmannMachineLayer::Reset()
    {
      m_WeightGradient.SetZero();
      m_BiasVisibleGradient = float( 0.0);
      m_NoiseVisibleGradient = float( 0.0);
      m_BiasHiddenGradient = float( 0.0);
      m_BiasVisibleGradient = float( 0.0);
      m_NumberFeaturesSeen = 0;
    }

    //! @brief Accumulate changes from a different trainer of the same layer (used for reduction after thread-completion)
    //! @param OTHER the other trainer's whose changes should be added to this layers
    void TrainRestrictedBoltzmannMachineLayer::AccumulateChangesFrom( const TrainRestrictedBoltzmannMachineLayer &OTHER)
    {
      m_WeightGradient += OTHER.m_WeightGradient;
      m_BiasVisibleGradient += OTHER.m_BiasVisibleGradient;
      m_BiasHiddenGradient += OTHER.m_BiasHiddenGradient;
      m_NoiseVisibleGradient += OTHER.m_NoiseVisibleGradient;
      m_NoiseHiddenGradient += OTHER.m_NoiseHiddenGradient;
      m_NumberFeaturesSeen += OTHER.m_NumberFeaturesSeen;
    }

  //////////////////////
  // input and output //
  //////////////////////

    //! read TrainRestrictedBoltzmannMachineLayer from std::istream
    std::istream &TrainRestrictedBoltzmannMachineLayer::Read( std::istream &ISTREAM)
    {
      io::Serialize::Read( *m_Layer, ISTREAM);
      io::Serialize::Read( m_StochasticStepCount, ISTREAM);
      io::Serialize::Read( m_WeightGradient, ISTREAM);
      io::Serialize::Read( m_BiasVisibleGradient, ISTREAM);
      io::Serialize::Read( m_BiasHiddenGradient, ISTREAM);
      io::Serialize::Read( m_NoiseVisibleGradient, ISTREAM);
      io::Serialize::Read( m_NoiseHiddenGradient, ISTREAM);
      io::Serialize::Read( m_NumberFeaturesSeen, ISTREAM);

      // reset the members that do not store state
      m_HiddenSample = m_HiddenReconstructed = m_Hidden = m_BiasVisibleGradient;
      m_VisibleReconstructed = m_VisibleSample = m_BiasVisibleGradient;
      return ISTREAM;
    }

    //! write TrainRestrictedBoltzmannMachineLayer into std::ostream
    std::ostream &TrainRestrictedBoltzmannMachineLayer::Write( std::ostream &OSTREAM, const size_t INDENT) const
    {
      io::Serialize::Write( *m_Layer, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_StochasticStepCount, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_WeightGradient, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasVisibleGradient, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_BiasHiddenGradient, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NoiseVisibleGradient, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NoiseHiddenGradient, OSTREAM, INDENT) << '\n';
      io::Serialize::Write( m_NumberFeaturesSeen, OSTREAM, INDENT);
      return OSTREAM;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_training_schedule.h"

// includes from bcl - sorted alphabetically
#include "descriptor/bcl_descriptor_dataset.h"
#include "io/bcl_io_directory_entry.h"
#include "io/bcl_io_file.h"
#include "io/bcl_io_serialization.h"
#include "iterate/bcl_iterate_reflecting.h"
#include "model/bcl_model_retrieve_data_set_from_delimited_file.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief constructor from members
    TrainingSchedule::TrainingSchedule
    (
      const bool &SHUFFLE,
      const bool &BALANCE,
      const size_t &MAX_REPEATS,
      const float &TARGET_OVERSAMPLING
    ) :
      m_Balance( BALANCE),
      m_Shuffle( SHUFFLE),
      m_BalanceMaxRepeatedFeatures( MAX_REPEATS),
      m_BalanceMaxOversampling( TARGET_OVERSAMPLING)
    {
    }

    //! @brief Clone function
    //! @return pointer to new TrainingSchedule
    TrainingSchedule *TrainingSchedule::Clone() const
    {
      return new TrainingSchedule( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TrainingSchedule::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &TrainingSchedule::GetAlias() const
    {
      static const std::string s_name( "TrainingScheduler");
      return s_name;
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief Setup the balancer
    //! @param RESULTS results feature data set
    //! @param CUTOFF cutoff from the objective function that separates classes
    void TrainingSchedule::Setup
    (
      const FeatureDataSetInterface< float> &RESULTS,
      const float &CUTOFF
    )
    {
      const linal::MatrixConstReference< float> results( RESULTS.GetMatrix());

      const size_t results_size( RESULTS.GetFeatureSize());
      const size_t dataset_size( RESULTS.GetNumberFeatures());

      if( util::IsDefined( CUTOFF))
      {
        // Rescaled cutoffs, one for each result
        linal::Vector< float> scaled_cutoffs( results_size, CUTOFF);

        // compute the rescaled cutoffs
        if( RESULTS.IsRescaled())
        {
          const util::SiPtr< const RescaleFeatureDataSet> results_scaling( RESULTS.GetScaling());
          for( size_t result( 0); result < results_size; ++result)
          {
            scaled_cutoffs( result) = results_scaling->RescaleValue( result, CUTOFF);
          }
        }

        const size_t bitsize( sizeof( size_t) * 8);
        const size_t n_sizets( ( results_size - 1) / bitsize + 1);
        storage::Vector< size_t> result_class( n_sizets, size_t( 0));
        m_ResultClass = storage::Vector< size_t>( RESULTS.GetNumberFeatures(), size_t( 0));
        // for each result, compute the result class
        size_t n_classes_so_far( 0);
        storage::Map< storage::Vector< size_t>, size_t> class_to_index;
        storage::Vector< storage::Vector< size_t> > classes;
        for( size_t feature( 0); feature < dataset_size; ++feature)
        {
          result_class.SetAllElements( 0);
          linal::VectorConstReference< float> result_row( results.GetRow( feature));
          for( size_t result_index( 0); result_index < results_size; ++result_index)
          {
            if( result_row( result_index) >= scaled_cutoffs( result_index))
            {
              result_class( result_index / bitsize) |= ( 1 << ( result_index % bitsize));
            }
          }
          storage::Map< storage::Vector< size_t>, size_t>::const_iterator itr_map( class_to_index.Find( result_class));
          if( itr_map == class_to_index.End())
          {
            class_to_index[ result_class] = m_ResultClass( feature) = n_classes_so_far++;
            classes.PushBack( result_class);
          }
          else
          {
            m_ResultClass( feature) = itr_map->second;
          }
        }
        // accumulate all classes
        m_PeerFeatures = storage::Vector< storage::Vector< size_t> >( n_classes_so_far);
        for( size_t feature( 0); feature < dataset_size; ++feature)
        {
          m_PeerFeatures( m_ResultClass( feature)).PushBack( feature);
        }
        std::stringstream info_stream;
        info_stream << "Found " << n_classes_so_far << " classes of training points. Counts per class: ";
        for( size_t class_id( 0); class_id < n_classes_so_far; ++class_id)
        {
          info_stream << m_PeerFeatures( class_id).GetSize() << ' ';
        }
        info_stream << ".  Class Binary IDs: ";
        for( size_t class_id( 0); class_id < n_classes_so_far; ++class_id)
        {
          for( size_t result_index( 0); result_index < results_size; ++result_index)
          {
            info_stream << int( bool( classes( class_id)( result_index / bitsize) & ( 1 << ( result_index % bitsize))));
          }
          info_stream << ' ';
        }
        BCL_MessageStd( info_stream.str());
      }

      m_Order.Resize( dataset_size);
      for( size_t i( 0); i < dataset_size; ++i)
      {
        m_Order( i) = i;
      }

      if( m_Balance)
      {
        // get scaling and cutoff information
        if( !util::IsDefined( CUTOFF))
        {
          BCL_MessageCrt( "Cannot balance a dataset when a non-classification type objective is used!");
          return;
        }
        // create reflecting iterators for each class and determine the size of the most populated class
        const size_t n_classes( m_PeerFeatures.GetSize());
        BCL_Assert( n_classes > 1, "Only a single class of features was found; balancing is impossible!");
        size_t max_class_size( m_PeerFeatures.FirstElement().GetSize());
        storage::Vector< iterate::Reflecting< const size_t> > class_iterators;
        for( size_t class_id( 0); class_id < n_classes; ++class_id)
        {
          class_iterators.PushBack
          (
            iterate::Reflecting< const size_t>( m_PeerFeatures( class_id).Begin(), m_PeerFeatures( class_id).End())
          );
          max_class_size = std::max( max_class_size, m_PeerFeatures( class_id).GetSize());
        }
        size_t second_most_common_class_size( 0);
        for( size_t class_id( 0); class_id < n_classes; ++class_id)
        {
          if( m_PeerFeatures( class_id).GetSize() != max_class_size)
          {
            second_most_common_class_size = std::max( second_most_common_class_size, m_PeerFeatures( class_id).GetSize());
          }
        }

        size_t max_target_size( std::min( size_t( max_class_size * m_BalanceMaxOversampling + 1), max_class_size));
        storage::Vector< size_t> max_this_class( n_classes, max_class_size);
        float oversampling_factor
        (
          std::min( float( m_BalanceMaxRepeatedFeatures), float( max_target_size) / float( second_most_common_class_size))
        );
        if( oversampling_factor > 1.0)
        {
          for( size_t class_id( 0); class_id < n_classes; ++class_id)
          {
            if( m_PeerFeatures( class_id).GetSize() < max_class_size)
            {
              max_this_class( class_id) =
                std::min( m_PeerFeatures( class_id).GetSize() * oversampling_factor, float( max_target_size));
            }
          }
          m_Order.Reset();
          m_Order.AllocateMemory( n_classes * max_class_size);
          for( size_t class_feature( 0); class_feature < max_class_size; ++class_feature)
          {
            for( size_t class_id( 0); class_id < n_classes; ++class_id)
            {
              if( class_feature <= max_this_class( class_id))
              {
                m_Order.PushBack( *class_iterators( class_id));
                ++class_iterators( class_id);
              }
            }
          }
        }
      }
      //m_Order.Shuffle();
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer TrainingSchedule::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription
      (
        "Chooses the order in which training examples are visited, optionally to balance classes"
      );
      parameters.AddInitializer
      (
        "shuffle",
        "primarily for non-batch update; if true, shuffle the order or data points between each run through the data",
        io::Serialization::GetAgent( &m_Shuffle),
        "False"
      );
      parameters.AddInitializer
      (
        "balance",
        "Whether to automatically balance each class (as defined by the objective function's cutoff, if applicable)",
        io::Serialization::GetAgent( &m_Balance),
        "False"
      );
      parameters.AddInitializer
      (
        "balance max repeats",
        "Applies only if balance=True; absolute maximum number of times that a feature can be repeated in order to reach"
        "the targeted ratio of positives to negatives",
        io::Serialization::GetAgent( &m_BalanceMaxRepeatedFeatures),
        "1000000"
      );
      parameters.AddInitializer
      (
        "balance target ratio",
        "Applies only if balance=True; target ratio between most-common and underrepresented class in the dataset "
        "achieved by data replication; to simulate normal balancing; this should be 1, but smaller values may yield "
        "more general models",
        io::Serialization::GetAgentWithRange( &m_BalanceMaxOversampling, float( 0.0), float( 1.0)),
        "1"
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_transfer_function_interface.h"

// includes from bcl - sorted alphabetically

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {
    //! @brief overloaded F for vectors, calls F(float)
    //! @param ARGUMENT_X the vector of arguments for the derivative of the transfer function
    //! @return function value of the transfer function for a vector of arguments
    linal::Vector< float> TransferFunctionInterface::F( const linal::VectorConstInterface< float> &ARGUMENT_X) const
    {
      linal::Vector< float> storage( ARGUMENT_X.GetSize());
      this->F( storage, ARGUMENT_X);
      return storage;
    }

    //! overloaded F for vectors, calls F(float), uses storage provided by user
    void TransferFunctionInterface::F
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X
    ) const
    {
      BCL_Assert( STORAGE.GetSize() == ARGUMENT_X.GetSize(), "storage and argument vectors must be the same size!");
      const float *x( ARGUMENT_X.Begin());
      for
      (
        float *itr_storage( STORAGE.Begin()), *itr_storage_end( STORAGE.End());
        itr_storage != itr_storage_end;
        ++itr_storage, ++x
      )
      {
        *itr_storage = this->F( *x);
      }
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_transfer_gaussian.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_range.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> TransferGaussian::s_Instance
    (
      util::Enumerated< TransferFunctionInterface>::AddInstance( new TransferGaussian())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    TransferGaussian::TransferGaussian()
    {
    }

    //! copy constructor
    TransferGaussian *TransferGaussian::Clone() const
    {
      return new TransferGaussian( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TransferGaussian::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the working x range of that function
    //! @return x math::Range this function works on
    const math::Range< float> &TransferGaussian::GetOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0, 1);
      return s_OutputRange;
    }

    //! @brief get the range between which the function has a significant derivative
    //! @return x math::Range this function works on
    //! @note this is set to the range at which dF(y) is == 1/25 max(dF(y))
    const math::Range< float> &TransferGaussian::GetDynamicOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0.024261, 1.0 - 0.024261);
      return s_OutputRange;
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &TransferGaussian::GetAlias() const
    {
      static const std::string s_Name( "Gaussian");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief function for one argument and result - depends on x ( ARGUMENT_X)
    //! @param ARGUMENT_X the argument for the transfer function
    //! @return function value of sigmoid transfer function
    float TransferGaussian::F( const float &ARGUMENT_X) const
    {
      return exp( -math::Sqr( ARGUMENT_X) / 2.0);
    }

    //! @brief derivative for one argument and result - depends on x ( ARGUMENT_X) and F( x) ( ARGUMENT_Y)
    //! @param ARGUMENT_X the argument for the derivative of the transfer function
    //! @param ARGUMENT_Y the result of F( ARGUMENT_X), can simplify calculating the derivative
    //! @return function value of the derivative of sigmoid transfer function
    float TransferGaussian::dF( const float &ARGUMENT_X, const float &ARGUMENT_Y) const
    {
      return ARGUMENT_X * ARGUMENT_Y;
    }

    //! @brief overloaded F for vectors, calls F(vector) of the base class
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    //! @return function value of sigmoid transfer function for a vector of arguments
    linal::Vector< float> TransferGaussian::F( const linal::VectorConstInterface< float> &ARGUMENT_X) const
    {
      return TransferFunctionInterface::F( ARGUMENT_X);
    }

    //! @brief overloaded F for vectors, calls F(vector,vector) of the base class
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    void TransferGaussian::F
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X
    ) const
    {
      return TransferFunctionInterface::F( STORAGE, ARGUMENT_X);
    }

    //! multiply the storage vector by dF; used to compute error derivative at prior hidden layers
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the derivative of the transfer function
    //! @param ARGUMENT_Y the vector of results of F( ARGUMENT_X), can simplify calculating the derivative
    void TransferGaussian::MultiplyBydF
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X,
      const linal::VectorConstInterface< float> &ARGUMENT_Y
    ) const
    {
      BCL_Assert
      (
        STORAGE.GetSize() == ARGUMENT_X.GetSize()
        && STORAGE.GetSize() == ARGUMENT_Y.GetSize(),
        "storage and argument vectors must be the same size!"
      );
      const float* x( ARGUMENT_X.Begin());
      const float* y( ARGUMENT_Y.Begin());

      for
      (
        float *itr_storage( STORAGE.Begin()), *itr_storage_end( STORAGE.End());
        itr_storage != itr_storage_end;
        ++itr_storage, ++x, ++y
      )
      {
        *itr_storage *= *x * ( *y);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer TransferGaussian::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "f(x) = e^( -x^2 / 2)");
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_transfer_linear.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_range.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> TransferLinear::s_Instance
    (
      util::Enumerated< TransferFunctionInterface>::AddInstance( new TransferLinear())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief default constructor
    TransferLinear::TransferLinear()
    {
    }

    //! copy constructor
    TransferLinear *TransferLinear::Clone() const
    {
      return new TransferLinear( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TransferLinear::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the working x range of that function
    //! @return x math::Range this function works on
    const math::Range< float> &TransferLinear::GetOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0, 1);
      return s_OutputRange;
    }

    //! @brief get the range between which the function has a significant derivative
    //! @return x math::Range this function works on
    //! @note this is set to the range at which dF(y) is == 1/25 max(dF(y))
    const math::Range< float> &TransferLinear::GetDynamicOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0.0, 1.0);
      return s_OutputRange;
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &TransferLinear::GetAlias() const
    {
      static const std::string s_Name( "Linear");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief function for one argument and result - depends on x ( ARGUMENT_X)
    //! @param ARGUMENT_X the argument for the transfer function
    //! @return function value of sigmoid transfer function
    float TransferLinear::F( const float &ARGUMENT_X) const
    {
      return ARGUMENT_X;
    }

    //! @brief derivative for one argument and result - depends on x ( ARGUMENT_X) and F( x) ( ARGUMENT_Y)
    //! @param ARGUMENT_X the argument for the derivative of the transfer function
    //! @param ARGUMENT_Y the result of F( ARGUMENT_X), can simplify calculating the derivative
    //! @return function value of the derivative of sigmoid transfer function
    float TransferLinear::dF( const float &ARGUMENT_X, const float &ARGUMENT_Y) const
    {
      return float( 1.0);
    }

    //! @brief overloaded F for vectors, calls F(vector) of the base class
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    //! @return function value of sigmoid transfer function for a vector of arguments
    linal::Vector< float> TransferLinear::F( const linal::VectorConstInterface< float> &ARGUMENT_X) const
    {
      return TransferFunctionInterface::F( ARGUMENT_X);
    }

    //! @brief overloaded F for vectors, calls F(vector,vector) of the base class
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    void TransferLinear::F
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X
    ) const
    {
      return TransferFunctionInterface::F( STORAGE, ARGUMENT_X);
    }

    //! multiply the storage vector by dF; used to compute error derivative at prior hidden layers
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the derivative of the transfer function
    //! @param ARGUMENT_Y the vector of results of F( ARGUMENT_X), can simplify calculating the derivative
    void TransferLinear::MultiplyBydF
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X,
      const linal::VectorConstInterface< float> &ARGUMENT_Y
    ) const
    {
      BCL_Assert
      (
        STORAGE.GetSize() == ARGUMENT_X.GetSize()
        && STORAGE.GetSize() == ARGUMENT_Y.GetSize(),
        "storage and argument vectors must be the same size!"
      );
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer TransferLinear::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "f(x) = x");
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_transfer_rectifier.h"

// includes from bcl - sorted alphabetically
#include "io/bcl_io_serialization.h"
#include "math/bcl_math_range.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> TransferRectifier::s_Instance
    (
      util::Enumerated< TransferFunctionInterface>::AddInstance( new TransferRectifier())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief copy constructor
    TransferRectifier *TransferRectifier::Clone() const
    {
      return new TransferRectifier( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TransferRectifier::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the working x range of that function
    //! @return x math::Range this function works on
    const math::Range< float> &TransferRectifier::GetOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0, 1);
      return s_OutputRange;
    }

    //! @brief get the range between which the function has a significant derivative
    //! @return x math::Range this function works on
    //! @note this is set to the range at which dF(y) is == 1/25 max(dF(y))
    const math::Range< float> &TransferRectifier::GetDynamicOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0.0, 1.0);
      return s_OutputRange;
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &TransferRectifier::GetAlias() const
    {
      static const std::string s_Name( "Rectifier");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief function for one argument and result - depends on x ( ARGUMENT_X)
    //! @param ARGUMENT_X the argument for the transfer function
    //! @return function value of sigmoid transfer function
    float TransferRectifier::F( const float &ARGUMENT_X) const
    {
      return ARGUMENT_X >= 0.0 ? ARGUMENT_X : m_Alpha * ARGUMENT_X;
    }

    //! @brief derivative for one argument and result - depends on x ( ARGUMENT_X) and F( x) ( ARGUMENT_Y)
    //! @param ARGUMENT_X the argument for the derivative of the transfer function
    //! @param ARGUMENT_Y the result of F( ARGUMENT_X), can simplify calculating the derivative
    //! @return function value of the derivative of sigmoid transfer function
    float TransferRectifier::dF( const float &ARGUMENT_X, const float &ARGUMENT_Y) const
    {
      return ARGUMENT_X > 0.0 ? 1.0 : m_Alpha;
    }

    //! @brief overloaded F for vectors, calls F(vector) of the base class
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    //! @return function value of sigmoid transfer function for a vector of arguments
    linal::Vector< float> TransferRectifier::F( const linal::VectorConstInterface< float> &ARGUMENT_X) const
    {
      return TransferFunctionInterface::F( ARGUMENT_X);
    }

    //! @brief overloaded F for vectors, calls F(vector,vector) of the base class
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    void TransferRectifier::F
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X
    ) const
    {
      return TransferFunctionInterface::F( STORAGE, ARGUMENT_X);
    }

    //! multiply the storage vector by dF; used to compute error derivative at prior hidden layers
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the derivative of the transfer function
    //! @param ARGUMENT_Y the vector of results of F( ARGUMENT_X), can simplify calculating the derivative
    void TransferRectifier::MultiplyBydF
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X,
      const linal::VectorConstInterface< float> &ARGUMENT_Y
    ) const
    {
      BCL_Assert
      (
        STORAGE.GetSize() == ARGUMENT_X.GetSize()
        && STORAGE.GetSize() == ARGUMENT_Y.GetSize(),
        "storage and argument vectors must be the same size!"
      );
      const float *x( ARGUMENT_X.Begin());

      for
      (
        float *itr_storage( STORAGE.Begin()), *itr_storage_end( STORAGE.End());
        itr_storage != itr_storage_end;
        ++itr_storage, ++x
      )
      {
        *itr_storage *= dF( *x, 0.0);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer TransferRectifier::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "f(x) = x * (x > 0 ? 1 : alpha)");
      parameters.AddInitializer
      (
        "",
        "multiplier for X below 0",
        io::Serialization::GetAgentWithRange( &m_Alpha, 0.0, 1.0)
      );
      return parameters;
    }

  } // namespace model
} // namespace bcl
// (c) Copyright BCL @ Vanderbilt University 2014
// (c) BCL Homepage: http://www.meilerlab.org/bclcommons
// (c) BCL Code Repository: https://github.com/BCLCommons/bcl
// (c)
// (c) The BioChemical Library (BCL) was originally developed by contributing members of the Meiler Lab @ Vanderbilt University.
// (c)
// (c) The BCL is now made available as an open-source software package distributed under the permissive MIT license,
// (c) developed and maintained by the Meiler Lab at Vanderbilt University and contributing members of the BCL Commons.
// (c)
// (c) External code contributions to the BCL are welcome. Please visit the BCL Commons GitHub page for information on how you can contribute.
// (c)
// (c) This file is part of the BCL software suite and is made available under the MIT license.
// (c)

// initialize the static initialization fiasco finder, if macro ENABLE_FIASCO_FINDER is defined
#include "util/bcl_util_static_initialization_fiasco_finder.h"
BCL_StaticInitializationFiascoFinder

// include header of this class
#include "model/bcl_model_transfer_sigmoid.h"

// includes from bcl - sorted alphabetically
#include "math/bcl_math_range.h"

// external includes - sorted alphabetically

namespace bcl
{
  namespace model
  {

  //////////
  // data //
  //////////

    // add the interface to the set of known implementations
    const util::SiPtr< const util::ObjectInterface> TransferSigmoid::s_Instance
    (
      util::Enumerated< TransferFunctionInterface>::AddInstance( new TransferSigmoid())
    );

  //////////////////////////////////
  // construction and destruction //
  //////////////////////////////////

    //! @brief copy constructor
    TransferSigmoid *TransferSigmoid::Clone() const
    {
      return new TransferSigmoid( *this);
    }

  /////////////////
  // data access //
  /////////////////

    //! @brief returns class name
    //! @return the class name as const ref std::string
    const std::string &TransferSigmoid::GetClassIdentifier() const
    {
      return GetStaticClassName( *this);
    }

    //! @brief get the working x range of that function
    //! @return x math::Range this function works on
    const math::Range< float> &TransferSigmoid::GetOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0, 1);
      return s_OutputRange;
    }

    //! @brief get the range between which the function has a significant derivative
    //! @return x math::Range this function works on
    //! @note this is set to the range at which dF(y) is == 1/25 max(dF(y))
    const math::Range< float> &TransferSigmoid::GetDynamicOutputRange() const
    {
      static const math::Range< float> s_OutputRange( 0.1, 0.9);
      return s_OutputRange;
    }

    //! @brief get a short name for this class
    //! @return a short name for this class
    const std::string &TransferSigmoid::GetAlias() const
    {
      static const std::string s_Name( "Sigmoid");
      return s_Name;
    }

  ////////////////
  // operations //
  ////////////////

    //! @brief function for one argument and result - depends on x ( ARGUMENT_X)
    //! @param ARGUMENT_X the argument for the transfer function
    //! @return function value of sigmoid transfer function
    float TransferSigmoid::F( const float &ARGUMENT_X) const
    {
      return 1.0 / ( 1.0 + exp( -ARGUMENT_X));
    }

    //! @brief derivative for one argument and result - depends on x ( ARGUMENT_X) and F( x) ( ARGUMENT_Y)
    //! @param ARGUMENT_X the argument for the derivative of the transfer function
    //! @param ARGUMENT_Y the result of F( ARGUMENT_X), can simplify calculating the derivative
    //! @return function value of the derivative of sigmoid transfer function
    float TransferSigmoid::dF( const float &, const float &ARGUMENT_Y) const
    {
      return ARGUMENT_Y * ( 1.0 - ARGUMENT_Y);
    }

    //! @brief overloaded F for vectors, calls F(vector) of the base class
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    //! @return function value of sigmoid transfer function for a vector of arguments
    linal::Vector< float> TransferSigmoid::F( const linal::VectorConstInterface< float> &ARGUMENT_X) const
    {
      return TransferFunctionInterface::F( ARGUMENT_X);
    }

    //! @brief overloaded F for vectors, calls F(vector,vector) of the base class
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the transfer function
    void TransferSigmoid::F
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X
    ) const
    {
      return TransferFunctionInterface::F( STORAGE, ARGUMENT_X);
    }

    //! multiply the storage vector by dF; used to compute error derivative at prior hidden layers
    //! @param STORAGE the vector that will hold the result
    //! @param ARGUMENT_X the vector of arguments for the derivative of the transfer function
    //! @param ARGUMENT_Y the vector of results of F( ARGUMENT_X), can simplify calculating the derivative
    void TransferSigmoid::MultiplyBydF
    (
      linal::VectorInterface< float> &STORAGE,
      const linal::VectorConstInterface< float> &ARGUMENT_X,
      const linal::VectorConstInterface< float> &ARGUMENT_Y
    ) const
    {
      BCL_Assert
      (
        STORAGE.GetSize() == ARGUMENT_X.GetSize()
        && STORAGE.GetSize() == ARGUMENT_Y.GetSize(),
        "storage and argument vectors must be the same size!"
      );
      const float *y( ARGUMENT_Y.Begin());

      for
      (
        float *itr_storage( STORAGE.Begin()), *itr_storage_end( STORAGE.End());
        itr_storage != itr_storage_end;
        ++itr_storage, ++y
      )
      {
        *itr_storage *= *y * ( 1.0 - *y);
      }
    }

    //! @brief return parameters for member data that are set up from the labels
    //! @return parameters for member data that are set up from the labels
    io::Serializer TransferSigmoid::GetSerializer() const
    {
      io::Serializer parameters;
      parameters.SetClassDescription( "f(x) = 1.0 / ( 1.0 + exp( -x))");
      return parameters;
    }

  } // namespace model
} // namespace bcl
